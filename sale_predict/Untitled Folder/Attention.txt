1. train set中有时候也会包含一些没有的信息，比如这个例子里面，并不是每个item都在test set里面的，所以我们只需要分析，test set中要求的item就行了。
2. 看见外语别慌，现在不缺翻译器
2.5. 做数据处理的时候，通过应该同时作用于train和test集，因此我们经常要合并两个数据集，一种方法是使用tuple，然后用for循环来一次处理数据，另一种方法是增加标记属性，然后直接使用stack，把两个数据集组合起来，这样做的好处是后续处理中就不用使用for循环了，而且可以规避掉因为粗心而导致的train，test数据集不一致的错误, 当然它也存在缺点，那就是做数据分析的时候，很多时候是要用到目标属性的，而这时test集数据所没有的，这使得在做数据分析的时候，总是要加入一个是否是训练集的判定条件。总上，具体使用哪种，其实是看个人兴趣 ***这其实也是可以写成一个方法的
3. downcast的必要性，对于动则百万条的数据来说，downcast可以节省很多空间
3.6 不明白grid的用处是什么？？？ ***** 这个应该考虑一下，是在有用，也可以写成一个方法， 他应该是为了补全空缺数据，方便下面的属性生成。为了保证lag数据的生成，必须保证每个时间点都是有对应数据的，尽管该在该时间点上物品的销售量为0。反过来，做时间序列预测的时候，应该注意获得的数据是否在时间上是完整的。又看了一下代码，这个解释好像有点问题？？？？？？难道只是为了防止，在后面总和另外两个表的时候，不要出现大量的空值？？？也不对啊，,,,,
4. 在机器学习与数据挖掘中，不论是分类问题（classification）还是回归问题（regression），采集的数据常常会包括定性特征（categorical feature）。因为定性特征表示某个数据属于一个特定的类别，所以在数值上，定性特征值通常是从0到n的离散整数。例子：花瓣的颜色（红、黄、蓝）、性别（男、女）、地址、某一列特征是否存在缺失值（这种NA 指示列常常会提供有效的额外信息）。一般情况下，针对定性特征，我们只需要使用sklearn的OneHotEncoder或LabelEncoder进行编码.定性特征的基数（cardinality）指的是这个定性特征所有可能的不同值的数量。在高基数（high cardinality）的定性特征面前，这些数据预处理的方法往往得不到令人满意的结果。
高基数定性特征的例子：IP地址、电子邮件域名、城市名、家庭住址、街道、产品号码。
我们可以尝试使用平均数编码（mean encoding）的编码方法，在贝叶斯的架构下，利用所要预测的应变量（target variable），有监督地确定最适合这个定性特征的编码方式。
*******注意该函数未添加入库，请及时处理，相应的还有code里面的其他几个函数
5. Creating item/shop pair lags lag-based features, 不清楚这些features的作用，*****可以加个方法在util中， 方法格式可以参考https://gallery.azure.ai/CustomModule/Generate-Lag-Features-1，注意应该出里NaN值（一般扔了）。同时写篇readTech
类似的，这几个feature也是应该记录在案的：https://machinelearningmastery.com/basic-feature-engineering-time-series-data-python/
他们分别是：date feature和window feature， 关于时间属性，他额外提出的属性的思路是很精彩的 
6. 不清楚为什么要删掉一整年的数据（2013）
7. 数据处理都是可以作为方法提取出来的
8. 了解一下，什么时候应该进行standard scalling **** 可以写成方法
9. 分层训练的方法也是可以几率进案的 **** 也可以写成一个方法
10. lightgbm不知道是什么 **** 写一篇readTech
11. keras也是不懂  **** 写一篇readTech