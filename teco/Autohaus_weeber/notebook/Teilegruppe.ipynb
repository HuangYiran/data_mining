{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ihuangyiran/anaconda2/envs/data_mining/lib/python3.6/site-packages/sklearn/utils/fixes.py:313: FutureWarning: numpy not_equal will not check object identity in the future. The comparison did not return the same result as suggested by the identity (`is`)) and will change.\n",
      "  _nan_object_mask = _nan_object_array != _nan_object_array\n"
     ]
    }
   ],
   "source": [
    "# load package\n",
    "# string \n",
    "import re\n",
    "\n",
    "# math\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import random\n",
    "\n",
    "# sys\n",
    "import sys\n",
    "import os\n",
    "import time\n",
    "import warnings\n",
    "\n",
    "# date\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# math\n",
    "import math\n",
    "\n",
    "# preprocessing\n",
    "from sklearn.preprocessing import KBinsDiscretizer, StandardScaler, MinMaxScaler, QuantileTransformer, PowerTransformer\n",
    "\n",
    "# machine learning\n",
    "from sklearn import svm, tree, linear_model, neighbors, naive_bayes, ensemble, gaussian_process, discriminant_analysis\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# model utils\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder, OrdinalEncoder\n",
    "from sklearn import feature_selection \n",
    "from sklearn import model_selection\n",
    "from sklearn import metrics\n",
    "\n",
    "# plot\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.pylab as pylab\n",
    "import seaborn as sns\n",
    "from pandas.tools.plotting import scatter_matrix #??\n",
    "# = show plots in Jupyter Notebook browser\n",
    "%matplotlib inline \n",
    "mpl.style.use('ggplot') #??\n",
    "sns.set_style('white')\n",
    "pylab.rcParams['figure.figsize'] = 12,8 #??\n",
    "\n",
    "# show all columns\n",
    "from IPython.display import display\n",
    "pd.options.display.max_columns = None\n",
    "\n",
    "# memory manage\n",
    "import gc\n",
    "\n",
    "# logging\n",
    "import logging \n",
    "\n",
    "# other\n",
    "import tqdm as tqdm\n",
    "\n",
    "# self define\n",
    "sys.path.append('../../utils/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# for network design\n",
    "import torch\n",
    "import argparse\n",
    "import os\n",
    "\n",
    "from scipy.stats import stats\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import lr_scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-11-20 09:12:25,687 - __main__ - INFO - This is a log info\n"
     ]
    }
   ],
   "source": [
    "logging.basicConfig(level = logging.INFO, format = '%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "handler = logging.FileHandler('../log/extract_features.log')\n",
    "handler.setLevel(logging.INFO)\n",
    "formater = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "handler.setFormatter(formater)\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.addHandler(handler)\n",
    "logger.info('This is a log info')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ihuangyiran/anaconda2/envs/data_mining/lib/python3.6/site-packages/IPython/core/interactiveshell.py:2785: DtypeWarning: Columns (15) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n",
      "/Users/ihuangyiran/anaconda2/envs/data_mining/lib/python3.6/site-packages/IPython/core/interactiveshell.py:2785: DtypeWarning: Columns (1,15) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "# read files\n",
    "lb_wsp_2014 = pd.read_csv('../data/Autohaus_weeber/leonberg_werkstattposten_2014.csv', sep = ';')\n",
    "lb_wsp_2015 = pd.read_csv('../data/Autohaus_weeber/leonberg_werkstattposten_2015.csv', sep = ';')\n",
    "lb_wsp_2016 = pd.read_csv('../data/Autohaus_weeber/leonberg_werkstattposten_2016.csv', sep = ';')\n",
    "lb_wsp_2017 = pd.read_csv('../data/Autohaus_weeber/leonberg_werkstattposten_2017.csv', sep = ';')\n",
    "lb_wsp_2018 = pd.read_csv('../data/Autohaus_weeber/leonberg_werkstattposten_2018.csv', sep = ';')\n",
    "std_wsp_2014 = pd.read_csv('../data/Autohaus_weeber/weil_der_stadt_werkstattposten_2014.csv', sep = ';')\n",
    "std_wsp_2015 = pd.read_csv('../data/Autohaus_weeber/weil_der_stadt_werkstattposten_2015.csv', sep = ';')\n",
    "std_wsp_2016 = pd.read_csv('../data/Autohaus_weeber/weil_der_stadt_werkstattposten_2016.csv', sep = ';')\n",
    "std_wsp_2017 = pd.read_csv('../data/Autohaus_weeber/weil_der_stadt_werkstattposten_2017.csv', sep = ';')\n",
    "std_wsp_2018 = pd.read_csv('../data/Autohaus_weeber/weil_der_stadt_werkstattposten_2018.csv', sep = ';')\n",
    "# cat\n",
    "d1 = lb_wsp_2014.copy()\n",
    "d2 = lb_wsp_2015.copy()\n",
    "d3 = lb_wsp_2016.copy()\n",
    "d4 = lb_wsp_2017.copy()\n",
    "d5 = lb_wsp_2018.copy()\n",
    "\n",
    "d6 = std_wsp_2014.copy()\n",
    "d7 = std_wsp_2015.copy()\n",
    "d8 = std_wsp_2016.copy()\n",
    "d9 = std_wsp_2017.copy()\n",
    "d10 = std_wsp_2018.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# for training we use data from 14 - 17 in lb_wsp\n",
    "train1 = pd.concat([d1,d2,d3,d4,d5], 0)\n",
    "train2 = pd.concat([d6,d7,d8,d9,d10], 0)\n",
    "train1['Autohaus'] = 'leonberg'\n",
    "train2['Autohaus'] = 'weil'\n",
    "#train = pd.concat([train1, train2], 0)\n",
    "#train = train1.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# because there exists reused Auftragsnummer in different Autohaus station. So we add some sign to the Auftragsnummer\n",
    "# in each Autohaus station\n",
    "train1['Auftragsnummer'] = 'A' + train1['Auftragsnummer']\n",
    "train2['Auftragsnummer'] = 'B' + train2['Auftragsnummer']\n",
    "train = pd.concat([train1, train2], 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## extract Teile-Gruppe\n",
    "An 4 . Stelle der Teile-Nr ist die Baugruppe \n",
    "- 1 - Motor\n",
    "- 2 - Kraftstoff, Abgas, Kühlung\n",
    "- 3 - Getrieb\n",
    "- 4 - Vorderachse, Lenkung\n",
    "- 5 - Hinterachse\n",
    "- 6 - Räder\n",
    "- 7 - Hebelwerk\n",
    "- 8 - Karosserie\n",
    "- 9 - Elektrik\n",
    "- 0 - Zubehör, Infotainment, Sonstiges"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### remove the outlier in the Teile-Nr\n",
    "- na wert\n",
    "- Teile-Nr, which length smaller than 4\n",
    "- doesn't contain number??\n",
    "- doesn't contain adjoining number \n",
    "\n",
    "Question: The Gruppe-Nr can also in the 5. stelle<br>\n",
    "#df[df['Teile-Nr'].map(lambda x: bool(re.search('[a-zA-Z]', x[3])))]['Teile-Nr']<br>\n",
    "the command show the Teile-Nr, which type are not number. We can see that many Teile-Nr doesn't contrain number; some Teile-Nr contain number, however there are not in the 4. stelle bu in the 5. oder 6. stelle. Therefore we suggest that the first coming adjoining nummer in the Teile-Nr may be the gruppe-Nr.<br>\n",
    "We extract the gruppe-Nr according to this idea.<br>\n",
    "This phase takes train data as input and output Dataframe df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# remove the outlier in the Teile-Nr\n",
    "# remove the na wert\n",
    "df = train[train['Teile-Nr'].isna().map(lambda x: not x)]\n",
    "# remove the value short value\n",
    "df = df[df['Teile-Nr'].map(lambda x: False if len(x) < 4 else True)]\n",
    "# remove the value that doesn't contain number: 30593 of 593527 in train1(5%)\n",
    "df = df[df['Teile-Nr'].map(lambda x: True if re.search('\\d', x) else False)]\n",
    "# remove the value that doesn't contrain adjoining number(min 2 number): 594 of 562934 in train1(0.1%)\n",
    "df = df[df['Teile-Nr'].map(lambda x: True if re.search('\\d\\d', x) else False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# use findall instead of search, because, the df here should contain adjoining number, otherwise it's wrong\n",
    "df['Gruppe-Nr'] = df['Teile-Nr'].map(lambda x: re.findall('\\d\\d', x)[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    930093\n",
       "1    281620\n",
       "9    241658\n",
       "3     46656\n",
       "2     37419\n",
       "6     23866\n",
       "8     21086\n",
       "5     19447\n",
       "4     14385\n",
       "7      6041\n",
       "Name: Gruppe-Nr, dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Gruppe-Nr'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "91"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# clean the memory\n",
    "del train1, train2, lb_wsp_2014, lb_wsp_2015, lb_wsp_2016, lb_wsp_2017, lb_wsp_2018\n",
    "del std_wsp_2014, std_wsp_2015, std_wsp_2016, std_wsp_2017, std_wsp_2018\n",
    "del d1, d2, d3, d4, d5, d6, d7, d8, d9, d10\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## remove the confused data\n",
    "- For the same Auftragsnummer, exists more than one value in the other attribute.\n",
    "- wierd Auftragsnummer: 107K, 77KW\n",
    "- Fahrgestellnummer is Na\n",
    "- Fahrgestellnummer that only contrain number.\n",
    "- In Auftragsdatum, the day of the datum is larger than 31\n",
    "- Wierd Auftragsdatum, these datum is shorter than 10 Ziffer.\n",
    "- Wierd Auftragsdatum, these datum is smaller than 2013"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# not only see that data with gruppe-nr == 9, 241658 items\n",
    "# the target of next coming can also for the other thing, not the eletric\n",
    "# df = df[df['Gruppe-Nr'] == '9']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# choose the features, which may be essentail to the model\n",
    "# We will take care of the 'AW-Nr' and 'KM-Stand' in the other place.\n",
    "# there are 98256 unique items in cf, more than 90423 items in af, which means they may create NA value after merging\n",
    "#cf = train[['Auftragsnummer', 'AW-Nr', 'Markencode', 'Lagerortcode', 'Adressanredecode', 'Motorcode', 'Fahrzeugmodellnummer', 'Modell', 'Typ',\n",
    "#        'Getriebecode', 'Gewicht', 'Leistung (KW)', 'Fahrgestellnummer']]\n",
    "cf = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# drop duplicate\n",
    "cf = cf.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# find out the confused data. For the same Auftragsnummer, exists more than one value in the other attribute.\n",
    "# here is AWSAU310019, BWSAU386471, BWSAU435051\n",
    "cf = cf.drop(cf[cf['Auftragsnummer'] == 'AWSAU310019'].index, axis= 0) # 8 items\n",
    "cf = cf.drop(cf[cf['Auftragsnummer'] == 'BWSAU386471'].index, axis= 0) # 3 items\n",
    "cf = cf.drop(cf[cf['Auftragsnummer'] == 'BWSAU435051'].index, axis= 0) # 2 items\n",
    "cf = cf.drop(cf[cf['Auftragsnummer'] == 'BWSAU271939'].index, axis= 0) # ? items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# remove wierd auftragsnummer\n",
    "cf = cf.drop(cf[cf['Auftragsnummer'] == '103K'].index) #\n",
    "cf = cf.drop(cf[cf['Auftragsnummer'] == '77KW'].index) #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# remove na value in Fahrgestellnummer\n",
    "cf = cf.drop(cf[cf['Fahrgestellnummer'].isna()].index, axis = 0) #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# remove the Fahrgestellnumer that only contain number, not tested!!!!\n",
    "cf = cf.drop(cf[cf['Fahrgestellnummer'].map(lambda x: False if re.search('[a-zA-Z]', x) else True)].index, axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# remove the items, which day of the Auftragsdatum lareger than 31, not tested!!!!\n",
    "cf = cf.drop(cf[cf['Auftragsdatum'].map(lambda x: int(x[0:2])) > 31].index, axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# remove the items, which length of the Auftragsdatum shorter than 10, not tested!!!!\n",
    "cf = cf.drop(cf[cf['Auftragsdatum'].map(lambda x:True if len(x) < 10 else False)].index, axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# remove the items, which year of the Auftragsdatum smaller than 2013, not tested!!!!\n",
    "cf = cf.drop(cf[cf['Auftragsdatum'].map(lambda x: int(x[6:]) < 2013)].index, axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#len(cf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "195"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = cf\n",
    "del cf\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#check_to_1(tmp, 'Auftragsnummer', 'Markencode')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#check_to_1(tmp, 'Auftragsnummer', 'Lagerortcode')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#check_to_1(tmp, 'Auftragsnummer', 'Adressanredecode')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#check_to_1(tmp, 'Auftragsnummer', 'Motorcode')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#check_to_1(tmp, 'Auftragsnummer', 'Fahrzeugmodellnummer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#check_to_1(tmp, 'Auftragsnummer', 'Modell')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#check_to_1(tmp, 'Auftragsnummer', 'Typ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#check_to_1(tmp, 'Auftragsnummer', 'Getriebecode')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#check_to_1(tmp, 'Auftragsnummer', 'Gewicht')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#check_to_1(tmp, 'Auftragsnummer', 'Leistung (KW)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#check_to_1(tmp, 'Auftragsnummer', 'Fahrgestellnummer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#check_to_1(train, 'Auftragsnummer', 'KM-Stand')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#check_to_1(train, 'Auftragsnummer', 'Erstzulassungsdatum')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Auftrag table for Gruppe-Nr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gn = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Falls Null date exist, drop these dates directly\n"
     ]
    }
   ],
   "source": [
    "gn = toAuftragTable(gn, 'Gruppe-Nr', 'Auftragsnummer') # number: 245018"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "244077"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(gn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## extract feature: total AW-Nr\n",
    "extract the feature total AW and combine aw with gruppe-nr auftragstable to form a new Dataframe agn. <br>\n",
    "combine the attribute in df to the agn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "aw = train.copy() # can't use df here, because df delete all the na value in Teile-Nr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ihuangyiran/anaconda2/envs/data_mining/lib/python3.6/site-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n",
      "/Users/ihuangyiran/anaconda2/envs/data_mining/lib/python3.6/site-packages/ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "# remove the items, that contain ziffer in the AW-Nr, set data type to int\n",
    "aw['AW-Nr'][aw['AW-Nr'].isnull()] = 0 # set null value to 0\n",
    "aw['AW-Nr'][aw['AW-Nr'] == '99999999'] = 0 # set 99999999 to 0\n",
    "aw['AW-Nr'] = aw['AW-Nr'].map(lambda x: re.sub(' ', '', str(x)))# remove space between the number\n",
    "del_index = aw[aw['AW-Nr'].map(lambda x: bool(re.search('[a-zA-Z]', str(x))))].index # remove fake data\n",
    "aw = aw.drop(del_index) \n",
    "aw['AW-Nr'] = aw['AW-Nr'].astype('float')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# to auftrag table, get the feature: total AW-Nr for each auftrag\n",
    "tmp = aw[['AW-Nr', 'Auftragsnummer']]\n",
    "ag = tmp.groupby('Auftragsnummer', as_index = False)\n",
    "out = []\n",
    "for name, gruppe in ag:\n",
    "    out.append(pd.DataFrame({'Auftragsnummer': [gruppe['Auftragsnummer'].iloc[0]], 'total AW': [gruppe['AW-Nr'].sum()]}))\n",
    "aw = pd.concat(out).reset_index()[['Auftragsnummer', 'total AW']] # 缺分段\n",
    "# drop the wierd Auftragsnummer\n",
    "aw = aw.drop(aw[aw['Auftragsnummer'] == '103K'].index)\n",
    "aw = aw.drop(aw[aw['Auftragsnummer'] == '77KW'].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# the lengths of aw and gn are different, because in some auftrag, the customer do not buy anything. \n",
    "# These part of auftrags are not included in gn, but gn is not a subset of aw. \n",
    "# because some AW-Nr in aw are wrong, we have deleted these items in aw. \n",
    "# Therefore, if we want to merge aw and gn, we have to use the method inner\n",
    "agn = pd.merge(aw, gn, how = 'inner', on = 'Auftragsnummer')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## extract feature: num_act\n",
    "not tested jet<br>\n",
    "make sure that, the length of the output, shoube be the same as the length of agn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ihuangyiran/anaconda2/envs/data_mining/lib/python3.6/site-packages/ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "/Users/ihuangyiran/anaconda2/envs/data_mining/lib/python3.6/site-packages/ipykernel_launcher.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  after removing the cwd from sys.path.\n"
     ]
    }
   ],
   "source": [
    "aw = train.copy()\n",
    "# remove the items, that contain ziffer in the AW-Nr, set data type to int\n",
    "aw['AW-Nr'][aw['AW-Nr'].isnull()] = 0 # set null value to 0\n",
    "aw['AW-Nr'][aw['AW-Nr'] == '99999999'] = 0 # set 99999999 to 0\n",
    "aw['AW-Nr'] = aw['AW-Nr'].map(lambda x: re.sub(' ', '', str(x)))# remove space between the number\n",
    "del_index = aw[aw['AW-Nr'].map(lambda x: bool(re.search('[a-zA-Z]', str(x))))].index # remove fake data\n",
    "aw = aw.drop(del_index) \n",
    "aw['AW-Nr'] = aw['AW-Nr'].astype('float')\n",
    "# to auftrag table, get the feature: total AW-Nr for each auftrag\n",
    "tmp = aw[['AW-Nr', 'Auftragsnummer']]\n",
    "ag = tmp.groupby('Auftragsnummer', as_index = False)\n",
    "out = []\n",
    "for name, gruppe in ag:\n",
    "    out.append(pd.DataFrame({'Auftragsnummer': [gruppe['Auftragsnummer'].iloc[0]], 'num_act': [len(gruppe)]}))\n",
    "aw = pd.concat(out).reset_index()[['Auftragsnummer', 'num_act']] # 缺分段\n",
    "# drop the wierd Auftragsnummer\n",
    "aw = aw.drop(aw[aw['Auftragsnummer'] == '103K'].index)\n",
    "aw = aw.drop(aw[aw['Auftragsnummer'] == '77KW'].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#agn = pd.merge(aw, agn, how = 'inner', on = 'Auftragsnummer')\n",
    "agn = pd.merge(aw, agn, how = 'inner', on = 'Auftragsnummer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Auftragsnummer</th>\n",
       "      <th>num_act</th>\n",
       "      <th>total AW</th>\n",
       "      <th>Gruppe-Nr</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AWSAU195981</td>\n",
       "      <td>31</td>\n",
       "      <td>1.526062e+09</td>\n",
       "      <td>0;3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AWSAU198256</td>\n",
       "      <td>31</td>\n",
       "      <td>6.398138e+08</td>\n",
       "      <td>0;1;9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AWSAU200129</td>\n",
       "      <td>14</td>\n",
       "      <td>3.832960e+08</td>\n",
       "      <td>0;6;1;9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AWSAU200223</td>\n",
       "      <td>60</td>\n",
       "      <td>1.841819e+09</td>\n",
       "      <td>6;5;3;0;9;1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AWSAU200258</td>\n",
       "      <td>23</td>\n",
       "      <td>5.367113e+08</td>\n",
       "      <td>0;1;9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Auftragsnummer  num_act      total AW    Gruppe-Nr\n",
       "0    AWSAU195981       31  1.526062e+09          0;3\n",
       "1    AWSAU198256       31  6.398138e+08        0;1;9\n",
       "2    AWSAU200129       14  3.832960e+08      0;6;1;9\n",
       "3    AWSAU200223       60  1.841819e+09  6;5;3;0;9;1\n",
       "4    AWSAU200258       23  5.367113e+08        0;1;9"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agn.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#train[train['Auftragsnummer'] == 'AWSAU195981']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## extract feature: num_teile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "agn['num_teile'] = agn['Gruppe-Nr'].map(lambda x: len(x.split(';')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>total AW</th>\n",
       "      <th>num_act</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>total AW</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.584288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>num_act</th>\n",
       "      <td>0.584288</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          total AW   num_act\n",
       "total AW  1.000000  0.584288\n",
       "num_act   0.584288  1.000000"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agn[['total AW', 'num_act']].corr() \n",
    "# why the correlation is so small??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "112"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del aw, gn\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## combine agn with df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tmp = df[['Auftragsnummer', 'KM-Stand','Markencode', 'Lagerortcode', 'Auftragsdatum', 'Adressanredecode', 'Motorcode', 'Fahrzeugmodellnummer', 'Modell', 'Typ', \n",
    "          'Getriebecode', 'Gewicht', 'Leistung (KW)', 'Fahrgestellnummer', 'Erstzulassungsdatum']]\n",
    "tmp = tmp.drop_duplicates()\n",
    "tmp = pd.merge(agn, tmp, how = 'inner', on = 'Auftragsnummer') # 244608"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "41"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = tmp\n",
    "del tmp\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#TODO\n",
    "# calculate the length of aw and original df\n",
    "# i suggest that, because i delete all the na value in Teile-Nr, the number of items in original df is smaller\n",
    "# than the number in aw.\n",
    "# the result is, after mergin, there are many na value in some attribute, \n",
    "# i want to see the number of distance.\n",
    "# or may be i can merge the data in the other direction: 'right'\n",
    "# but mit direction right, there exist also 600 null value but in total AN, the reason may be\n",
    "# that i delete some fake data in aw.\n",
    "# the problem here is, should i maintain the na value in total AW, other just remove all of them?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## extract features: month, day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "md = df.copy()\n",
    "# remove the outlier in the data\n",
    "#fake = md[md['Auftragsdatum'].map(lambda x:True if re.search('[a-zA-Z]', x) else False)]\n",
    "#md = md.drop(fake.index, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "md['year'] = md['Auftragsdatum'].map(lambda x: x[6:])\n",
    "md['month'] = md['Auftragsdatum'].map(lambda x: x[3:5])\n",
    "md['day'] = md['Auftragsdatum'].map(lambda x: x[0:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = md\n",
    "del md\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1     99287\n",
       "3     66428\n",
       "2     50047\n",
       "4     20344\n",
       "5      5373\n",
       "6      1446\n",
       "7       494\n",
       "8       185\n",
       "9        53\n",
       "10       10\n",
       "Name: num_teile, dtype: int64"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['num_teile'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## extract feature: age\n",
    "not tested jet!!!<br>\n",
    "age_auto = auftragsdatum - Erstzulassungsdatum<br>\n",
    "age_autohaus = auftragsdatum - 2013"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#tmp = train[['Auftragsdatum', 'Erstzulassungsdatum']]\n",
    "tmp = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tmp['Auftragsdatum'] = pd.to_datetime(tmp['Auftragsdatum'], dayfirst=True)\n",
    "tmp['Erstzulassungsdatum'] = pd.to_datetime(tmp['Erstzulassungsdatum'], dayfirst=True)\n",
    "tmp['age_auto'] = tmp['Auftragsdatum'] - tmp['Erstzulassungsdatum']\n",
    "tmp['age_auto'] = tmp['age_auto'].map(lambda x: x.days) % 365 # not test jet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tmp['age_autohaus'] = tmp['Auftragsdatum'].map(lambda x: x.year) - 2013"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = tmp\n",
    "del tmp\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Auftragsnummer</th>\n",
       "      <th>num_act</th>\n",
       "      <th>total AW</th>\n",
       "      <th>Gruppe-Nr</th>\n",
       "      <th>num_teile</th>\n",
       "      <th>KM-Stand</th>\n",
       "      <th>Markencode</th>\n",
       "      <th>Lagerortcode</th>\n",
       "      <th>Auftragsdatum</th>\n",
       "      <th>Adressanredecode</th>\n",
       "      <th>Motorcode</th>\n",
       "      <th>Fahrzeugmodellnummer</th>\n",
       "      <th>Modell</th>\n",
       "      <th>Typ</th>\n",
       "      <th>Getriebecode</th>\n",
       "      <th>Gewicht</th>\n",
       "      <th>Leistung (KW)</th>\n",
       "      <th>Fahrgestellnummer</th>\n",
       "      <th>Erstzulassungsdatum</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>day</th>\n",
       "      <th>age_auto</th>\n",
       "      <th>age_autohaus</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AWSAU195981</td>\n",
       "      <td>31</td>\n",
       "      <td>1.526062e+09</td>\n",
       "      <td>0;3</td>\n",
       "      <td>2</td>\n",
       "      <td>38110,00</td>\n",
       "      <td>VW</td>\n",
       "      <td>Weeber Leonberg VW (Glemseckstraße 49, 71229 L...</td>\n",
       "      <td>2014-01-13</td>\n",
       "      <td>Firma</td>\n",
       "      <td>CRBC</td>\n",
       "      <td>5G131X</td>\n",
       "      <td>Golf VII 2.0 TDI CL BMT</td>\n",
       "      <td>X0A</td>\n",
       "      <td>PFL</td>\n",
       "      <td>0,00</td>\n",
       "      <td>110.0</td>\n",
       "      <td>WVWZZZAUZDP084627</td>\n",
       "      <td>2013-05-17</td>\n",
       "      <td>2014</td>\n",
       "      <td>01</td>\n",
       "      <td>13</td>\n",
       "      <td>241.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AWSAU198256</td>\n",
       "      <td>31</td>\n",
       "      <td>6.398138e+08</td>\n",
       "      <td>0;1;9</td>\n",
       "      <td>3</td>\n",
       "      <td>59660,00</td>\n",
       "      <td>AUDI</td>\n",
       "      <td>Weeber Leonberg Audi (Glemseckstraße 39, 71229...</td>\n",
       "      <td>2014-01-16</td>\n",
       "      <td>AN</td>\n",
       "      <td>CJC</td>\n",
       "      <td>8K50SC</td>\n",
       "      <td>Audi A4 Attraction Avant 2.0 T</td>\n",
       "      <td>X0A</td>\n",
       "      <td>MVS</td>\n",
       "      <td>0,00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>WAUZZZ8K0DA098516</td>\n",
       "      <td>2012-07-27</td>\n",
       "      <td>2014</td>\n",
       "      <td>01</td>\n",
       "      <td>16</td>\n",
       "      <td>173.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AWSAU200129</td>\n",
       "      <td>14</td>\n",
       "      <td>3.832960e+08</td>\n",
       "      <td>0;6;1;9</td>\n",
       "      <td>4</td>\n",
       "      <td>227777,00</td>\n",
       "      <td>VW</td>\n",
       "      <td>Weeber Leonberg VW (Glemseckstraße 49, 71229 L...</td>\n",
       "      <td>2014-01-09</td>\n",
       "      <td>Herr</td>\n",
       "      <td>CBAB</td>\n",
       "      <td>5K143M</td>\n",
       "      <td>Golf Highline 2,0 l</td>\n",
       "      <td>X0A</td>\n",
       "      <td>LQV</td>\n",
       "      <td>0,00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>WVWZZZ1KZAW187314</td>\n",
       "      <td>2009-11-25</td>\n",
       "      <td>2014</td>\n",
       "      <td>01</td>\n",
       "      <td>09</td>\n",
       "      <td>46.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AWSAU200223</td>\n",
       "      <td>60</td>\n",
       "      <td>1.841819e+09</td>\n",
       "      <td>6;5;3;0;9;1</td>\n",
       "      <td>6</td>\n",
       "      <td>65174,00</td>\n",
       "      <td>VW</td>\n",
       "      <td>Weeber Leonberg VW (Glemseckstraße 49, 71229 L...</td>\n",
       "      <td>2014-01-07</td>\n",
       "      <td>Herr</td>\n",
       "      <td>AXR</td>\n",
       "      <td>1J5134</td>\n",
       "      <td>VW Golf IV &gt; 98 / Variant &gt; 99</td>\n",
       "      <td>NaN</td>\n",
       "      <td>EUH</td>\n",
       "      <td>0,00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>WVWZZZ1JZ2W511716</td>\n",
       "      <td>2002-02-15</td>\n",
       "      <td>2014</td>\n",
       "      <td>01</td>\n",
       "      <td>07</td>\n",
       "      <td>329.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AWSAU200258</td>\n",
       "      <td>23</td>\n",
       "      <td>5.367113e+08</td>\n",
       "      <td>0;1;9</td>\n",
       "      <td>3</td>\n",
       "      <td>212110,00</td>\n",
       "      <td>AUDI</td>\n",
       "      <td>Weeber Leonberg Audi (Glemseckstraße 39, 71229...</td>\n",
       "      <td>2014-01-02</td>\n",
       "      <td>Firma</td>\n",
       "      <td>CAGA</td>\n",
       "      <td>8K50QC</td>\n",
       "      <td>A4 Avant 2.0 TDI Attraction</td>\n",
       "      <td>X0A</td>\n",
       "      <td>LLN</td>\n",
       "      <td>0,00</td>\n",
       "      <td>105.0</td>\n",
       "      <td>WAUZZZ8K5AA164327</td>\n",
       "      <td>2010-05-06</td>\n",
       "      <td>2014</td>\n",
       "      <td>01</td>\n",
       "      <td>02</td>\n",
       "      <td>242.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>AWSAU200259</td>\n",
       "      <td>9</td>\n",
       "      <td>1.862719e+08</td>\n",
       "      <td>0;9</td>\n",
       "      <td>2</td>\n",
       "      <td>91895,00</td>\n",
       "      <td>AUDI</td>\n",
       "      <td>Weeber Leonberg Audi (Glemseckstraße 39, 71229...</td>\n",
       "      <td>2014-01-02</td>\n",
       "      <td>Herr</td>\n",
       "      <td>ALT</td>\n",
       "      <td>8E20B4</td>\n",
       "      <td>A4 Limousine 2.0 R4 96 M5S</td>\n",
       "      <td>NaN</td>\n",
       "      <td>GBM</td>\n",
       "      <td>0,00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>WAUZZZ8E13A363100</td>\n",
       "      <td>2003-04-24</td>\n",
       "      <td>2014</td>\n",
       "      <td>01</td>\n",
       "      <td>02</td>\n",
       "      <td>256.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>AWSAU200260</td>\n",
       "      <td>11</td>\n",
       "      <td>3.962096e+08</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>122325,00</td>\n",
       "      <td>AUDI</td>\n",
       "      <td>Weeber Leonberg Audi (Glemseckstraße 39, 71229...</td>\n",
       "      <td>2014-01-02</td>\n",
       "      <td>Herr</td>\n",
       "      <td>CAYC</td>\n",
       "      <td>8P1AE4</td>\n",
       "      <td>Audi A3 Attraction 3-Türer 1.6</td>\n",
       "      <td>X0A</td>\n",
       "      <td>LUB</td>\n",
       "      <td>0,00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>WAUZZZ8PXAA157479</td>\n",
       "      <td>2010-04-29</td>\n",
       "      <td>2014</td>\n",
       "      <td>01</td>\n",
       "      <td>02</td>\n",
       "      <td>249.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>AWSAU200263</td>\n",
       "      <td>36</td>\n",
       "      <td>6.978901e+08</td>\n",
       "      <td>0;1;9;3</td>\n",
       "      <td>4</td>\n",
       "      <td>112386,00</td>\n",
       "      <td>AUDI</td>\n",
       "      <td>Weeber Leonberg Audi (Glemseckstraße 39, 71229...</td>\n",
       "      <td>2014-01-02</td>\n",
       "      <td>Herr</td>\n",
       "      <td>ASB</td>\n",
       "      <td>4F20LL</td>\n",
       "      <td>Audi A6 Limousine 3.0 TDI (DPF</td>\n",
       "      <td>X0A</td>\n",
       "      <td>KGX</td>\n",
       "      <td>0,00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>WAUZZZ4F28N185258</td>\n",
       "      <td>2008-10-04</td>\n",
       "      <td>2014</td>\n",
       "      <td>01</td>\n",
       "      <td>02</td>\n",
       "      <td>91.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>AWSAU200264</td>\n",
       "      <td>12</td>\n",
       "      <td>1.656055e+08</td>\n",
       "      <td>0;8;3</td>\n",
       "      <td>3</td>\n",
       "      <td>42783,00</td>\n",
       "      <td>AUDI</td>\n",
       "      <td>Weeber Leonberg Audi (Glemseckstraße 39, 71229...</td>\n",
       "      <td>2014-01-02</td>\n",
       "      <td>Herr</td>\n",
       "      <td>CLAB</td>\n",
       "      <td>4G50NH</td>\n",
       "      <td>A6 Avant 3.0 TDI</td>\n",
       "      <td>X0A</td>\n",
       "      <td>NKP</td>\n",
       "      <td>0,00</td>\n",
       "      <td>150.0</td>\n",
       "      <td>WAUZZZ4G4CN101295</td>\n",
       "      <td>2012-02-21</td>\n",
       "      <td>2014</td>\n",
       "      <td>01</td>\n",
       "      <td>02</td>\n",
       "      <td>316.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>AWSAU200269</td>\n",
       "      <td>8</td>\n",
       "      <td>1.641055e+08</td>\n",
       "      <td>0;3</td>\n",
       "      <td>2</td>\n",
       "      <td>180999,00</td>\n",
       "      <td>AUDI</td>\n",
       "      <td>Weeber Leonberg Audi (Glemseckstraße 39, 71229...</td>\n",
       "      <td>2014-01-02</td>\n",
       "      <td>Herr</td>\n",
       "      <td>BPP</td>\n",
       "      <td>8ED0YH</td>\n",
       "      <td>Audi A4 Avant 2.7 TDI(DPF)</td>\n",
       "      <td>X0A</td>\n",
       "      <td>JBB</td>\n",
       "      <td>0,00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>WAUZZZ8E07A224307</td>\n",
       "      <td>2007-03-29</td>\n",
       "      <td>2014</td>\n",
       "      <td>01</td>\n",
       "      <td>02</td>\n",
       "      <td>281.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Auftragsnummer  num_act      total AW    Gruppe-Nr  num_teile   KM-Stand  \\\n",
       "0    AWSAU195981       31  1.526062e+09          0;3          2   38110,00   \n",
       "1    AWSAU198256       31  6.398138e+08        0;1;9          3   59660,00   \n",
       "2    AWSAU200129       14  3.832960e+08      0;6;1;9          4  227777,00   \n",
       "3    AWSAU200223       60  1.841819e+09  6;5;3;0;9;1          6   65174,00   \n",
       "4    AWSAU200258       23  5.367113e+08        0;1;9          3  212110,00   \n",
       "5    AWSAU200259        9  1.862719e+08          0;9          2   91895,00   \n",
       "6    AWSAU200260       11  3.962096e+08            0          1  122325,00   \n",
       "7    AWSAU200263       36  6.978901e+08      0;1;9;3          4  112386,00   \n",
       "8    AWSAU200264       12  1.656055e+08        0;8;3          3   42783,00   \n",
       "9    AWSAU200269        8  1.641055e+08          0;3          2  180999,00   \n",
       "\n",
       "  Markencode                                       Lagerortcode Auftragsdatum  \\\n",
       "0         VW  Weeber Leonberg VW (Glemseckstraße 49, 71229 L...    2014-01-13   \n",
       "1       AUDI  Weeber Leonberg Audi (Glemseckstraße 39, 71229...    2014-01-16   \n",
       "2         VW  Weeber Leonberg VW (Glemseckstraße 49, 71229 L...    2014-01-09   \n",
       "3         VW  Weeber Leonberg VW (Glemseckstraße 49, 71229 L...    2014-01-07   \n",
       "4       AUDI  Weeber Leonberg Audi (Glemseckstraße 39, 71229...    2014-01-02   \n",
       "5       AUDI  Weeber Leonberg Audi (Glemseckstraße 39, 71229...    2014-01-02   \n",
       "6       AUDI  Weeber Leonberg Audi (Glemseckstraße 39, 71229...    2014-01-02   \n",
       "7       AUDI  Weeber Leonberg Audi (Glemseckstraße 39, 71229...    2014-01-02   \n",
       "8       AUDI  Weeber Leonberg Audi (Glemseckstraße 39, 71229...    2014-01-02   \n",
       "9       AUDI  Weeber Leonberg Audi (Glemseckstraße 39, 71229...    2014-01-02   \n",
       "\n",
       "  Adressanredecode Motorcode Fahrzeugmodellnummer  \\\n",
       "0            Firma      CRBC               5G131X   \n",
       "1               AN       CJC               8K50SC   \n",
       "2             Herr      CBAB               5K143M   \n",
       "3             Herr       AXR               1J5134   \n",
       "4            Firma      CAGA               8K50QC   \n",
       "5             Herr       ALT               8E20B4   \n",
       "6             Herr      CAYC               8P1AE4   \n",
       "7             Herr       ASB               4F20LL   \n",
       "8             Herr      CLAB               4G50NH   \n",
       "9             Herr       BPP               8ED0YH   \n",
       "\n",
       "                           Modell  Typ Getriebecode Gewicht  Leistung (KW)  \\\n",
       "0         Golf VII 2.0 TDI CL BMT  X0A          PFL    0,00          110.0   \n",
       "1  Audi A4 Attraction Avant 2.0 T  X0A          MVS    0,00            0.0   \n",
       "2             Golf Highline 2,0 l  X0A          LQV    0,00            0.0   \n",
       "3  VW Golf IV > 98 / Variant > 99  NaN          EUH    0,00            0.0   \n",
       "4     A4 Avant 2.0 TDI Attraction  X0A          LLN    0,00          105.0   \n",
       "5      A4 Limousine 2.0 R4 96 M5S  NaN          GBM    0,00            0.0   \n",
       "6  Audi A3 Attraction 3-Türer 1.6  X0A          LUB    0,00            0.0   \n",
       "7  Audi A6 Limousine 3.0 TDI (DPF  X0A          KGX    0,00            0.0   \n",
       "8                A6 Avant 3.0 TDI  X0A          NKP    0,00          150.0   \n",
       "9      Audi A4 Avant 2.7 TDI(DPF)  X0A          JBB    0,00            0.0   \n",
       "\n",
       "   Fahrgestellnummer Erstzulassungsdatum  year month day  age_auto  \\\n",
       "0  WVWZZZAUZDP084627          2013-05-17  2014    01  13     241.0   \n",
       "1  WAUZZZ8K0DA098516          2012-07-27  2014    01  16     173.0   \n",
       "2  WVWZZZ1KZAW187314          2009-11-25  2014    01  09      46.0   \n",
       "3  WVWZZZ1JZ2W511716          2002-02-15  2014    01  07     329.0   \n",
       "4  WAUZZZ8K5AA164327          2010-05-06  2014    01  02     242.0   \n",
       "5  WAUZZZ8E13A363100          2003-04-24  2014    01  02     256.0   \n",
       "6  WAUZZZ8PXAA157479          2010-04-29  2014    01  02     249.0   \n",
       "7  WAUZZZ4F28N185258          2008-10-04  2014    01  02      91.0   \n",
       "8  WAUZZZ4G4CN101295          2012-02-21  2014    01  02     316.0   \n",
       "9  WAUZZZ8E07A224307          2007-03-29  2014    01  02     281.0   \n",
       "\n",
       "   age_autohaus  \n",
       "0             1  \n",
       "1             1  \n",
       "2             1  \n",
       "3             1  \n",
       "4             1  \n",
       "5             1  \n",
       "6             1  \n",
       "7             1  \n",
       "8             1  \n",
       "9             1  "
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## transfer the type of km stand to float"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df['KM-Stand'] = df['KM-Stand'].map(lambda x: float(re.sub(',', '.', x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## extract feature: KM age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 243667 entries, 0 to 243666\n",
      "Data columns (total 24 columns):\n",
      "Auftragsnummer          243667 non-null object\n",
      "num_act                 243667 non-null int64\n",
      "total AW                243667 non-null float64\n",
      "Gruppe-Nr               243667 non-null object\n",
      "num_teile               243667 non-null int64\n",
      "KM-Stand                243667 non-null float64\n",
      "Markencode              243468 non-null object\n",
      "Lagerortcode            243667 non-null object\n",
      "Auftragsdatum           243667 non-null datetime64[ns]\n",
      "Adressanredecode        241586 non-null object\n",
      "Motorcode               235550 non-null object\n",
      "Fahrzeugmodellnummer    243477 non-null object\n",
      "Modell                  242305 non-null object\n",
      "Typ                     162462 non-null object\n",
      "Getriebecode            234752 non-null object\n",
      "Gewicht                 243461 non-null object\n",
      "Leistung (KW)           243461 non-null float64\n",
      "Fahrgestellnummer       243667 non-null object\n",
      "Erstzulassungsdatum     243461 non-null datetime64[ns]\n",
      "year                    243667 non-null object\n",
      "month                   243667 non-null object\n",
      "day                     243667 non-null object\n",
      "age_auto                243461 non-null float64\n",
      "age_autohaus            243667 non-null int64\n",
      "dtypes: datetime64[ns](2), float64(4), int64(3), object(15)\n",
      "memory usage: 56.5+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## extract feature: time relative\n",
    "- times in group\n",
    "- time distance since last come\n",
    "- time distance to next come\n",
    "- km stand distance since last come(correct the data first)\n",
    "- km stand distance to next come\n",
    "- is km stand to/from next/last come larger than threshold\n",
    "- frequent in each year, in total\n",
    "- (not jet)weather come in X month again\n",
    "- time shift for num_act, total aw, Gruppe-Nr, num_teile\n",
    "- repair age in year or in month\n",
    "- repair frequent in last X month\n",
    "- trend of frequent in the past\n",
    "- mean, max, min, quater repaire km distance\n",
    "- mean, max, min, quater repaire time distance\n",
    "\n",
    "We sort the dataframe with ['Fahrgestellnummer', 'Auftragsdatum'], and calculate the datum distance betwenn the nearby items.<br>\n",
    "One thing to be clear is that, the first distance for each Fahrgestellnummer is wrong, because we calculate it with the its Auftragsdatum and the Auftragsdatum of the other Fahrgestellnummer.<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "td = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "td = td[['Fahrgestellnummer', 'Auftragsdatum', 'KM-Stand','num_act', 'total AW', 'num_teile', 'year']]\n",
    "#1C8FYN8O63T585854 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# get sample, to test the code\n",
    "for name, group in td.groupby('Fahrgestellnummer', as_index = False):\n",
    "    if name == '1C8FYN8O63T585854':\n",
    "        tmp = group.reindex()\n",
    "        print(tmp.columns)\n",
    "        print(tmp.index)\n",
    "        # time distance since/to last/next come\n",
    "     #create feature: time distance from last coming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# code to extract new features, remmenber to remove counter\n",
    "out = []\n",
    "counter = 0\n",
    "logger.info('create features: time relative:')\n",
    "total_group = len(td['Fahrgestellnummer'].unique())\n",
    "for name, group in td.groupby('Fahrgestellnummer', as_index = False):\n",
    "    counter = counter + 1\n",
    "    if counter > 1000:\n",
    "        num_group = counter + 1\n",
    "        logger.info('number of group: {} [{}/{} ({:0f}%)]'.format(num_group, num_group, total_group, 100*0.1*num_group/total_group))\n",
    "        tmp_df = pd.concat(out)\n",
    "        tmp_df.to_pickle('tmp.pkl')\n",
    "        break\n",
    "    #if name == '1C8FYN8O63T585854':\n",
    "    #    tmp = group.reindex()\n",
    "    #    print(tmp.columns)\n",
    "    #    print(tmp.index)\n",
    "    #    # time distance since/to last/next come\n",
    "    # create feature: time distance from last coming\n",
    "    tmp = group\n",
    "    if len(tmp) > 1:\n",
    "        tmp['times'] = len(tmp)\n",
    "        tmp = tmp.sort_values(by = 'Auftragsdatum').reset_index().iloc[:,1:] # not tested\n",
    "        # create feature: time distance from last coming\n",
    "        time_shift_forward = pd.concat([pd.Series(datetime(2010, 1, 1)), tmp['Auftragsdatum'][:-1]])\n",
    "        time_shift_forward.index = tmp.index\n",
    "        tmp['time_distance_from_last'] = tmp['Auftragsdatum'] - time_shift_forward\n",
    "        tmp['time_distance_from_last'] = tmp['time_distance_from_last'].clip(lower = timedelta(0), upper = timedelta(365)) # clip\n",
    "        # create feature: mean time distance in group, should i clip the data first????\n",
    "        tmp['mean_time_distance_in_group'] = tmp['time_distance_from_last'].mean()\n",
    "        # create feature: max time distance in group\n",
    "        tmp['max_time_distance_in_group'] = tmp['time_distance_from_last'].max()\n",
    "        # create feature: min time distance in group\n",
    "        tmp['min_time_distance_in_group'] = tmp['time_distance_from_last'].min()\n",
    "        # create feature: time distance to next coming\n",
    "        time_distance_to_next = pd.concat([tmp['time_distance_from_last'][1:], pd.Series(np.NaN)])\n",
    "        time_distance_to_next.index = tmp.index\n",
    "        tmp['time_distance_to_next'] = time_distance_to_next\n",
    "        # reset the first value and last value\n",
    "        tmp.iloc[0, tmp.columns.get_loc('time_distance_from_last')] = np.NaN\n",
    "        tmp.iloc[-1, tmp.columns.get_loc('time_distance_to_next')] = np.NaN\n",
    "        # create feature: km stand distance from last coming\n",
    "        km_shift_forward = pd.concat([pd.Series(0.0), tmp['KM-Stand'][:-1]])\n",
    "        km_shift_forward.index = tmp.index\n",
    "        tmp['km_distance_from_last'] = tmp['KM-Stand'] -km_shift_forward\n",
    "        tmp['km_distance_from_last'] = tmp['km_distance_from_last'].abs() # correct the negative value in a stupid way!!!\n",
    "        tmp['km_distance_from_last'][tmp['km_distance_from_last'] > 10000] = 10000 # clip\n",
    "        # create feature: mean km stand distance, not test\n",
    "        tmp['mean_km_distance_in_group'] = tmp['km_distance_from_last'].mean() \n",
    "        # create feature: max km stand distance, not test\n",
    "        tmp['max_km_distance_in_group'] = tmp['km_distance_from_last'].max()\n",
    "        # create feature: min km stand distance, not test\n",
    "        tmp['min_km_distance_in_group'] = tmp['km_distance_from_last'].min()\n",
    "        # create feature: km stand distance to next coming\n",
    "        km_distance_to_next = pd.concat([tmp['km_distance_from_last'][1:], pd.Series(0.0)])\n",
    "        km_distance_to_next.index = tmp.index\n",
    "        tmp['km_distance_to_next'] = km_distance_to_next\n",
    "        # reset the first value and last value\n",
    "        tmp.iloc[0, tmp.columns.get_loc('km_distance_from_last')] = np.NaN\n",
    "        tmp.iloc[-1, tmp.columns.get_loc('km_distance_to_next')] = np.NaN\n",
    "        # create feature: frequent in each year\n",
    "        year_candidates = ['2014', '2015', '2016', '2017', '2018']\n",
    "        for year in year_candidates:\n",
    "            tmp['frequent_in_'+year] = len(tmp[tmp['year'] == year])\n",
    "        # create feature: time shift for num_act\n",
    "        num_act_shift_forward  = pd.concat([pd.Series(np.NaN), tmp['num_act'][:-1]])\n",
    "        num_act_shift_forward.index = tmp.index\n",
    "        tmp['num_act_shift1'] = num_act_shift_forward\n",
    "        # create feature: time shift for num_teile\n",
    "        num_teile_shift_forward  = pd.concat([pd.Series(np.NaN), tmp['num_teile'][:-1]])\n",
    "        num_teile_shift_forward.index = tmp.index\n",
    "        tmp['num_teile_shift1'] = num_teile_shift_forward\n",
    "        # create feature: time shift for total aw\n",
    "        total_aw_shift_forward  = pd.concat([pd.Series(np.NaN), tmp['total AW'][:-1]])\n",
    "        total_aw_shift_forward.index = tmp.index\n",
    "        tmp['total_aw_shift1'] = total_aw_shift_forward\n",
    "        if len(tmp) > 2:\n",
    "            num_act_shift_forward2 = pd.concat([pd.Series([np.NaN, np.NaN]), tmp['num_act'][:-2]])\n",
    "            num_act_shift_forward2.index = tmp.index\n",
    "            tmp['num_act_shift2'] = num_act_shift_forward2\n",
    "            num_teile_shift_forward2 = pd.concat([pd.Series([np.NaN, np.NaN]), tmp['num_teile'][:-2]])\n",
    "            num_teile_shift_forward2.index = tmp.index\n",
    "            tmp['num_teile_shift2'] = num_teile_shift_forward2\n",
    "            total_aw_shift_forward2 = pd.concat([pd.Series([np.NaN, np.NaN]), tmp['total AW'][:-2]])\n",
    "            total_aw_shift_forward2.index = tmp.index\n",
    "            tmp['total_aw_shift2'] = total_aw_shift_forward2\n",
    "        else:\n",
    "            tmp['num_act_shift2'] = np.NaN\n",
    "            tmp['num_teile_shift2'] = np.NaN\n",
    "            tmp['total_aw_shift2'] = np.NaN\n",
    "        # create feature: repair age in year or in month\n",
    "        tmp['first_date'] = tmp.iloc[0, tmp.columns.get_loc('Auftragsdatum')]\n",
    "        tmp['repair_age'] = tmp['Auftragsdatum'] - tmp['first_date']\n",
    "        tmp = tmp.drop('first_date', axis=1)\n",
    "        # create feature: repair frequent in last X month\n",
    "        time_distance_shift1 = pd.concat([pd.Series(np.NaN), tmp['time_distance_from_last'][:-1]])\n",
    "        time_distance_shift1.index = tmp.index\n",
    "        tmp['time_distance_shift1'] = time_distance_shift1\n",
    "        time_distance_shift2 = pd.concat([pd.Series(np.NaN), tmp['time_distance_shift1'][:-1]])\n",
    "        time_distance_shift2.index = tmp.index\n",
    "        tmp['time_distance_shift2'] = time_distance_shift2\n",
    "        time_distance_shift3 = pd.concat([pd.Series(np.NaN), tmp['time_distance_shift2'][:-1]])\n",
    "        time_distance_shift3.index = tmp.index\n",
    "        tmp['time_distance_shift3'] = time_distance_shift3\n",
    "        tmp['time_distance_from_last2'] = tmp['time_distance_from_last'] + tmp['time_distance_shift1']\n",
    "        tmp['time_distance_from_last3'] = tmp['time_distance_from_last2'] + tmp['time_distance_shift2']\n",
    "        tmp['time_distance_from_last4'] = tmp['time_distance_from_last3'] + tmp['time_distance_shift3']\n",
    "        def get_frequent(x, d = 30):\n",
    "            if x['time_distance_from_last4'] < timedelta(days = d):\n",
    "                return 4\n",
    "            elif x['time_distance_from_last3'] < timedelta(days = d):\n",
    "                return 3\n",
    "            elif x['time_distance_from_last2'] < timedelta(days = d):\n",
    "                return 2\n",
    "            elif x['time_distance_from_last'] < timedelta(days = d):\n",
    "                return 1\n",
    "            else:\n",
    "                return 0\n",
    "        tmp['frequent_in_last_1_month'] = tmp.apply(get_frequent, axis=1, args = [1*30])\n",
    "        tmp['frequent_in_last_2_month'] = tmp.apply(get_frequent, axis=1, args = [2*30])\n",
    "        tmp['frequent_in_last_3_month'] = tmp.apply(get_frequent, axis=1, args = [3*30])\n",
    "        tmp['frequent_in_last_4_month'] = tmp.apply(get_frequent, axis=1, args = [4*30])\n",
    "        tmp['frequent_in_last_5_month'] = tmp.apply(get_frequent, axis=1, args = [5*30])\n",
    "        tmp['frequent_in_last_6_month'] = tmp.apply(get_frequent, axis=1, args = [6*30])\n",
    "        # create feature: weather come in last X month\n",
    "        tmp['weather_come_in_last_1_month'] = tmp['time_distance_from_last'] < timedelta(days = 1*30)\n",
    "        tmp['weather_come_in_last_2_month'] = tmp['time_distance_from_last'] < timedelta(days = 2*30)\n",
    "        tmp['weather_come_in_last_3_month'] = tmp['time_distance_from_last'] < timedelta(days = 3*30)\n",
    "        tmp['weather_come_in_last_4_month'] = tmp['time_distance_from_last'] < timedelta(days = 4*30)\n",
    "        tmp['weather_come_in_last_5_month'] = tmp['time_distance_from_last'] < timedelta(days = 5*30)\n",
    "        tmp['weather_come_in_last_6_month'] = tmp['time_distance_from_last'] < timedelta(days = 6*30)\n",
    "        # create feature: trend of frequent in the past\n",
    "        def get_trend(x):\n",
    "            xs = x[['time_distance_from_last', 'time_distance_shift1', 'time_distance_shift2', 'time_distance_shift3']].tolist()\n",
    "            xs = [x.days for x in xs if not pd.isnull(x)]\n",
    "            if len(xs) < 2:\n",
    "                return 0\n",
    "            ys = range(1, len(xs) + 1)\n",
    "            xm = sum(xs)/len(xs)\n",
    "            ym = sum(ys)/len(ys)\n",
    "            xy = [x*y for x, y in zip(xs, ys)]\n",
    "            xx = [x*y for x, y in zip(xs, xs)]\n",
    "            up = sum(xy) - len(xs)*xm*ym\n",
    "            down = sum(xx) - len(xs)*xm*xm\n",
    "            if down == 0:\n",
    "                return 0\n",
    "            return up*1.0/down\n",
    "        tmp['trend_of_frequent_in_the_past'] = tmp.apply(get_trend, axis=1)\n",
    "        # mean km distance:\n",
    "    else:\n",
    "        # set features to NaN\n",
    "        tmp['times'] = 1\n",
    "        tmp['time_distance_from_last'] = pd.NaT\n",
    "        tmp['time_distance_to_next'] = pd.NaT\n",
    "        tmp['mean_time_distance_in_group'] = pd.NaT\n",
    "        tmp['max_time_distance_in_group'] = pd.NaT\n",
    "        tmp['min_time_distance_in_group'] = pd.NaT\n",
    "        tmp['km_distance_from_last'] = np.NaN\n",
    "        tmp['km_distance_to_next'] = np.NaN\n",
    "        tmp['mean_km_distance_in_group'] = np.NaN\n",
    "        tmp['max_km_distance_in_group'] = np.NaN\n",
    "        tmp['min_km_distance_in_group'] = np.NaN\n",
    "        tmp['frequent_in_2014'] = 1 if tmp.iloc[0, tmp.columns.get_loc('year')] == '2014' else 0\n",
    "        tmp['frequent_in_2015'] = 1 if tmp.iloc[0, tmp.columns.get_loc('year')] == '2015' else 0\n",
    "        tmp['frequent_in_2016'] = 1 if tmp.iloc[0, tmp.columns.get_loc('year')] == '2016' else 0\n",
    "        tmp['frequent_in_2017'] = 1 if tmp.iloc[0, tmp.columns.get_loc('year')] == '2017' else 0\n",
    "        tmp['frequent_in_2018'] = 1 if tmp.iloc[0, tmp.columns.get_loc('year')] == '2018' else 0\n",
    "        tmp['num_act_shift1'] = np.NaN\n",
    "        tmp['num_teile_shift1'] = np.NaN\n",
    "        tmp['total_aw_shift1'] = np.NaN\n",
    "        tmp['num_act_shift2'] = np.NaN\n",
    "        tmp['num_teile_shift2'] = np.NaN\n",
    "        tmp['total_aw_shift2'] = np.NaN\n",
    "        tmp['time_distance_shift1'] = pd.NaT\n",
    "        tmp['time_distance_shift2'] = pd.NaT\n",
    "        tmp['time_distance_shift3'] = pd.NaT\n",
    "        tmp['time_distance_from_last2'] = pd.NaT\n",
    "        tmp['time_distance_from_last3'] = pd.NaT\n",
    "        tmp['time_distance_from_last4'] = pd.NaT\n",
    "        tmp['weather_come_in_last_1_month'] = False\n",
    "        tmp['weather_come_in_last_2_month'] = False\n",
    "        tmp['weather_come_in_last_3_month'] = False\n",
    "        tmp['weather_come_in_last_4_month'] = False\n",
    "        tmp['weather_come_in_last_5_month'] = False\n",
    "        tmp['weather_come_in_last_6_month'] = False\n",
    "        tmp['trend_of_frequent_in_the_past'] = 0\n",
    "    out.append(tmp)\n",
    "\n",
    "# i don't known why the warning appear??? try to figure it out, it can save lots of time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df3 = pd.concat(out).reset_index().iloc[:, 1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#df3 = pd.read_pickle('../code/tmp.pkl')\n",
    "df3 = pd.read_pickle(\"../code/ec1.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df3 = df3.reset_index().iloc[:, 1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tmp' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-72-afb2458cef47>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# test code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# create feature: time distance from last coming\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtime_shift_forward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSeries\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2010\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtmp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Auftragsdatum'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mtime_shift_forward\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtmp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtmp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'time_distance_from_last'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtmp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Auftragsdatum'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mtime_shift_forward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tmp' is not defined"
     ]
    }
   ],
   "source": [
    "# test code\n",
    "# create feature: time distance from last coming\n",
    "time_shift_forward = pd.concat([pd.Series(datetime(2010, 1, 1)), tmp['Auftragsdatum'][:-1]])\n",
    "time_shift_forward.index = tmp.index\n",
    "tmp['time_distance_from_last'] = tmp['Auftragsdatum'] - time_shift_forward\n",
    "# create feature: time distance to next coming\n",
    "time_distance_to_next = pd.concat([tmp['time_distance_from_last'][1:], pd.Series(np.NaN)])\n",
    "time_distance_to_next.index = tmp.index\n",
    "tmp['time_distance_to_next'] = time_distance_to_next\n",
    "# reset the first value and last value\n",
    "tmp.iloc[0, tmp.columns.get_loc('time_distance_from_last')] = np.NaN\n",
    "tmp.iloc[-1, tmp.columns.get_loc('time_distance_to_next')] = np.NaN\n",
    "# create feature: km stand distance from last coming\n",
    "km_shift_forward = pd.concat([pd.Series(0.0), tmp['KM-Stand'][:-1]])\n",
    "km_shift_forward.index = tmp.index\n",
    "tmp['km_distance_from_last'] = tmp['KM-Stand'] -km_shift_forward\n",
    "# create feature: km stand distance to next coming\n",
    "km_distance_to_next = pd.concat([tmp['km_distance_from_last'][1:], pd.Series(0.0)])\n",
    "km_distance_to_next.index = tmp.index\n",
    "tmp['km_distance_to_next'] = km_distance_to_next\n",
    "# reset the first value and last value\n",
    "tmp.iloc[0, tmp.columns.get_loc('km_distance_from_last')] = np.NaN\n",
    "tmp.iloc[-1, tmp.columns.get_loc('km_distance_to_next')] = np.NaN\n",
    "# create feature: frequent in each year\n",
    "year_candidates = ['2014', '2015', '2016', '2017', '2018']\n",
    "for year in year_candidates:\n",
    "    tmp['frequent_in_'+year] = len(tmp[tmp['year'] == year])\n",
    "# create feature: time shift for num_act\n",
    "num_act_shift_forward  = pd.concat([pd.Series(np.NaN), tmp['num_act'][:-1]])\n",
    "num_act_shift_forward.index = tmp.index\n",
    "tmp['num_act_shift1'] = num_act_shift_forward\n",
    "# create feature: time shift for num_teile\n",
    "num_teile_shift_forward  = pd.concat([pd.Series(np.NaN), tmp['num_teile'][:-1]])\n",
    "num_teile_shift_forward.index = tmp.index\n",
    "tmp['num_teile_shift1'] = num_teile_shift_forward\n",
    "# create feature: time shift for total aw\n",
    "total_aw_shift_forward  = pd.concat([pd.Series(np.NaN), tmp['total AW'][:-1]])\n",
    "total_aw_shift_forward.index = tmp.index\n",
    "tmp['total_aw_shift1'] = total_aw_shift_forward\n",
    "if len(tmp) > 2:\n",
    "    num_act_shift_forward2 = pd.concat([pd.Series([np.NaN, np.NaN]), tmp['num_act'][:-2]])\n",
    "    num_act_shift_forward2.index = tmp.index\n",
    "    tmp['num_act_shift2'] = num_act_shift_forward2\n",
    "    num_teile_shift_forward2 = pd.concat([pd.Series([np.NaN, np.NaN]), tmp['num_teile'][:-2]])\n",
    "    num_teile_shift_forward2.index = tmp.index\n",
    "    tmp['num_teile_shift2'] = num_teile_shift_forward2\n",
    "    total_aw_shift_forward2 = pd.concat([pd.Series([np.NaN, np.NaN]), tmp['total AW'][:-2]])\n",
    "    total_aw_shift_forward2.index = tmp.index\n",
    "    tmp['total_aw_shift2'] = total_aw_shift_forward2\n",
    "else:\n",
    "    tmp['num_act_shift2'] = np.NaN\n",
    "    tmp['num_teile_shift2'] = np.NaN\n",
    "    tmp['total_aw_shift2'] = np.NaN\n",
    "# create feature: repair age in year or in month\n",
    "tmp['first_date'] = tmp.iloc[0, tmp.columns.get_loc('Auftragsdatum')]\n",
    "tmp['repair_age'] = tmp['Auftragsdatum'] - tmp['first_date']\n",
    "tmp = tmp.drop('first_date', axis=1)\n",
    "# create feature: repair frequent in last X month\n",
    "time_distance_shift1 = pd.concat([pd.Series(np.NaN), tmp['time_distance_from_last'][:-1]])\n",
    "time_distance_shift1.index = tmp.index\n",
    "tmp['time_distance_shift1'] = time_distance_shift1\n",
    "time_distance_shift2 = pd.concat([pd.Series(np.NaN), tmp['time_distance_shift1'][:-1]])\n",
    "time_distance_shift2.index = tmp.index\n",
    "tmp['time_distance_shift2'] = time_distance_shift2\n",
    "time_distance_shift3 = pd.concat([pd.Series(np.NaN), tmp['time_distance_shift2'][:-1]])\n",
    "time_distance_shift3.index = tmp.index\n",
    "tmp['time_distance_shift3'] = time_distance_shift3\n",
    "tmp['time_distance_from_last2'] = tmp['time_distance_from_last'] + tmp['time_distance_shift1']\n",
    "tmp['time_distance_from_last3'] = tmp['time_distance_from_last2'] + tmp['time_distance_shift2']\n",
    "tmp['time_distance_from_last4'] = tmp['time_distance_from_last3'] + tmp['time_distance_shift3']\n",
    "def get_frequent(x, d = 30):\n",
    "    if x['time_distance_from_last4'] < timedelta(days = d):\n",
    "        return 4\n",
    "    elif x['time_distance_from_last3'] < timedelta(days = d):\n",
    "        return 3\n",
    "    elif x['time_distance_from_last2'] < timedelta(days = d):\n",
    "        return 2\n",
    "    elif x['time_distance_from_last'] < timedelta(days = d):\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "tmp['frequent_in_last_1_month'] = tmp.apply(get_frequent, axis=1, args = [1*30])\n",
    "tmp['frequent_in_last_2_month'] = tmp.apply(get_frequent, axis=1, args = [2*30])\n",
    "tmp['frequent_in_last_3_month'] = tmp.apply(get_frequent, axis=1, args = [3*30])\n",
    "tmp['frequent_in_last_4_month'] = tmp.apply(get_frequent, axis=1, args = [4*30])\n",
    "tmp['frequent_in_last_5_month'] = tmp.apply(get_frequent, axis=1, args = [5*30])\n",
    "tmp['frequent_in_last_6_month'] = tmp.apply(get_frequent, axis=1, args = [6*30])\n",
    "# create feature: weather come in last X month\n",
    "tmp['weather_come_in_last_1_month'] = tmp['time_distance_from_last'] < timedelta(days = 1*30)\n",
    "tmp['weather_come_in_last_2_month'] = tmp['time_distance_from_last'] < timedelta(days = 2*30)\n",
    "tmp['weather_come_in_last_3_month'] = tmp['time_distance_from_last'] < timedelta(days = 3*30)\n",
    "tmp['weather_come_in_last_4_month'] = tmp['time_distance_from_last'] < timedelta(days = 4*30)\n",
    "tmp['weather_come_in_last_5_month'] = tmp['time_distance_from_last'] < timedelta(days = 5*30)\n",
    "tmp['weather_come_in_last_6_month'] = tmp['time_distance_from_last'] < timedelta(days = 6*30)\n",
    "# create feature: trend of frequent in the past\n",
    "def get_trend(x):\n",
    "    xs = x[['time_distance_from_last', 'time_distance_shift1', 'time_distance_shift2', 'time_distance_shift3']].tolist()\n",
    "    xs = [x.days for x in xs if not pd.isnull(x)]\n",
    "    if len(xs) < 2:\n",
    "        return 0\n",
    "    ys = range(1, len(xs) + 1)\n",
    "    xm = sum(xs)/len(xs)\n",
    "    ym = sum(ys)/len(ys)\n",
    "    xy = [x*y for x, y in zip(xs, ys)]\n",
    "    xx = [x*y for x, y in zip(xs, xs)]\n",
    "    up = sum(xy) - len(xs)*xm*ym\n",
    "    down = sum(xx) - len(xs)*xm*xm\n",
    "    if down == 0:\n",
    "        return 0\n",
    "    return up*1.0/down\n",
    "tmp['trend_of_frequent_in_the_past'] = tmp.apply(get_trend, axis=1)\n",
    "# create feature: weather come in X month again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tmp' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-76-783a45687f37>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mdel\u001b[0m \u001b[0mtmp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mgc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tmp' is not defined"
     ]
    }
   ],
   "source": [
    "del tmp\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 77298 entries, 0 to 77297\n",
      "Data columns (total 49 columns):\n",
      "Auftragsdatum                    77298 non-null datetime64[ns]\n",
      "Fahrgestellnummer                77298 non-null object\n",
      "KM-Stand                         77298 non-null float64\n",
      "frequent_in_2014                 77298 non-null int64\n",
      "frequent_in_2015                 77298 non-null int64\n",
      "frequent_in_2016                 77298 non-null int64\n",
      "frequent_in_2017                 77298 non-null int64\n",
      "frequent_in_2018                 77298 non-null int64\n",
      "frequent_in_last_1_month         69279 non-null float64\n",
      "frequent_in_last_2_month         69279 non-null float64\n",
      "frequent_in_last_3_month         69279 non-null float64\n",
      "frequent_in_last_4_month         69279 non-null float64\n",
      "frequent_in_last_5_month         69279 non-null float64\n",
      "frequent_in_last_6_month         69279 non-null float64\n",
      "km_distance_from_last            55299 non-null float64\n",
      "km_distance_to_next              55299 non-null float64\n",
      "max_km_distance_in_group         69279 non-null float64\n",
      "max_time_distance_in_group       69279 non-null object\n",
      "mean_km_distance_in_group        69279 non-null float64\n",
      "mean_time_distance_in_group      69279 non-null object\n",
      "min_km_distance_in_group         69279 non-null float64\n",
      "min_time_distance_in_group       69279 non-null object\n",
      "num_act                          77298 non-null int64\n",
      "num_act_shift1                   55299 non-null float64\n",
      "num_act_shift2                   41319 non-null float64\n",
      "num_teile                        77298 non-null int64\n",
      "num_teile_shift1                 55299 non-null float64\n",
      "num_teile_shift2                 41319 non-null float64\n",
      "repair_age                       69279 non-null timedelta64[ns]\n",
      "time_distance_from_last          55299 non-null object\n",
      "time_distance_from_last2         41319 non-null object\n",
      "time_distance_from_last3         31194 non-null object\n",
      "time_distance_from_last4         23580 non-null object\n",
      "time_distance_shift1             41319 non-null object\n",
      "time_distance_shift2             31194 non-null object\n",
      "time_distance_shift3             23580 non-null object\n",
      "time_distance_to_next            55299 non-null object\n",
      "times                            77298 non-null int64\n",
      "total AW                         77298 non-null float64\n",
      "total_aw_shift1                  55299 non-null float64\n",
      "total_aw_shift2                  41319 non-null float64\n",
      "trend_of_frequent_in_the_past    77298 non-null float64\n",
      "weather_come_in_last_1_month     77298 non-null bool\n",
      "weather_come_in_last_2_month     77298 non-null bool\n",
      "weather_come_in_last_3_month     77298 non-null bool\n",
      "weather_come_in_last_4_month     77298 non-null bool\n",
      "weather_come_in_last_5_month     77298 non-null bool\n",
      "weather_come_in_last_6_month     77298 non-null bool\n",
      "year                             77298 non-null object\n",
      "dtypes: bool(6), datetime64[ns](1), float64(20), int64(8), object(13), timedelta64[ns](1)\n",
      "memory usage: 25.8+ MB\n"
     ]
    }
   ],
   "source": [
    "df3.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## extract feature, weather a customer will come in X months for special Gruppe-Nr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tg = df3.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tg['weather_come_in_1_month'] = tg['time_distance_to_next'] < timedelta(days = 1*30)\n",
    "tg['weather_come_in_2_month'] = tg['time_distance_to_next'] < timedelta(days = 2*30)\n",
    "tg['weather_come_in_3_month'] = tg['time_distance_to_next'] < timedelta(days = 3*30)\n",
    "tg['weather_come_in_4_month'] = tg['time_distance_to_next'] < timedelta(days = 4*30)\n",
    "tg['weather_come_in_5_month'] = tg['time_distance_to_next'] < timedelta(days = 5*30)\n",
    "tg['weather_come_in_6_month'] = tg['time_distance_to_next'] < timedelta(days = 6*30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Auftragsdatum</th>\n",
       "      <th>Fahrgestellnummer</th>\n",
       "      <th>KM-Stand</th>\n",
       "      <th>frequent_in_2014</th>\n",
       "      <th>frequent_in_2015</th>\n",
       "      <th>frequent_in_2016</th>\n",
       "      <th>frequent_in_2017</th>\n",
       "      <th>frequent_in_2018</th>\n",
       "      <th>frequent_in_last_1_month</th>\n",
       "      <th>frequent_in_last_2_month</th>\n",
       "      <th>frequent_in_last_3_month</th>\n",
       "      <th>frequent_in_last_4_month</th>\n",
       "      <th>frequent_in_last_5_month</th>\n",
       "      <th>frequent_in_last_6_month</th>\n",
       "      <th>km_distance_from_last</th>\n",
       "      <th>km_distance_to_next</th>\n",
       "      <th>max_km_distance_in_group</th>\n",
       "      <th>max_time_distance_in_group</th>\n",
       "      <th>mean_km_distance_in_group</th>\n",
       "      <th>mean_time_distance_in_group</th>\n",
       "      <th>min_km_distance_in_group</th>\n",
       "      <th>min_time_distance_in_group</th>\n",
       "      <th>num_act</th>\n",
       "      <th>num_act_shift1</th>\n",
       "      <th>num_act_shift2</th>\n",
       "      <th>num_teile</th>\n",
       "      <th>num_teile_shift1</th>\n",
       "      <th>num_teile_shift2</th>\n",
       "      <th>repair_age</th>\n",
       "      <th>time_distance_from_last</th>\n",
       "      <th>time_distance_from_last2</th>\n",
       "      <th>time_distance_from_last3</th>\n",
       "      <th>time_distance_from_last4</th>\n",
       "      <th>time_distance_shift1</th>\n",
       "      <th>time_distance_shift2</th>\n",
       "      <th>time_distance_shift3</th>\n",
       "      <th>time_distance_to_next</th>\n",
       "      <th>times</th>\n",
       "      <th>total AW</th>\n",
       "      <th>total_aw_shift1</th>\n",
       "      <th>total_aw_shift2</th>\n",
       "      <th>trend_of_frequent_in_the_past</th>\n",
       "      <th>weather_come_in_last_1_month</th>\n",
       "      <th>weather_come_in_last_2_month</th>\n",
       "      <th>weather_come_in_last_3_month</th>\n",
       "      <th>weather_come_in_last_4_month</th>\n",
       "      <th>weather_come_in_last_5_month</th>\n",
       "      <th>weather_come_in_last_6_month</th>\n",
       "      <th>year</th>\n",
       "      <th>weather_come_in_1_month</th>\n",
       "      <th>weather_come_in_2_month</th>\n",
       "      <th>weather_come_in_3_month</th>\n",
       "      <th>weather_come_in_4_month</th>\n",
       "      <th>weather_come_in_5_month</th>\n",
       "      <th>weather_come_in_6_month</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2014-05-20</td>\n",
       "      <td>------AU-00201251</td>\n",
       "      <td>266265.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>16</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>2014</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2015-08-04</td>\n",
       "      <td>159247535XXXXXXXX</td>\n",
       "      <td>87944.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>42</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>1</td>\n",
       "      <td>6.920609e+08</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>2015</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2014-07-26</td>\n",
       "      <td>1C3ANE9L24X016248</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>2014</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2014-10-20</td>\n",
       "      <td>1C8FYN8O63T585854</td>\n",
       "      <td>107950.0</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2192.0</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>365 days 00:00:00</td>\n",
       "      <td>3536.250000</td>\n",
       "      <td>158 days 18:00:00</td>\n",
       "      <td>587.0</td>\n",
       "      <td>23 days 00:00:00</td>\n",
       "      <td>10</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0 days</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>53 days 00:00:00</td>\n",
       "      <td>8</td>\n",
       "      <td>2.506427e+08</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>2014</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2014-12-12</td>\n",
       "      <td>1C8FYN8O63T585854</td>\n",
       "      <td>110142.0</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2192.0</td>\n",
       "      <td>636.0</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>365 days 00:00:00</td>\n",
       "      <td>3536.250000</td>\n",
       "      <td>158 days 18:00:00</td>\n",
       "      <td>587.0</td>\n",
       "      <td>23 days 00:00:00</td>\n",
       "      <td>8</td>\n",
       "      <td>10.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>53 days</td>\n",
       "      <td>53 days 00:00:00</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>52 days 00:00:00</td>\n",
       "      <td>8</td>\n",
       "      <td>7.282570e+07</td>\n",
       "      <td>2.506427e+08</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>2014</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2015-02-02</td>\n",
       "      <td>1C8FYN8O63T585854</td>\n",
       "      <td>110778.0</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>636.0</td>\n",
       "      <td>587.0</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>365 days 00:00:00</td>\n",
       "      <td>3536.250000</td>\n",
       "      <td>158 days 18:00:00</td>\n",
       "      <td>587.0</td>\n",
       "      <td>23 days 00:00:00</td>\n",
       "      <td>14</td>\n",
       "      <td>8.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>6</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>105 days</td>\n",
       "      <td>52 days 00:00:00</td>\n",
       "      <td>105 days 00:00:00</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>53 days 00:00:00</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>23 days 00:00:00</td>\n",
       "      <td>8</td>\n",
       "      <td>5.437718e+07</td>\n",
       "      <td>7.282570e+07</td>\n",
       "      <td>2.506427e+08</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>2015</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2015-02-25</td>\n",
       "      <td>1C8FYN8O63T585854</td>\n",
       "      <td>111365.0</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>587.0</td>\n",
       "      <td>1565.0</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>365 days 00:00:00</td>\n",
       "      <td>3536.250000</td>\n",
       "      <td>158 days 18:00:00</td>\n",
       "      <td>587.0</td>\n",
       "      <td>23 days 00:00:00</td>\n",
       "      <td>4</td>\n",
       "      <td>14.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>2</td>\n",
       "      <td>6.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>128 days</td>\n",
       "      <td>23 days 00:00:00</td>\n",
       "      <td>75 days 00:00:00</td>\n",
       "      <td>128 days 00:00:00</td>\n",
       "      <td>NaT</td>\n",
       "      <td>52 days 00:00:00</td>\n",
       "      <td>53 days 00:00:00</td>\n",
       "      <td>NaT</td>\n",
       "      <td>117 days 00:00:00</td>\n",
       "      <td>8</td>\n",
       "      <td>8.299390e+07</td>\n",
       "      <td>5.437718e+07</td>\n",
       "      <td>7.282570e+07</td>\n",
       "      <td>0.051665</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>2015</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2015-06-22</td>\n",
       "      <td>1C8FYN8O63T585854</td>\n",
       "      <td>112930.0</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1565.0</td>\n",
       "      <td>8010.0</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>365 days 00:00:00</td>\n",
       "      <td>3536.250000</td>\n",
       "      <td>158 days 18:00:00</td>\n",
       "      <td>587.0</td>\n",
       "      <td>23 days 00:00:00</td>\n",
       "      <td>5</td>\n",
       "      <td>4.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>245 days</td>\n",
       "      <td>117 days 00:00:00</td>\n",
       "      <td>140 days 00:00:00</td>\n",
       "      <td>192 days 00:00:00</td>\n",
       "      <td>245 days 00:00:00</td>\n",
       "      <td>23 days 00:00:00</td>\n",
       "      <td>52 days 00:00:00</td>\n",
       "      <td>53 days 00:00:00</td>\n",
       "      <td>354 days 00:00:00</td>\n",
       "      <td>8</td>\n",
       "      <td>1.740418e+08</td>\n",
       "      <td>8.299390e+07</td>\n",
       "      <td>5.437718e+07</td>\n",
       "      <td>-0.017250</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>2015</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2016-06-10</td>\n",
       "      <td>1C8FYN8O63T585854</td>\n",
       "      <td>120940.0</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8010.0</td>\n",
       "      <td>1138.0</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>365 days 00:00:00</td>\n",
       "      <td>3536.250000</td>\n",
       "      <td>158 days 18:00:00</td>\n",
       "      <td>587.0</td>\n",
       "      <td>23 days 00:00:00</td>\n",
       "      <td>10</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>599 days</td>\n",
       "      <td>354 days 00:00:00</td>\n",
       "      <td>471 days 00:00:00</td>\n",
       "      <td>494 days 00:00:00</td>\n",
       "      <td>546 days 00:00:00</td>\n",
       "      <td>117 days 00:00:00</td>\n",
       "      <td>23 days 00:00:00</td>\n",
       "      <td>52 days 00:00:00</td>\n",
       "      <td>123 days 00:00:00</td>\n",
       "      <td>8</td>\n",
       "      <td>1.449721e+08</td>\n",
       "      <td>1.740418e+08</td>\n",
       "      <td>8.299390e+07</td>\n",
       "      <td>-0.007385</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>2016</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2016-10-11</td>\n",
       "      <td>1C8FYN8O63T585854</td>\n",
       "      <td>122078.0</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1138.0</td>\n",
       "      <td>4162.0</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>365 days 00:00:00</td>\n",
       "      <td>3536.250000</td>\n",
       "      <td>158 days 18:00:00</td>\n",
       "      <td>587.0</td>\n",
       "      <td>23 days 00:00:00</td>\n",
       "      <td>15</td>\n",
       "      <td>10.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>722 days</td>\n",
       "      <td>123 days 00:00:00</td>\n",
       "      <td>477 days 00:00:00</td>\n",
       "      <td>594 days 00:00:00</td>\n",
       "      <td>617 days 00:00:00</td>\n",
       "      <td>354 days 00:00:00</td>\n",
       "      <td>117 days 00:00:00</td>\n",
       "      <td>23 days 00:00:00</td>\n",
       "      <td>183 days 00:00:00</td>\n",
       "      <td>8</td>\n",
       "      <td>3.492072e+08</td>\n",
       "      <td>1.449721e+08</td>\n",
       "      <td>1.740418e+08</td>\n",
       "      <td>-0.004513</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>2016</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2017-04-12</td>\n",
       "      <td>1C8FYN8O63T585854</td>\n",
       "      <td>126240.0</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4162.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>365 days 00:00:00</td>\n",
       "      <td>3536.250000</td>\n",
       "      <td>158 days 18:00:00</td>\n",
       "      <td>587.0</td>\n",
       "      <td>23 days 00:00:00</td>\n",
       "      <td>4</td>\n",
       "      <td>15.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>2</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>905 days</td>\n",
       "      <td>183 days 00:00:00</td>\n",
       "      <td>306 days 00:00:00</td>\n",
       "      <td>660 days 00:00:00</td>\n",
       "      <td>777 days 00:00:00</td>\n",
       "      <td>123 days 00:00:00</td>\n",
       "      <td>354 days 00:00:00</td>\n",
       "      <td>117 days 00:00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8</td>\n",
       "      <td>9.844455e+07</td>\n",
       "      <td>3.492072e+08</td>\n",
       "      <td>1.449721e+08</td>\n",
       "      <td>0.000450</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>2017</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2014-10-08</td>\n",
       "      <td>1G0MG35X28Y130800</td>\n",
       "      <td>77986.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>1</td>\n",
       "      <td>9.100526e+06</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>2014</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2014-01-02</td>\n",
       "      <td>1G0R86E41CU106188</td>\n",
       "      <td>13415.0</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>365 days 00:00:00</td>\n",
       "      <td>7188.400000</td>\n",
       "      <td>203 days 09:36:00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0 days 00:00:00</td>\n",
       "      <td>16</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0 days</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>287 days 00:00:00</td>\n",
       "      <td>5</td>\n",
       "      <td>6.008080e+05</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>2014</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2014-10-16</td>\n",
       "      <td>1G0R86E41CU106188</td>\n",
       "      <td>24271.0</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>8187.0</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>365 days 00:00:00</td>\n",
       "      <td>7188.400000</td>\n",
       "      <td>203 days 09:36:00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0 days 00:00:00</td>\n",
       "      <td>7</td>\n",
       "      <td>16.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>287 days</td>\n",
       "      <td>287 days 00:00:00</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>146 days 00:00:00</td>\n",
       "      <td>5</td>\n",
       "      <td>6.009330e+05</td>\n",
       "      <td>6.008080e+05</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>2014</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2015-03-11</td>\n",
       "      <td>1G0R86E41CU106188</td>\n",
       "      <td>32458.0</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>8187.0</td>\n",
       "      <td>7755.0</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>365 days 00:00:00</td>\n",
       "      <td>7188.400000</td>\n",
       "      <td>203 days 09:36:00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0 days 00:00:00</td>\n",
       "      <td>7</td>\n",
       "      <td>7.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>433 days</td>\n",
       "      <td>146 days 00:00:00</td>\n",
       "      <td>433 days 00:00:00</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>287 days 00:00:00</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>219 days 00:00:00</td>\n",
       "      <td>5</td>\n",
       "      <td>1.710782e+07</td>\n",
       "      <td>6.009330e+05</td>\n",
       "      <td>6.008080e+05</td>\n",
       "      <td>0.007092</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>2015</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2015-10-16</td>\n",
       "      <td>1G0R86E41CU106188</td>\n",
       "      <td>40213.0</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7755.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>365 days 00:00:00</td>\n",
       "      <td>7188.400000</td>\n",
       "      <td>203 days 09:36:00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0 days 00:00:00</td>\n",
       "      <td>15</td>\n",
       "      <td>7.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>652 days</td>\n",
       "      <td>219 days 00:00:00</td>\n",
       "      <td>365 days 00:00:00</td>\n",
       "      <td>652 days 00:00:00</td>\n",
       "      <td>NaT</td>\n",
       "      <td>146 days 00:00:00</td>\n",
       "      <td>287 days 00:00:00</td>\n",
       "      <td>NaT</td>\n",
       "      <td>0 days 00:00:00</td>\n",
       "      <td>5</td>\n",
       "      <td>6.141441e+08</td>\n",
       "      <td>1.710782e+07</td>\n",
       "      <td>6.009330e+05</td>\n",
       "      <td>0.006838</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>2015</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2015-10-16</td>\n",
       "      <td>1G0R86E41CU106188</td>\n",
       "      <td>40213.0</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>365 days 00:00:00</td>\n",
       "      <td>7188.400000</td>\n",
       "      <td>203 days 09:36:00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0 days 00:00:00</td>\n",
       "      <td>4</td>\n",
       "      <td>15.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>652 days</td>\n",
       "      <td>0 days 00:00:00</td>\n",
       "      <td>219 days 00:00:00</td>\n",
       "      <td>365 days 00:00:00</td>\n",
       "      <td>652 days 00:00:00</td>\n",
       "      <td>219 days 00:00:00</td>\n",
       "      <td>146 days 00:00:00</td>\n",
       "      <td>287 days 00:00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5</td>\n",
       "      <td>1.820254e+07</td>\n",
       "      <td>6.141441e+08</td>\n",
       "      <td>1.710782e+07</td>\n",
       "      <td>0.008684</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>2015</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2014-06-04</td>\n",
       "      <td>1G0R86E41CU114453</td>\n",
       "      <td>30456.0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.0</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>365 days 00:00:00</td>\n",
       "      <td>4523.250000</td>\n",
       "      <td>150 days 06:00:00</td>\n",
       "      <td>4.0</td>\n",
       "      <td>19 days 00:00:00</td>\n",
       "      <td>38</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0 days</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>19 days 00:00:00</td>\n",
       "      <td>4</td>\n",
       "      <td>1.312889e+10</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>2014</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2014-06-23</td>\n",
       "      <td>1G0R86E41CU114453</td>\n",
       "      <td>30452.0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>7469.0</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>365 days 00:00:00</td>\n",
       "      <td>4523.250000</td>\n",
       "      <td>150 days 06:00:00</td>\n",
       "      <td>4.0</td>\n",
       "      <td>19 days 00:00:00</td>\n",
       "      <td>2</td>\n",
       "      <td>38.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>19 days</td>\n",
       "      <td>19 days 00:00:00</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>169 days 00:00:00</td>\n",
       "      <td>4</td>\n",
       "      <td>8.012790e+05</td>\n",
       "      <td>1.312889e+10</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>2014</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2014-12-09</td>\n",
       "      <td>1G0R86E41CU114453</td>\n",
       "      <td>37921.0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7469.0</td>\n",
       "      <td>620.0</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>365 days 00:00:00</td>\n",
       "      <td>4523.250000</td>\n",
       "      <td>150 days 06:00:00</td>\n",
       "      <td>4.0</td>\n",
       "      <td>19 days 00:00:00</td>\n",
       "      <td>132</td>\n",
       "      <td>2.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>188 days</td>\n",
       "      <td>169 days 00:00:00</td>\n",
       "      <td>188 days 00:00:00</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>19 days 00:00:00</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>48 days 00:00:00</td>\n",
       "      <td>4</td>\n",
       "      <td>3.416196e+10</td>\n",
       "      <td>8.012790e+05</td>\n",
       "      <td>1.312889e+10</td>\n",
       "      <td>-0.006667</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>2014</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>2015-01-26</td>\n",
       "      <td>1G0R86E41CU114453</td>\n",
       "      <td>38541.0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>620.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>365 days 00:00:00</td>\n",
       "      <td>4523.250000</td>\n",
       "      <td>150 days 06:00:00</td>\n",
       "      <td>4.0</td>\n",
       "      <td>19 days 00:00:00</td>\n",
       "      <td>10</td>\n",
       "      <td>132.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>236 days</td>\n",
       "      <td>48 days 00:00:00</td>\n",
       "      <td>217 days 00:00:00</td>\n",
       "      <td>236 days 00:00:00</td>\n",
       "      <td>NaT</td>\n",
       "      <td>169 days 00:00:00</td>\n",
       "      <td>19 days 00:00:00</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "      <td>6.015440e+08</td>\n",
       "      <td>3.416196e+10</td>\n",
       "      <td>8.012790e+05</td>\n",
       "      <td>-0.002291</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>2015</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>2015-11-09</td>\n",
       "      <td>1G0R86E49CU107668</td>\n",
       "      <td>28772.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>44.0</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>365 days 00:00:00</td>\n",
       "      <td>5022.000000</td>\n",
       "      <td>182 days 12:00:00</td>\n",
       "      <td>44.0</td>\n",
       "      <td>0 days 00:00:00</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0 days</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0 days 00:00:00</td>\n",
       "      <td>2</td>\n",
       "      <td>1.820254e+07</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>2015</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>2015-11-09</td>\n",
       "      <td>1G0R86E49CU107668</td>\n",
       "      <td>28728.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>365 days 00:00:00</td>\n",
       "      <td>5022.000000</td>\n",
       "      <td>182 days 12:00:00</td>\n",
       "      <td>44.0</td>\n",
       "      <td>0 days 00:00:00</td>\n",
       "      <td>7</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0 days</td>\n",
       "      <td>0 days 00:00:00</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>1.502184e+07</td>\n",
       "      <td>1.820254e+07</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>2015</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>2015-05-15</td>\n",
       "      <td>1G0R86E49CU111686</td>\n",
       "      <td>18084.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>14</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>1</td>\n",
       "      <td>6.078441e+08</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>2015</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>2016-02-01</td>\n",
       "      <td>1VWCN7A30EC091435</td>\n",
       "      <td>31934.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>365 days 00:00:00</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>320 days 00:00:00</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>275 days 00:00:00</td>\n",
       "      <td>24</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0 days</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>275 days 00:00:00</td>\n",
       "      <td>2</td>\n",
       "      <td>4.623936e+08</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>2016</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>2016-11-02</td>\n",
       "      <td>1VWCN7A30EC091435</td>\n",
       "      <td>48047.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>365 days 00:00:00</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>320 days 00:00:00</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>275 days 00:00:00</td>\n",
       "      <td>17</td>\n",
       "      <td>24.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>275 days</td>\n",
       "      <td>275 days 00:00:00</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>3.766853e+08</td>\n",
       "      <td>4.623936e+08</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>2016</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>2017-05-24</td>\n",
       "      <td>30000000ANHÄNGER</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>1</td>\n",
       "      <td>1.964710e+08</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>2017</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>2018-05-15</td>\n",
       "      <td>3C4PFBCY3DT536122</td>\n",
       "      <td>132027.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>10</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>1</td>\n",
       "      <td>2.569950e+08</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>2018</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>2018-01-16</td>\n",
       "      <td>3LNHL2GC8AR607962</td>\n",
       "      <td>159155.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>1</td>\n",
       "      <td>1.040050e+06</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>2018</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>2014-11-12</td>\n",
       "      <td>3VW1931HLSM303335</td>\n",
       "      <td>246036.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>365 days 00:00:00</td>\n",
       "      <td>2500.000000</td>\n",
       "      <td>214 days 00:00:00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>40 days 00:00:00</td>\n",
       "      <td>9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0 days</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>40 days 00:00:00</td>\n",
       "      <td>4</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>2014</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77268</th>\n",
       "      <td>2014-07-02</td>\n",
       "      <td>WAUZZZ8P7CA085465</td>\n",
       "      <td>73687.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>365 days 00:00:00</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>323 days 12:00:00</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>282 days 00:00:00</td>\n",
       "      <td>46</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0 days</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>282 days 00:00:00</td>\n",
       "      <td>2</td>\n",
       "      <td>1.262346e+09</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>2014</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77269</th>\n",
       "      <td>2015-04-10</td>\n",
       "      <td>WAUZZZ8P7CA085465</td>\n",
       "      <td>98584.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>365 days 00:00:00</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>323 days 12:00:00</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>282 days 00:00:00</td>\n",
       "      <td>17</td>\n",
       "      <td>46.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>282 days</td>\n",
       "      <td>282 days 00:00:00</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>3.550296e+08</td>\n",
       "      <td>1.262346e+09</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>2015</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77270</th>\n",
       "      <td>2014-09-02</td>\n",
       "      <td>WAUZZZ8P7CA087085</td>\n",
       "      <td>30293.0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7749.0</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>365 days 00:00:00</td>\n",
       "      <td>9099.600000</td>\n",
       "      <td>202 days 04:48:00</td>\n",
       "      <td>7749.0</td>\n",
       "      <td>4 days 00:00:00</td>\n",
       "      <td>11</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0 days</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>100 days 00:00:00</td>\n",
       "      <td>5</td>\n",
       "      <td>2.756869e+08</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>2014</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77271</th>\n",
       "      <td>2014-12-11</td>\n",
       "      <td>WAUZZZ8P7CA087085</td>\n",
       "      <td>38042.0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7749.0</td>\n",
       "      <td>7749.0</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>365 days 00:00:00</td>\n",
       "      <td>9099.600000</td>\n",
       "      <td>202 days 04:48:00</td>\n",
       "      <td>7749.0</td>\n",
       "      <td>4 days 00:00:00</td>\n",
       "      <td>25</td>\n",
       "      <td>11.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>100 days</td>\n",
       "      <td>100 days 00:00:00</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4 days 00:00:00</td>\n",
       "      <td>5</td>\n",
       "      <td>7.435043e+08</td>\n",
       "      <td>2.756869e+08</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>2014</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77272</th>\n",
       "      <td>2014-12-15</td>\n",
       "      <td>WAUZZZ8P7CA087085</td>\n",
       "      <td>30293.0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>7749.0</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>365 days 00:00:00</td>\n",
       "      <td>9099.600000</td>\n",
       "      <td>202 days 04:48:00</td>\n",
       "      <td>7749.0</td>\n",
       "      <td>4 days 00:00:00</td>\n",
       "      <td>3</td>\n",
       "      <td>25.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>104 days</td>\n",
       "      <td>4 days 00:00:00</td>\n",
       "      <td>104 days 00:00:00</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>100 days 00:00:00</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>177 days 00:00:00</td>\n",
       "      <td>5</td>\n",
       "      <td>1.143133e+08</td>\n",
       "      <td>7.435043e+08</td>\n",
       "      <td>2.756869e+08</td>\n",
       "      <td>0.010417</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>2014</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77273</th>\n",
       "      <td>2015-06-10</td>\n",
       "      <td>WAUZZZ8P7CA087085</td>\n",
       "      <td>49753.0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>365 days 00:00:00</td>\n",
       "      <td>9099.600000</td>\n",
       "      <td>202 days 04:48:00</td>\n",
       "      <td>7749.0</td>\n",
       "      <td>4 days 00:00:00</td>\n",
       "      <td>44</td>\n",
       "      <td>3.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>281 days</td>\n",
       "      <td>177 days 00:00:00</td>\n",
       "      <td>181 days 00:00:00</td>\n",
       "      <td>281 days 00:00:00</td>\n",
       "      <td>NaT</td>\n",
       "      <td>4 days 00:00:00</td>\n",
       "      <td>100 days 00:00:00</td>\n",
       "      <td>NaT</td>\n",
       "      <td>365 days 00:00:00</td>\n",
       "      <td>5</td>\n",
       "      <td>1.180864e+09</td>\n",
       "      <td>1.143133e+08</td>\n",
       "      <td>7.435043e+08</td>\n",
       "      <td>-0.005125</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>2015</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77274</th>\n",
       "      <td>2017-10-18</td>\n",
       "      <td>WAUZZZ8P7CA087085</td>\n",
       "      <td>103745.0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>365 days 00:00:00</td>\n",
       "      <td>9099.600000</td>\n",
       "      <td>202 days 04:48:00</td>\n",
       "      <td>7749.0</td>\n",
       "      <td>4 days 00:00:00</td>\n",
       "      <td>6</td>\n",
       "      <td>44.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1142 days</td>\n",
       "      <td>365 days 00:00:00</td>\n",
       "      <td>542 days 00:00:00</td>\n",
       "      <td>546 days 00:00:00</td>\n",
       "      <td>646 days 00:00:00</td>\n",
       "      <td>177 days 00:00:00</td>\n",
       "      <td>4 days 00:00:00</td>\n",
       "      <td>100 days 00:00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5</td>\n",
       "      <td>7.704780e+07</td>\n",
       "      <td>1.180864e+09</td>\n",
       "      <td>1.143133e+08</td>\n",
       "      <td>-0.006891</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>2017</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77275</th>\n",
       "      <td>2016-12-29</td>\n",
       "      <td>WAUZZZ8P7CA091251</td>\n",
       "      <td>80027.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>15</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>1</td>\n",
       "      <td>1.537540e+08</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>2016</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77276</th>\n",
       "      <td>2015-01-29</td>\n",
       "      <td>WAUZZZ8P7CA092593</td>\n",
       "      <td>24398.0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>365 days 00:00:00</td>\n",
       "      <td>4458.571429</td>\n",
       "      <td>172 days 20:34:17.142857</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0 days 00:00:00</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0 days</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4 days 00:00:00</td>\n",
       "      <td>7</td>\n",
       "      <td>1.011400e+08</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>2015</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77277</th>\n",
       "      <td>2015-02-02</td>\n",
       "      <td>WAUZZZ8P7CA092593</td>\n",
       "      <td>24398.0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3739.0</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>365 days 00:00:00</td>\n",
       "      <td>4458.571429</td>\n",
       "      <td>172 days 20:34:17.142857</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0 days 00:00:00</td>\n",
       "      <td>3</td>\n",
       "      <td>5.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4 days</td>\n",
       "      <td>4 days 00:00:00</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>137 days 00:00:00</td>\n",
       "      <td>7</td>\n",
       "      <td>9.609190e+07</td>\n",
       "      <td>1.011400e+08</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>2015</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77278</th>\n",
       "      <td>2015-06-19</td>\n",
       "      <td>WAUZZZ8P7CA092593</td>\n",
       "      <td>28137.0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3739.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>365 days 00:00:00</td>\n",
       "      <td>4458.571429</td>\n",
       "      <td>172 days 20:34:17.142857</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0 days 00:00:00</td>\n",
       "      <td>7</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>141 days</td>\n",
       "      <td>137 days 00:00:00</td>\n",
       "      <td>141 days 00:00:00</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>4 days 00:00:00</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0 days 00:00:00</td>\n",
       "      <td>7</td>\n",
       "      <td>2.009209e+08</td>\n",
       "      <td>9.609190e+07</td>\n",
       "      <td>1.011400e+08</td>\n",
       "      <td>-0.007519</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>2015</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77279</th>\n",
       "      <td>2015-06-19</td>\n",
       "      <td>WAUZZZ8P7CA092593</td>\n",
       "      <td>28133.0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1138.0</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>365 days 00:00:00</td>\n",
       "      <td>4458.571429</td>\n",
       "      <td>172 days 20:34:17.142857</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0 days 00:00:00</td>\n",
       "      <td>103</td>\n",
       "      <td>7.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>141 days</td>\n",
       "      <td>0 days 00:00:00</td>\n",
       "      <td>137 days 00:00:00</td>\n",
       "      <td>141 days 00:00:00</td>\n",
       "      <td>NaT</td>\n",
       "      <td>137 days 00:00:00</td>\n",
       "      <td>4 days 00:00:00</td>\n",
       "      <td>NaT</td>\n",
       "      <td>77 days 00:00:00</td>\n",
       "      <td>7</td>\n",
       "      <td>2.883962e+09</td>\n",
       "      <td>2.009209e+08</td>\n",
       "      <td>9.609190e+07</td>\n",
       "      <td>0.000329</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>2015</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77280</th>\n",
       "      <td>2015-09-04</td>\n",
       "      <td>WAUZZZ8P7CA092593</td>\n",
       "      <td>29271.0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1138.0</td>\n",
       "      <td>6329.0</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>365 days 00:00:00</td>\n",
       "      <td>4458.571429</td>\n",
       "      <td>172 days 20:34:17.142857</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0 days 00:00:00</td>\n",
       "      <td>5</td>\n",
       "      <td>103.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>218 days</td>\n",
       "      <td>77 days 00:00:00</td>\n",
       "      <td>77 days 00:00:00</td>\n",
       "      <td>214 days 00:00:00</td>\n",
       "      <td>218 days 00:00:00</td>\n",
       "      <td>0 days 00:00:00</td>\n",
       "      <td>137 days 00:00:00</td>\n",
       "      <td>4 days 00:00:00</td>\n",
       "      <td>262 days 00:00:00</td>\n",
       "      <td>7</td>\n",
       "      <td>2.806735e+08</td>\n",
       "      <td>2.883962e+09</td>\n",
       "      <td>2.009209e+08</td>\n",
       "      <td>-0.003195</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>2015</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77281</th>\n",
       "      <td>2016-05-23</td>\n",
       "      <td>WAUZZZ8P7CA092593</td>\n",
       "      <td>35600.0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6329.0</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>365 days 00:00:00</td>\n",
       "      <td>4458.571429</td>\n",
       "      <td>172 days 20:34:17.142857</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0 days 00:00:00</td>\n",
       "      <td>7</td>\n",
       "      <td>5.0</td>\n",
       "      <td>103.0</td>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>480 days</td>\n",
       "      <td>262 days 00:00:00</td>\n",
       "      <td>339 days 00:00:00</td>\n",
       "      <td>339 days 00:00:00</td>\n",
       "      <td>476 days 00:00:00</td>\n",
       "      <td>77 days 00:00:00</td>\n",
       "      <td>0 days 00:00:00</td>\n",
       "      <td>137 days 00:00:00</td>\n",
       "      <td>365 days 00:00:00</td>\n",
       "      <td>7</td>\n",
       "      <td>1.011400e+08</td>\n",
       "      <td>2.806735e+08</td>\n",
       "      <td>2.883962e+09</td>\n",
       "      <td>-0.006158</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>2016</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77282</th>\n",
       "      <td>2018-07-07</td>\n",
       "      <td>WAUZZZ8P7CA092593</td>\n",
       "      <td>57106.0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>365 days 00:00:00</td>\n",
       "      <td>4458.571429</td>\n",
       "      <td>172 days 20:34:17.142857</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0 days 00:00:00</td>\n",
       "      <td>6</td>\n",
       "      <td>7.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1255 days</td>\n",
       "      <td>365 days 00:00:00</td>\n",
       "      <td>627 days 00:00:00</td>\n",
       "      <td>704 days 00:00:00</td>\n",
       "      <td>704 days 00:00:00</td>\n",
       "      <td>262 days 00:00:00</td>\n",
       "      <td>77 days 00:00:00</td>\n",
       "      <td>0 days 00:00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7</td>\n",
       "      <td>3.003192e+07</td>\n",
       "      <td>1.011400e+08</td>\n",
       "      <td>2.806735e+08</td>\n",
       "      <td>-0.007629</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>2018</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77283</th>\n",
       "      <td>2017-04-08</td>\n",
       "      <td>WAUZZZ8P7CA092948</td>\n",
       "      <td>57977.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>1</td>\n",
       "      <td>6.655190e+07</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>2017</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77284</th>\n",
       "      <td>2014-06-12</td>\n",
       "      <td>WAUZZZ8P7CA098152</td>\n",
       "      <td>29565.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>365 days 00:00:00</td>\n",
       "      <td>6600.500000</td>\n",
       "      <td>218 days 08:00:00</td>\n",
       "      <td>821.0</td>\n",
       "      <td>20 days 00:00:00</td>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0 days</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>321 days 00:00:00</td>\n",
       "      <td>6</td>\n",
       "      <td>1.526729e+08</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>2014</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77285</th>\n",
       "      <td>2015-04-29</td>\n",
       "      <td>WAUZZZ8P7CA098152</td>\n",
       "      <td>42102.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>857.0</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>365 days 00:00:00</td>\n",
       "      <td>6600.500000</td>\n",
       "      <td>218 days 08:00:00</td>\n",
       "      <td>821.0</td>\n",
       "      <td>20 days 00:00:00</td>\n",
       "      <td>7</td>\n",
       "      <td>7.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>321 days</td>\n",
       "      <td>321 days 00:00:00</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>31 days 00:00:00</td>\n",
       "      <td>6</td>\n",
       "      <td>2.304808e+08</td>\n",
       "      <td>1.526729e+08</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>2015</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77286</th>\n",
       "      <td>2015-05-30</td>\n",
       "      <td>WAUZZZ8P7CA098152</td>\n",
       "      <td>42959.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>857.0</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>365 days 00:00:00</td>\n",
       "      <td>6600.500000</td>\n",
       "      <td>218 days 08:00:00</td>\n",
       "      <td>821.0</td>\n",
       "      <td>20 days 00:00:00</td>\n",
       "      <td>10</td>\n",
       "      <td>7.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>352 days</td>\n",
       "      <td>31 days 00:00:00</td>\n",
       "      <td>352 days 00:00:00</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>321 days 00:00:00</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>365 days 00:00:00</td>\n",
       "      <td>6</td>\n",
       "      <td>5.375397e+07</td>\n",
       "      <td>2.304808e+08</td>\n",
       "      <td>1.526729e+08</td>\n",
       "      <td>0.003448</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>2015</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77287</th>\n",
       "      <td>2016-09-08</td>\n",
       "      <td>WAUZZZ8P7CA098152</td>\n",
       "      <td>61299.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>821.0</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>365 days 00:00:00</td>\n",
       "      <td>6600.500000</td>\n",
       "      <td>218 days 08:00:00</td>\n",
       "      <td>821.0</td>\n",
       "      <td>20 days 00:00:00</td>\n",
       "      <td>8</td>\n",
       "      <td>10.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>819 days</td>\n",
       "      <td>365 days 00:00:00</td>\n",
       "      <td>396 days 00:00:00</td>\n",
       "      <td>717 days 00:00:00</td>\n",
       "      <td>NaT</td>\n",
       "      <td>31 days 00:00:00</td>\n",
       "      <td>321 days 00:00:00</td>\n",
       "      <td>NaT</td>\n",
       "      <td>20 days 00:00:00</td>\n",
       "      <td>6</td>\n",
       "      <td>2.862720e+08</td>\n",
       "      <td>5.375397e+07</td>\n",
       "      <td>2.304808e+08</td>\n",
       "      <td>-0.000668</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>2016</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77288</th>\n",
       "      <td>2016-09-28</td>\n",
       "      <td>WAUZZZ8P7CA098152</td>\n",
       "      <td>62120.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>821.0</td>\n",
       "      <td>7925.0</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>365 days 00:00:00</td>\n",
       "      <td>6600.500000</td>\n",
       "      <td>218 days 08:00:00</td>\n",
       "      <td>821.0</td>\n",
       "      <td>20 days 00:00:00</td>\n",
       "      <td>8</td>\n",
       "      <td>8.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>839 days</td>\n",
       "      <td>20 days 00:00:00</td>\n",
       "      <td>385 days 00:00:00</td>\n",
       "      <td>416 days 00:00:00</td>\n",
       "      <td>737 days 00:00:00</td>\n",
       "      <td>365 days 00:00:00</td>\n",
       "      <td>31 days 00:00:00</td>\n",
       "      <td>321 days 00:00:00</td>\n",
       "      <td>208 days 00:00:00</td>\n",
       "      <td>6</td>\n",
       "      <td>2.204294e+08</td>\n",
       "      <td>2.862720e+08</td>\n",
       "      <td>5.375397e+07</td>\n",
       "      <td>0.002794</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>2016</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77289</th>\n",
       "      <td>2017-04-24</td>\n",
       "      <td>WAUZZZ8P7CA098152</td>\n",
       "      <td>70045.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7925.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>365 days 00:00:00</td>\n",
       "      <td>6600.500000</td>\n",
       "      <td>218 days 08:00:00</td>\n",
       "      <td>821.0</td>\n",
       "      <td>20 days 00:00:00</td>\n",
       "      <td>6</td>\n",
       "      <td>8.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1047 days</td>\n",
       "      <td>208 days 00:00:00</td>\n",
       "      <td>228 days 00:00:00</td>\n",
       "      <td>593 days 00:00:00</td>\n",
       "      <td>624 days 00:00:00</td>\n",
       "      <td>20 days 00:00:00</td>\n",
       "      <td>365 days 00:00:00</td>\n",
       "      <td>31 days 00:00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6</td>\n",
       "      <td>7.704780e+07</td>\n",
       "      <td>2.204294e+08</td>\n",
       "      <td>2.862720e+08</td>\n",
       "      <td>-0.001155</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>2017</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77290</th>\n",
       "      <td>2016-04-21</td>\n",
       "      <td>WAUZZZ8P7CA105990</td>\n",
       "      <td>85250.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>365 days 00:00:00</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>238 days 08:00:00</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>163 days 00:00:00</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0 days</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>187 days 00:00:00</td>\n",
       "      <td>3</td>\n",
       "      <td>9.999999e+07</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>2016</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77291</th>\n",
       "      <td>2016-10-25</td>\n",
       "      <td>WAUZZZ8P7CA105990</td>\n",
       "      <td>98000.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>365 days 00:00:00</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>238 days 08:00:00</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>163 days 00:00:00</td>\n",
       "      <td>2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>187 days</td>\n",
       "      <td>187 days 00:00:00</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>163 days 00:00:00</td>\n",
       "      <td>3</td>\n",
       "      <td>9.999999e+07</td>\n",
       "      <td>9.999999e+07</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>2016</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77292</th>\n",
       "      <td>2017-04-06</td>\n",
       "      <td>WAUZZZ8P7CA105990</td>\n",
       "      <td>108293.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>365 days 00:00:00</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>238 days 08:00:00</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>163 days 00:00:00</td>\n",
       "      <td>2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>350 days</td>\n",
       "      <td>163 days 00:00:00</td>\n",
       "      <td>350 days 00:00:00</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>187 days 00:00:00</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>9.999999e+07</td>\n",
       "      <td>9.999999e+07</td>\n",
       "      <td>9.999999e+07</td>\n",
       "      <td>0.041667</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>2017</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77293</th>\n",
       "      <td>2014-05-10</td>\n",
       "      <td>WAUZZZ8P7CA107139</td>\n",
       "      <td>28060.0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1333.0</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>365 days 00:00:00</td>\n",
       "      <td>6533.200000</td>\n",
       "      <td>215 days 14:24:00</td>\n",
       "      <td>1333.0</td>\n",
       "      <td>1 days 00:00:00</td>\n",
       "      <td>13</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0 days</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3 days 00:00:00</td>\n",
       "      <td>5</td>\n",
       "      <td>2.908894e+08</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>2014</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77294</th>\n",
       "      <td>2014-05-13</td>\n",
       "      <td>WAUZZZ8P7CA107139</td>\n",
       "      <td>26727.0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1333.0</td>\n",
       "      <td>1333.0</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>365 days 00:00:00</td>\n",
       "      <td>6533.200000</td>\n",
       "      <td>215 days 14:24:00</td>\n",
       "      <td>1333.0</td>\n",
       "      <td>1 days 00:00:00</td>\n",
       "      <td>4</td>\n",
       "      <td>13.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3 days</td>\n",
       "      <td>3 days 00:00:00</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1 days 00:00:00</td>\n",
       "      <td>5</td>\n",
       "      <td>1.665133e+08</td>\n",
       "      <td>2.908894e+08</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>2014</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77295</th>\n",
       "      <td>2014-05-14</td>\n",
       "      <td>WAUZZZ8P7CA107139</td>\n",
       "      <td>28060.0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1333.0</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>365 days 00:00:00</td>\n",
       "      <td>6533.200000</td>\n",
       "      <td>215 days 14:24:00</td>\n",
       "      <td>1333.0</td>\n",
       "      <td>1 days 00:00:00</td>\n",
       "      <td>19</td>\n",
       "      <td>4.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4 days</td>\n",
       "      <td>1 days 00:00:00</td>\n",
       "      <td>4 days 00:00:00</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>3 days 00:00:00</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>344 days 00:00:00</td>\n",
       "      <td>5</td>\n",
       "      <td>4.757098e+08</td>\n",
       "      <td>1.665133e+08</td>\n",
       "      <td>2.908894e+08</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>2014</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77296</th>\n",
       "      <td>2015-04-23</td>\n",
       "      <td>WAUZZZ8P7CA107139</td>\n",
       "      <td>41618.0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>365 days 00:00:00</td>\n",
       "      <td>6533.200000</td>\n",
       "      <td>215 days 14:24:00</td>\n",
       "      <td>1333.0</td>\n",
       "      <td>1 days 00:00:00</td>\n",
       "      <td>4</td>\n",
       "      <td>19.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>348 days</td>\n",
       "      <td>344 days 00:00:00</td>\n",
       "      <td>345 days 00:00:00</td>\n",
       "      <td>348 days 00:00:00</td>\n",
       "      <td>NaT</td>\n",
       "      <td>1 days 00:00:00</td>\n",
       "      <td>3 days 00:00:00</td>\n",
       "      <td>NaT</td>\n",
       "      <td>365 days 00:00:00</td>\n",
       "      <td>5</td>\n",
       "      <td>1.314107e+08</td>\n",
       "      <td>4.757098e+08</td>\n",
       "      <td>1.665133e+08</td>\n",
       "      <td>-0.004373</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>2015</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77297</th>\n",
       "      <td>2017-12-15</td>\n",
       "      <td>WAUZZZ8P7CA107139</td>\n",
       "      <td>73312.0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>365 days 00:00:00</td>\n",
       "      <td>6533.200000</td>\n",
       "      <td>215 days 14:24:00</td>\n",
       "      <td>1333.0</td>\n",
       "      <td>1 days 00:00:00</td>\n",
       "      <td>19</td>\n",
       "      <td>4.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1315 days</td>\n",
       "      <td>365 days 00:00:00</td>\n",
       "      <td>709 days 00:00:00</td>\n",
       "      <td>710 days 00:00:00</td>\n",
       "      <td>713 days 00:00:00</td>\n",
       "      <td>344 days 00:00:00</td>\n",
       "      <td>1 days 00:00:00</td>\n",
       "      <td>3 days 00:00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5</td>\n",
       "      <td>2.452708e+08</td>\n",
       "      <td>1.314107e+08</td>\n",
       "      <td>4.757098e+08</td>\n",
       "      <td>-0.005740</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>2017</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>77298 rows × 55 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Auftragsdatum  Fahrgestellnummer  KM-Stand  frequent_in_2014  \\\n",
       "0        2014-05-20  ------AU-00201251  266265.0                 1   \n",
       "1        2015-08-04  159247535XXXXXXXX   87944.0                 0   \n",
       "2        2014-07-26  1C3ANE9L24X016248       0.0                 1   \n",
       "3        2014-10-20  1C8FYN8O63T585854  107950.0                 2   \n",
       "4        2014-12-12  1C8FYN8O63T585854  110142.0                 2   \n",
       "5        2015-02-02  1C8FYN8O63T585854  110778.0                 2   \n",
       "6        2015-02-25  1C8FYN8O63T585854  111365.0                 2   \n",
       "7        2015-06-22  1C8FYN8O63T585854  112930.0                 2   \n",
       "8        2016-06-10  1C8FYN8O63T585854  120940.0                 2   \n",
       "9        2016-10-11  1C8FYN8O63T585854  122078.0                 2   \n",
       "10       2017-04-12  1C8FYN8O63T585854  126240.0                 2   \n",
       "11       2014-10-08  1G0MG35X28Y130800   77986.0                 1   \n",
       "12       2014-01-02  1G0R86E41CU106188   13415.0                 2   \n",
       "13       2014-10-16  1G0R86E41CU106188   24271.0                 2   \n",
       "14       2015-03-11  1G0R86E41CU106188   32458.0                 2   \n",
       "15       2015-10-16  1G0R86E41CU106188   40213.0                 2   \n",
       "16       2015-10-16  1G0R86E41CU106188   40213.0                 2   \n",
       "17       2014-06-04  1G0R86E41CU114453   30456.0                 3   \n",
       "18       2014-06-23  1G0R86E41CU114453   30452.0                 3   \n",
       "19       2014-12-09  1G0R86E41CU114453   37921.0                 3   \n",
       "20       2015-01-26  1G0R86E41CU114453   38541.0                 3   \n",
       "21       2015-11-09  1G0R86E49CU107668   28772.0                 0   \n",
       "22       2015-11-09  1G0R86E49CU107668   28728.0                 0   \n",
       "23       2015-05-15  1G0R86E49CU111686   18084.0                 0   \n",
       "24       2016-02-01  1VWCN7A30EC091435   31934.0                 0   \n",
       "25       2016-11-02  1VWCN7A30EC091435   48047.0                 0   \n",
       "26       2017-05-24   30000000ANHÄNGER       0.0                 0   \n",
       "27       2018-05-15  3C4PFBCY3DT536122  132027.0                 0   \n",
       "28       2018-01-16  3LNHL2GC8AR607962  159155.0                 0   \n",
       "29       2014-11-12  3VW1931HLSM303335  246036.0                 2   \n",
       "...             ...                ...       ...               ...   \n",
       "77268    2014-07-02  WAUZZZ8P7CA085465   73687.0                 1   \n",
       "77269    2015-04-10  WAUZZZ8P7CA085465   98584.0                 1   \n",
       "77270    2014-09-02  WAUZZZ8P7CA087085   30293.0                 3   \n",
       "77271    2014-12-11  WAUZZZ8P7CA087085   38042.0                 3   \n",
       "77272    2014-12-15  WAUZZZ8P7CA087085   30293.0                 3   \n",
       "77273    2015-06-10  WAUZZZ8P7CA087085   49753.0                 3   \n",
       "77274    2017-10-18  WAUZZZ8P7CA087085  103745.0                 3   \n",
       "77275    2016-12-29  WAUZZZ8P7CA091251   80027.0                 0   \n",
       "77276    2015-01-29  WAUZZZ8P7CA092593   24398.0                 0   \n",
       "77277    2015-02-02  WAUZZZ8P7CA092593   24398.0                 0   \n",
       "77278    2015-06-19  WAUZZZ8P7CA092593   28137.0                 0   \n",
       "77279    2015-06-19  WAUZZZ8P7CA092593   28133.0                 0   \n",
       "77280    2015-09-04  WAUZZZ8P7CA092593   29271.0                 0   \n",
       "77281    2016-05-23  WAUZZZ8P7CA092593   35600.0                 0   \n",
       "77282    2018-07-07  WAUZZZ8P7CA092593   57106.0                 0   \n",
       "77283    2017-04-08  WAUZZZ8P7CA092948   57977.0                 0   \n",
       "77284    2014-06-12  WAUZZZ8P7CA098152   29565.0                 1   \n",
       "77285    2015-04-29  WAUZZZ8P7CA098152   42102.0                 1   \n",
       "77286    2015-05-30  WAUZZZ8P7CA098152   42959.0                 1   \n",
       "77287    2016-09-08  WAUZZZ8P7CA098152   61299.0                 1   \n",
       "77288    2016-09-28  WAUZZZ8P7CA098152   62120.0                 1   \n",
       "77289    2017-04-24  WAUZZZ8P7CA098152   70045.0                 1   \n",
       "77290    2016-04-21  WAUZZZ8P7CA105990   85250.0                 0   \n",
       "77291    2016-10-25  WAUZZZ8P7CA105990   98000.0                 0   \n",
       "77292    2017-04-06  WAUZZZ8P7CA105990  108293.0                 0   \n",
       "77293    2014-05-10  WAUZZZ8P7CA107139   28060.0                 3   \n",
       "77294    2014-05-13  WAUZZZ8P7CA107139   26727.0                 3   \n",
       "77295    2014-05-14  WAUZZZ8P7CA107139   28060.0                 3   \n",
       "77296    2015-04-23  WAUZZZ8P7CA107139   41618.0                 3   \n",
       "77297    2017-12-15  WAUZZZ8P7CA107139   73312.0                 3   \n",
       "\n",
       "       frequent_in_2015  frequent_in_2016  frequent_in_2017  frequent_in_2018  \\\n",
       "0                     0                 0                 0                 0   \n",
       "1                     1                 0                 0                 0   \n",
       "2                     0                 0                 0                 0   \n",
       "3                     3                 2                 1                 0   \n",
       "4                     3                 2                 1                 0   \n",
       "5                     3                 2                 1                 0   \n",
       "6                     3                 2                 1                 0   \n",
       "7                     3                 2                 1                 0   \n",
       "8                     3                 2                 1                 0   \n",
       "9                     3                 2                 1                 0   \n",
       "10                    3                 2                 1                 0   \n",
       "11                    0                 0                 0                 0   \n",
       "12                    3                 0                 0                 0   \n",
       "13                    3                 0                 0                 0   \n",
       "14                    3                 0                 0                 0   \n",
       "15                    3                 0                 0                 0   \n",
       "16                    3                 0                 0                 0   \n",
       "17                    1                 0                 0                 0   \n",
       "18                    1                 0                 0                 0   \n",
       "19                    1                 0                 0                 0   \n",
       "20                    1                 0                 0                 0   \n",
       "21                    2                 0                 0                 0   \n",
       "22                    2                 0                 0                 0   \n",
       "23                    1                 0                 0                 0   \n",
       "24                    0                 2                 0                 0   \n",
       "25                    0                 2                 0                 0   \n",
       "26                    0                 0                 1                 0   \n",
       "27                    0                 0                 0                 1   \n",
       "28                    0                 0                 0                 1   \n",
       "29                    1                 0                 1                 0   \n",
       "...                 ...               ...               ...               ...   \n",
       "77268                 1                 0                 0                 0   \n",
       "77269                 1                 0                 0                 0   \n",
       "77270                 1                 0                 1                 0   \n",
       "77271                 1                 0                 1                 0   \n",
       "77272                 1                 0                 1                 0   \n",
       "77273                 1                 0                 1                 0   \n",
       "77274                 1                 0                 1                 0   \n",
       "77275                 0                 1                 0                 0   \n",
       "77276                 5                 1                 0                 1   \n",
       "77277                 5                 1                 0                 1   \n",
       "77278                 5                 1                 0                 1   \n",
       "77279                 5                 1                 0                 1   \n",
       "77280                 5                 1                 0                 1   \n",
       "77281                 5                 1                 0                 1   \n",
       "77282                 5                 1                 0                 1   \n",
       "77283                 0                 0                 1                 0   \n",
       "77284                 2                 2                 1                 0   \n",
       "77285                 2                 2                 1                 0   \n",
       "77286                 2                 2                 1                 0   \n",
       "77287                 2                 2                 1                 0   \n",
       "77288                 2                 2                 1                 0   \n",
       "77289                 2                 2                 1                 0   \n",
       "77290                 0                 2                 1                 0   \n",
       "77291                 0                 2                 1                 0   \n",
       "77292                 0                 2                 1                 0   \n",
       "77293                 1                 0                 1                 0   \n",
       "77294                 1                 0                 1                 0   \n",
       "77295                 1                 0                 1                 0   \n",
       "77296                 1                 0                 1                 0   \n",
       "77297                 1                 0                 1                 0   \n",
       "\n",
       "       frequent_in_last_1_month  frequent_in_last_2_month  \\\n",
       "0                           NaN                       NaN   \n",
       "1                           NaN                       NaN   \n",
       "2                           NaN                       NaN   \n",
       "3                           0.0                       0.0   \n",
       "4                           0.0                       1.0   \n",
       "5                           0.0                       1.0   \n",
       "6                           1.0                       1.0   \n",
       "7                           0.0                       0.0   \n",
       "8                           0.0                       0.0   \n",
       "9                           0.0                       0.0   \n",
       "10                          0.0                       0.0   \n",
       "11                          NaN                       NaN   \n",
       "12                          0.0                       0.0   \n",
       "13                          0.0                       0.0   \n",
       "14                          0.0                       0.0   \n",
       "15                          0.0                       0.0   \n",
       "16                          1.0                       1.0   \n",
       "17                          0.0                       0.0   \n",
       "18                          1.0                       1.0   \n",
       "19                          0.0                       0.0   \n",
       "20                          0.0                       1.0   \n",
       "21                          0.0                       0.0   \n",
       "22                          1.0                       1.0   \n",
       "23                          NaN                       NaN   \n",
       "24                          0.0                       0.0   \n",
       "25                          0.0                       0.0   \n",
       "26                          NaN                       NaN   \n",
       "27                          NaN                       NaN   \n",
       "28                          NaN                       NaN   \n",
       "29                          0.0                       0.0   \n",
       "...                         ...                       ...   \n",
       "77268                       0.0                       0.0   \n",
       "77269                       0.0                       0.0   \n",
       "77270                       0.0                       0.0   \n",
       "77271                       0.0                       0.0   \n",
       "77272                       1.0                       1.0   \n",
       "77273                       0.0                       0.0   \n",
       "77274                       0.0                       0.0   \n",
       "77275                       NaN                       NaN   \n",
       "77276                       0.0                       0.0   \n",
       "77277                       1.0                       1.0   \n",
       "77278                       0.0                       0.0   \n",
       "77279                       1.0                       1.0   \n",
       "77280                       0.0                       0.0   \n",
       "77281                       0.0                       0.0   \n",
       "77282                       0.0                       0.0   \n",
       "77283                       NaN                       NaN   \n",
       "77284                       0.0                       0.0   \n",
       "77285                       0.0                       0.0   \n",
       "77286                       0.0                       1.0   \n",
       "77287                       0.0                       0.0   \n",
       "77288                       1.0                       1.0   \n",
       "77289                       0.0                       0.0   \n",
       "77290                       0.0                       0.0   \n",
       "77291                       0.0                       0.0   \n",
       "77292                       0.0                       0.0   \n",
       "77293                       0.0                       0.0   \n",
       "77294                       1.0                       1.0   \n",
       "77295                       2.0                       2.0   \n",
       "77296                       0.0                       0.0   \n",
       "77297                       0.0                       0.0   \n",
       "\n",
       "       frequent_in_last_3_month  frequent_in_last_4_month  \\\n",
       "0                           NaN                       NaN   \n",
       "1                           NaN                       NaN   \n",
       "2                           NaN                       NaN   \n",
       "3                           0.0                       0.0   \n",
       "4                           1.0                       1.0   \n",
       "5                           1.0                       2.0   \n",
       "6                           2.0                       2.0   \n",
       "7                           0.0                       1.0   \n",
       "8                           0.0                       0.0   \n",
       "9                           0.0                       0.0   \n",
       "10                          0.0                       0.0   \n",
       "11                          NaN                       NaN   \n",
       "12                          0.0                       0.0   \n",
       "13                          0.0                       0.0   \n",
       "14                          0.0                       0.0   \n",
       "15                          0.0                       0.0   \n",
       "16                          1.0                       1.0   \n",
       "17                          0.0                       0.0   \n",
       "18                          1.0                       1.0   \n",
       "19                          0.0                       0.0   \n",
       "20                          1.0                       1.0   \n",
       "21                          0.0                       0.0   \n",
       "22                          1.0                       1.0   \n",
       "23                          NaN                       NaN   \n",
       "24                          0.0                       0.0   \n",
       "25                          0.0                       0.0   \n",
       "26                          NaN                       NaN   \n",
       "27                          NaN                       NaN   \n",
       "28                          NaN                       NaN   \n",
       "29                          0.0                       0.0   \n",
       "...                         ...                       ...   \n",
       "77268                       0.0                       0.0   \n",
       "77269                       0.0                       0.0   \n",
       "77270                       0.0                       0.0   \n",
       "77271                       0.0                       1.0   \n",
       "77272                       1.0                       2.0   \n",
       "77273                       0.0                       0.0   \n",
       "77274                       0.0                       0.0   \n",
       "77275                       NaN                       NaN   \n",
       "77276                       0.0                       0.0   \n",
       "77277                       1.0                       1.0   \n",
       "77278                       0.0                       0.0   \n",
       "77279                       1.0                       1.0   \n",
       "77280                       2.0                       2.0   \n",
       "77281                       0.0                       0.0   \n",
       "77282                       0.0                       0.0   \n",
       "77283                       NaN                       NaN   \n",
       "77284                       0.0                       0.0   \n",
       "77285                       0.0                       0.0   \n",
       "77286                       1.0                       1.0   \n",
       "77287                       0.0                       0.0   \n",
       "77288                       1.0                       1.0   \n",
       "77289                       0.0                       0.0   \n",
       "77290                       0.0                       0.0   \n",
       "77291                       0.0                       0.0   \n",
       "77292                       0.0                       0.0   \n",
       "77293                       0.0                       0.0   \n",
       "77294                       1.0                       1.0   \n",
       "77295                       2.0                       2.0   \n",
       "77296                       0.0                       0.0   \n",
       "77297                       0.0                       0.0   \n",
       "\n",
       "       frequent_in_last_5_month  frequent_in_last_6_month  \\\n",
       "0                           NaN                       NaN   \n",
       "1                           NaN                       NaN   \n",
       "2                           NaN                       NaN   \n",
       "3                           0.0                       0.0   \n",
       "4                           1.0                       1.0   \n",
       "5                           2.0                       2.0   \n",
       "6                           3.0                       3.0   \n",
       "7                           2.0                       2.0   \n",
       "8                           0.0                       0.0   \n",
       "9                           1.0                       1.0   \n",
       "10                          0.0                       0.0   \n",
       "11                          NaN                       NaN   \n",
       "12                          0.0                       0.0   \n",
       "13                          0.0                       0.0   \n",
       "14                          1.0                       1.0   \n",
       "15                          0.0                       0.0   \n",
       "16                          1.0                       1.0   \n",
       "17                          0.0                       0.0   \n",
       "18                          1.0                       1.0   \n",
       "19                          0.0                       1.0   \n",
       "20                          1.0                       1.0   \n",
       "21                          0.0                       0.0   \n",
       "22                          1.0                       1.0   \n",
       "23                          NaN                       NaN   \n",
       "24                          0.0                       0.0   \n",
       "25                          0.0                       0.0   \n",
       "26                          NaN                       NaN   \n",
       "27                          NaN                       NaN   \n",
       "28                          NaN                       NaN   \n",
       "29                          0.0                       0.0   \n",
       "...                         ...                       ...   \n",
       "77268                       0.0                       0.0   \n",
       "77269                       0.0                       0.0   \n",
       "77270                       0.0                       0.0   \n",
       "77271                       1.0                       1.0   \n",
       "77272                       2.0                       2.0   \n",
       "77273                       0.0                       1.0   \n",
       "77274                       0.0                       0.0   \n",
       "77275                       NaN                       NaN   \n",
       "77276                       0.0                       0.0   \n",
       "77277                       1.0                       1.0   \n",
       "77278                       2.0                       2.0   \n",
       "77279                       3.0                       3.0   \n",
       "77280                       2.0                       2.0   \n",
       "77281                       0.0                       0.0   \n",
       "77282                       0.0                       0.0   \n",
       "77283                       NaN                       NaN   \n",
       "77284                       0.0                       0.0   \n",
       "77285                       0.0                       0.0   \n",
       "77286                       1.0                       1.0   \n",
       "77287                       0.0                       0.0   \n",
       "77288                       1.0                       1.0   \n",
       "77289                       0.0                       0.0   \n",
       "77290                       0.0                       0.0   \n",
       "77291                       0.0                       0.0   \n",
       "77292                       0.0                       1.0   \n",
       "77293                       0.0                       0.0   \n",
       "77294                       1.0                       1.0   \n",
       "77295                       2.0                       2.0   \n",
       "77296                       0.0                       0.0   \n",
       "77297                       0.0                       0.0   \n",
       "\n",
       "       km_distance_from_last  km_distance_to_next  max_km_distance_in_group  \\\n",
       "0                        NaN                  NaN                       NaN   \n",
       "1                        NaN                  NaN                       NaN   \n",
       "2                        NaN                  NaN                       NaN   \n",
       "3                        NaN               2192.0                   10000.0   \n",
       "4                     2192.0                636.0                   10000.0   \n",
       "5                      636.0                587.0                   10000.0   \n",
       "6                      587.0               1565.0                   10000.0   \n",
       "7                     1565.0               8010.0                   10000.0   \n",
       "8                     8010.0               1138.0                   10000.0   \n",
       "9                     1138.0               4162.0                   10000.0   \n",
       "10                    4162.0                  NaN                   10000.0   \n",
       "11                       NaN                  NaN                       NaN   \n",
       "12                       NaN              10000.0                   10000.0   \n",
       "13                   10000.0               8187.0                   10000.0   \n",
       "14                    8187.0               7755.0                   10000.0   \n",
       "15                    7755.0                  0.0                   10000.0   \n",
       "16                       0.0                  NaN                   10000.0   \n",
       "17                       NaN                  4.0                   10000.0   \n",
       "18                       4.0               7469.0                   10000.0   \n",
       "19                    7469.0                620.0                   10000.0   \n",
       "20                     620.0                  NaN                   10000.0   \n",
       "21                       NaN                 44.0                   10000.0   \n",
       "22                      44.0                  NaN                   10000.0   \n",
       "23                       NaN                  NaN                       NaN   \n",
       "24                       NaN              10000.0                   10000.0   \n",
       "25                   10000.0                  NaN                   10000.0   \n",
       "26                       NaN                  NaN                       NaN   \n",
       "27                       NaN                  NaN                       NaN   \n",
       "28                       NaN                  NaN                       NaN   \n",
       "29                       NaN                  0.0                   10000.0   \n",
       "...                      ...                  ...                       ...   \n",
       "77268                    NaN              10000.0                   10000.0   \n",
       "77269                10000.0                  NaN                   10000.0   \n",
       "77270                    NaN               7749.0                   10000.0   \n",
       "77271                 7749.0               7749.0                   10000.0   \n",
       "77272                 7749.0              10000.0                   10000.0   \n",
       "77273                10000.0              10000.0                   10000.0   \n",
       "77274                10000.0                  NaN                   10000.0   \n",
       "77275                    NaN                  NaN                       NaN   \n",
       "77276                    NaN                  0.0                   10000.0   \n",
       "77277                    0.0               3739.0                   10000.0   \n",
       "77278                 3739.0                  4.0                   10000.0   \n",
       "77279                    4.0               1138.0                   10000.0   \n",
       "77280                 1138.0               6329.0                   10000.0   \n",
       "77281                 6329.0              10000.0                   10000.0   \n",
       "77282                10000.0                  NaN                   10000.0   \n",
       "77283                    NaN                  NaN                       NaN   \n",
       "77284                    NaN              10000.0                   10000.0   \n",
       "77285                10000.0                857.0                   10000.0   \n",
       "77286                  857.0              10000.0                   10000.0   \n",
       "77287                10000.0                821.0                   10000.0   \n",
       "77288                  821.0               7925.0                   10000.0   \n",
       "77289                 7925.0                  NaN                   10000.0   \n",
       "77290                    NaN              10000.0                   10000.0   \n",
       "77291                10000.0              10000.0                   10000.0   \n",
       "77292                10000.0                  NaN                   10000.0   \n",
       "77293                    NaN               1333.0                   10000.0   \n",
       "77294                 1333.0               1333.0                   10000.0   \n",
       "77295                 1333.0              10000.0                   10000.0   \n",
       "77296                10000.0              10000.0                   10000.0   \n",
       "77297                10000.0                  NaN                   10000.0   \n",
       "\n",
       "      max_time_distance_in_group  mean_km_distance_in_group  \\\n",
       "0                            NaT                        NaN   \n",
       "1                            NaT                        NaN   \n",
       "2                            NaT                        NaN   \n",
       "3              365 days 00:00:00                3536.250000   \n",
       "4              365 days 00:00:00                3536.250000   \n",
       "5              365 days 00:00:00                3536.250000   \n",
       "6              365 days 00:00:00                3536.250000   \n",
       "7              365 days 00:00:00                3536.250000   \n",
       "8              365 days 00:00:00                3536.250000   \n",
       "9              365 days 00:00:00                3536.250000   \n",
       "10             365 days 00:00:00                3536.250000   \n",
       "11                           NaT                        NaN   \n",
       "12             365 days 00:00:00                7188.400000   \n",
       "13             365 days 00:00:00                7188.400000   \n",
       "14             365 days 00:00:00                7188.400000   \n",
       "15             365 days 00:00:00                7188.400000   \n",
       "16             365 days 00:00:00                7188.400000   \n",
       "17             365 days 00:00:00                4523.250000   \n",
       "18             365 days 00:00:00                4523.250000   \n",
       "19             365 days 00:00:00                4523.250000   \n",
       "20             365 days 00:00:00                4523.250000   \n",
       "21             365 days 00:00:00                5022.000000   \n",
       "22             365 days 00:00:00                5022.000000   \n",
       "23                           NaT                        NaN   \n",
       "24             365 days 00:00:00               10000.000000   \n",
       "25             365 days 00:00:00               10000.000000   \n",
       "26                           NaT                        NaN   \n",
       "27                           NaT                        NaN   \n",
       "28                           NaT                        NaN   \n",
       "29             365 days 00:00:00                2500.000000   \n",
       "...                          ...                        ...   \n",
       "77268          365 days 00:00:00               10000.000000   \n",
       "77269          365 days 00:00:00               10000.000000   \n",
       "77270          365 days 00:00:00                9099.600000   \n",
       "77271          365 days 00:00:00                9099.600000   \n",
       "77272          365 days 00:00:00                9099.600000   \n",
       "77273          365 days 00:00:00                9099.600000   \n",
       "77274          365 days 00:00:00                9099.600000   \n",
       "77275                        NaT                        NaN   \n",
       "77276          365 days 00:00:00                4458.571429   \n",
       "77277          365 days 00:00:00                4458.571429   \n",
       "77278          365 days 00:00:00                4458.571429   \n",
       "77279          365 days 00:00:00                4458.571429   \n",
       "77280          365 days 00:00:00                4458.571429   \n",
       "77281          365 days 00:00:00                4458.571429   \n",
       "77282          365 days 00:00:00                4458.571429   \n",
       "77283                        NaT                        NaN   \n",
       "77284          365 days 00:00:00                6600.500000   \n",
       "77285          365 days 00:00:00                6600.500000   \n",
       "77286          365 days 00:00:00                6600.500000   \n",
       "77287          365 days 00:00:00                6600.500000   \n",
       "77288          365 days 00:00:00                6600.500000   \n",
       "77289          365 days 00:00:00                6600.500000   \n",
       "77290          365 days 00:00:00               10000.000000   \n",
       "77291          365 days 00:00:00               10000.000000   \n",
       "77292          365 days 00:00:00               10000.000000   \n",
       "77293          365 days 00:00:00                6533.200000   \n",
       "77294          365 days 00:00:00                6533.200000   \n",
       "77295          365 days 00:00:00                6533.200000   \n",
       "77296          365 days 00:00:00                6533.200000   \n",
       "77297          365 days 00:00:00                6533.200000   \n",
       "\n",
       "      mean_time_distance_in_group  min_km_distance_in_group  \\\n",
       "0                             NaT                       NaN   \n",
       "1                             NaT                       NaN   \n",
       "2                             NaT                       NaN   \n",
       "3               158 days 18:00:00                     587.0   \n",
       "4               158 days 18:00:00                     587.0   \n",
       "5               158 days 18:00:00                     587.0   \n",
       "6               158 days 18:00:00                     587.0   \n",
       "7               158 days 18:00:00                     587.0   \n",
       "8               158 days 18:00:00                     587.0   \n",
       "9               158 days 18:00:00                     587.0   \n",
       "10              158 days 18:00:00                     587.0   \n",
       "11                            NaT                       NaN   \n",
       "12              203 days 09:36:00                       0.0   \n",
       "13              203 days 09:36:00                       0.0   \n",
       "14              203 days 09:36:00                       0.0   \n",
       "15              203 days 09:36:00                       0.0   \n",
       "16              203 days 09:36:00                       0.0   \n",
       "17              150 days 06:00:00                       4.0   \n",
       "18              150 days 06:00:00                       4.0   \n",
       "19              150 days 06:00:00                       4.0   \n",
       "20              150 days 06:00:00                       4.0   \n",
       "21              182 days 12:00:00                      44.0   \n",
       "22              182 days 12:00:00                      44.0   \n",
       "23                            NaT                       NaN   \n",
       "24              320 days 00:00:00                   10000.0   \n",
       "25              320 days 00:00:00                   10000.0   \n",
       "26                            NaT                       NaN   \n",
       "27                            NaT                       NaN   \n",
       "28                            NaT                       NaN   \n",
       "29              214 days 00:00:00                       0.0   \n",
       "...                           ...                       ...   \n",
       "77268           323 days 12:00:00                   10000.0   \n",
       "77269           323 days 12:00:00                   10000.0   \n",
       "77270           202 days 04:48:00                    7749.0   \n",
       "77271           202 days 04:48:00                    7749.0   \n",
       "77272           202 days 04:48:00                    7749.0   \n",
       "77273           202 days 04:48:00                    7749.0   \n",
       "77274           202 days 04:48:00                    7749.0   \n",
       "77275                         NaT                       NaN   \n",
       "77276    172 days 20:34:17.142857                       0.0   \n",
       "77277    172 days 20:34:17.142857                       0.0   \n",
       "77278    172 days 20:34:17.142857                       0.0   \n",
       "77279    172 days 20:34:17.142857                       0.0   \n",
       "77280    172 days 20:34:17.142857                       0.0   \n",
       "77281    172 days 20:34:17.142857                       0.0   \n",
       "77282    172 days 20:34:17.142857                       0.0   \n",
       "77283                         NaT                       NaN   \n",
       "77284           218 days 08:00:00                     821.0   \n",
       "77285           218 days 08:00:00                     821.0   \n",
       "77286           218 days 08:00:00                     821.0   \n",
       "77287           218 days 08:00:00                     821.0   \n",
       "77288           218 days 08:00:00                     821.0   \n",
       "77289           218 days 08:00:00                     821.0   \n",
       "77290           238 days 08:00:00                   10000.0   \n",
       "77291           238 days 08:00:00                   10000.0   \n",
       "77292           238 days 08:00:00                   10000.0   \n",
       "77293           215 days 14:24:00                    1333.0   \n",
       "77294           215 days 14:24:00                    1333.0   \n",
       "77295           215 days 14:24:00                    1333.0   \n",
       "77296           215 days 14:24:00                    1333.0   \n",
       "77297           215 days 14:24:00                    1333.0   \n",
       "\n",
       "      min_time_distance_in_group  num_act  num_act_shift1  num_act_shift2  \\\n",
       "0                            NaT       16             NaN             NaN   \n",
       "1                            NaT       42             NaN             NaN   \n",
       "2                            NaT        3             NaN             NaN   \n",
       "3               23 days 00:00:00       10             NaN             NaN   \n",
       "4               23 days 00:00:00        8            10.0             NaN   \n",
       "5               23 days 00:00:00       14             8.0            10.0   \n",
       "6               23 days 00:00:00        4            14.0             8.0   \n",
       "7               23 days 00:00:00        5             4.0            14.0   \n",
       "8               23 days 00:00:00       10             5.0             4.0   \n",
       "9               23 days 00:00:00       15            10.0             5.0   \n",
       "10              23 days 00:00:00        4            15.0            10.0   \n",
       "11                           NaT        2             NaN             NaN   \n",
       "12               0 days 00:00:00       16             NaN             NaN   \n",
       "13               0 days 00:00:00        7            16.0             NaN   \n",
       "14               0 days 00:00:00        7             7.0            16.0   \n",
       "15               0 days 00:00:00       15             7.0             7.0   \n",
       "16               0 days 00:00:00        4            15.0             7.0   \n",
       "17              19 days 00:00:00       38             NaN             NaN   \n",
       "18              19 days 00:00:00        2            38.0             NaN   \n",
       "19              19 days 00:00:00      132             2.0            38.0   \n",
       "20              19 days 00:00:00       10           132.0             2.0   \n",
       "21               0 days 00:00:00        4             NaN             NaN   \n",
       "22               0 days 00:00:00        7             4.0             NaN   \n",
       "23                           NaT       14             NaN             NaN   \n",
       "24             275 days 00:00:00       24             NaN             NaN   \n",
       "25             275 days 00:00:00       17            24.0             NaN   \n",
       "26                           NaT        5             NaN             NaN   \n",
       "27                           NaT       10             NaN             NaN   \n",
       "28                           NaT        2             NaN             NaN   \n",
       "29              40 days 00:00:00        9             NaN             NaN   \n",
       "...                          ...      ...             ...             ...   \n",
       "77268          282 days 00:00:00       46             NaN             NaN   \n",
       "77269          282 days 00:00:00       17            46.0             NaN   \n",
       "77270            4 days 00:00:00       11             NaN             NaN   \n",
       "77271            4 days 00:00:00       25            11.0             NaN   \n",
       "77272            4 days 00:00:00        3            25.0            11.0   \n",
       "77273            4 days 00:00:00       44             3.0            25.0   \n",
       "77274            4 days 00:00:00        6            44.0             3.0   \n",
       "77275                        NaT       15             NaN             NaN   \n",
       "77276            0 days 00:00:00        5             NaN             NaN   \n",
       "77277            0 days 00:00:00        3             5.0             NaN   \n",
       "77278            0 days 00:00:00        7             3.0             5.0   \n",
       "77279            0 days 00:00:00      103             7.0             3.0   \n",
       "77280            0 days 00:00:00        5           103.0             7.0   \n",
       "77281            0 days 00:00:00        7             5.0           103.0   \n",
       "77282            0 days 00:00:00        6             7.0             5.0   \n",
       "77283                        NaT        8             NaN             NaN   \n",
       "77284           20 days 00:00:00        7             NaN             NaN   \n",
       "77285           20 days 00:00:00        7             7.0             NaN   \n",
       "77286           20 days 00:00:00       10             7.0             7.0   \n",
       "77287           20 days 00:00:00        8            10.0             7.0   \n",
       "77288           20 days 00:00:00        8             8.0            10.0   \n",
       "77289           20 days 00:00:00        6             8.0             8.0   \n",
       "77290          163 days 00:00:00        2             NaN             NaN   \n",
       "77291          163 days 00:00:00        2             2.0             NaN   \n",
       "77292          163 days 00:00:00        2             2.0             2.0   \n",
       "77293            1 days 00:00:00       13             NaN             NaN   \n",
       "77294            1 days 00:00:00        4            13.0             NaN   \n",
       "77295            1 days 00:00:00       19             4.0            13.0   \n",
       "77296            1 days 00:00:00        4            19.0             4.0   \n",
       "77297            1 days 00:00:00       19             4.0            19.0   \n",
       "\n",
       "       num_teile  num_teile_shift1  num_teile_shift2 repair_age  \\\n",
       "0              2               NaN               NaN        NaT   \n",
       "1              5               NaN               NaN        NaT   \n",
       "2              1               NaN               NaN        NaT   \n",
       "3              2               NaN               NaN     0 days   \n",
       "4              2               2.0               NaN    53 days   \n",
       "5              6               2.0               2.0   105 days   \n",
       "6              2               6.0               2.0   128 days   \n",
       "7              2               2.0               6.0   245 days   \n",
       "8              2               2.0               2.0   599 days   \n",
       "9              3               2.0               2.0   722 days   \n",
       "10             2               3.0               2.0   905 days   \n",
       "11             1               NaN               NaN        NaT   \n",
       "12             4               NaN               NaN     0 days   \n",
       "13             2               4.0               NaN   287 days   \n",
       "14             1               2.0               4.0   433 days   \n",
       "15             5               1.0               2.0   652 days   \n",
       "16             1               5.0               1.0   652 days   \n",
       "17             4               NaN               NaN     0 days   \n",
       "18             1               4.0               NaN    19 days   \n",
       "19             3               1.0               4.0   188 days   \n",
       "20             4               3.0               1.0   236 days   \n",
       "21             1               NaN               NaN     0 days   \n",
       "22             1               1.0               NaN     0 days   \n",
       "23             4               NaN               NaN        NaT   \n",
       "24             3               NaN               NaN     0 days   \n",
       "25             4               3.0               NaN   275 days   \n",
       "26             1               NaN               NaN        NaT   \n",
       "27             4               NaN               NaN        NaT   \n",
       "28             1               NaN               NaN        NaT   \n",
       "29             3               NaN               NaN     0 days   \n",
       "...          ...               ...               ...        ...   \n",
       "77268          3               NaN               NaN     0 days   \n",
       "77269          3               3.0               NaN   282 days   \n",
       "77270          3               NaN               NaN     0 days   \n",
       "77271          1               3.0               NaN   100 days   \n",
       "77272          1               1.0               3.0   104 days   \n",
       "77273          3               1.0               1.0   281 days   \n",
       "77274          1               3.0               1.0  1142 days   \n",
       "77275          3               NaN               NaN        NaT   \n",
       "77276          3               NaN               NaN     0 days   \n",
       "77277          1               3.0               NaN     4 days   \n",
       "77278          2               1.0               3.0   141 days   \n",
       "77279          3               2.0               1.0   141 days   \n",
       "77280          1               3.0               2.0   218 days   \n",
       "77281          3               1.0               3.0   480 days   \n",
       "77282          1               3.0               1.0  1255 days   \n",
       "77283          1               NaN               NaN        NaT   \n",
       "77284          3               NaN               NaN     0 days   \n",
       "77285          1               3.0               NaN   321 days   \n",
       "77286          2               1.0               3.0   352 days   \n",
       "77287          3               2.0               1.0   819 days   \n",
       "77288          1               3.0               2.0   839 days   \n",
       "77289          1               1.0               3.0  1047 days   \n",
       "77290          1               NaN               NaN     0 days   \n",
       "77291          1               1.0               NaN   187 days   \n",
       "77292          1               1.0               1.0   350 days   \n",
       "77293          3               NaN               NaN     0 days   \n",
       "77294          1               3.0               NaN     3 days   \n",
       "77295          3               1.0               3.0     4 days   \n",
       "77296          1               3.0               1.0   348 days   \n",
       "77297          3               1.0               3.0  1315 days   \n",
       "\n",
       "      time_distance_from_last time_distance_from_last2  \\\n",
       "0                         NaT                      NaT   \n",
       "1                         NaT                      NaT   \n",
       "2                         NaT                      NaT   \n",
       "3                         NaT                      NaT   \n",
       "4            53 days 00:00:00                      NaT   \n",
       "5            52 days 00:00:00        105 days 00:00:00   \n",
       "6            23 days 00:00:00         75 days 00:00:00   \n",
       "7           117 days 00:00:00        140 days 00:00:00   \n",
       "8           354 days 00:00:00        471 days 00:00:00   \n",
       "9           123 days 00:00:00        477 days 00:00:00   \n",
       "10          183 days 00:00:00        306 days 00:00:00   \n",
       "11                        NaT                      NaT   \n",
       "12                        NaT                      NaT   \n",
       "13          287 days 00:00:00                      NaT   \n",
       "14          146 days 00:00:00        433 days 00:00:00   \n",
       "15          219 days 00:00:00        365 days 00:00:00   \n",
       "16            0 days 00:00:00        219 days 00:00:00   \n",
       "17                        NaT                      NaT   \n",
       "18           19 days 00:00:00                      NaT   \n",
       "19          169 days 00:00:00        188 days 00:00:00   \n",
       "20           48 days 00:00:00        217 days 00:00:00   \n",
       "21                        NaT                      NaT   \n",
       "22            0 days 00:00:00                      NaT   \n",
       "23                        NaT                      NaT   \n",
       "24                        NaT                      NaT   \n",
       "25          275 days 00:00:00                      NaT   \n",
       "26                        NaT                      NaT   \n",
       "27                        NaT                      NaT   \n",
       "28                        NaT                      NaT   \n",
       "29                        NaT                      NaT   \n",
       "...                       ...                      ...   \n",
       "77268                     NaT                      NaT   \n",
       "77269       282 days 00:00:00                      NaT   \n",
       "77270                     NaT                      NaT   \n",
       "77271       100 days 00:00:00                      NaT   \n",
       "77272         4 days 00:00:00        104 days 00:00:00   \n",
       "77273       177 days 00:00:00        181 days 00:00:00   \n",
       "77274       365 days 00:00:00        542 days 00:00:00   \n",
       "77275                     NaT                      NaT   \n",
       "77276                     NaT                      NaT   \n",
       "77277         4 days 00:00:00                      NaT   \n",
       "77278       137 days 00:00:00        141 days 00:00:00   \n",
       "77279         0 days 00:00:00        137 days 00:00:00   \n",
       "77280        77 days 00:00:00         77 days 00:00:00   \n",
       "77281       262 days 00:00:00        339 days 00:00:00   \n",
       "77282       365 days 00:00:00        627 days 00:00:00   \n",
       "77283                     NaT                      NaT   \n",
       "77284                     NaT                      NaT   \n",
       "77285       321 days 00:00:00                      NaT   \n",
       "77286        31 days 00:00:00        352 days 00:00:00   \n",
       "77287       365 days 00:00:00        396 days 00:00:00   \n",
       "77288        20 days 00:00:00        385 days 00:00:00   \n",
       "77289       208 days 00:00:00        228 days 00:00:00   \n",
       "77290                     NaT                      NaT   \n",
       "77291       187 days 00:00:00                      NaT   \n",
       "77292       163 days 00:00:00        350 days 00:00:00   \n",
       "77293                     NaT                      NaT   \n",
       "77294         3 days 00:00:00                      NaT   \n",
       "77295         1 days 00:00:00          4 days 00:00:00   \n",
       "77296       344 days 00:00:00        345 days 00:00:00   \n",
       "77297       365 days 00:00:00        709 days 00:00:00   \n",
       "\n",
       "      time_distance_from_last3 time_distance_from_last4 time_distance_shift1  \\\n",
       "0                          NaT                      NaT                  NaT   \n",
       "1                          NaT                      NaT                  NaT   \n",
       "2                          NaT                      NaT                  NaT   \n",
       "3                          NaT                      NaT                  NaN   \n",
       "4                          NaT                      NaT                  NaT   \n",
       "5                          NaT                      NaT     53 days 00:00:00   \n",
       "6            128 days 00:00:00                      NaT     52 days 00:00:00   \n",
       "7            192 days 00:00:00        245 days 00:00:00     23 days 00:00:00   \n",
       "8            494 days 00:00:00        546 days 00:00:00    117 days 00:00:00   \n",
       "9            594 days 00:00:00        617 days 00:00:00    354 days 00:00:00   \n",
       "10           660 days 00:00:00        777 days 00:00:00    123 days 00:00:00   \n",
       "11                         NaT                      NaT                  NaT   \n",
       "12                         NaT                      NaT                  NaN   \n",
       "13                         NaT                      NaT                  NaT   \n",
       "14                         NaT                      NaT    287 days 00:00:00   \n",
       "15           652 days 00:00:00                      NaT    146 days 00:00:00   \n",
       "16           365 days 00:00:00        652 days 00:00:00    219 days 00:00:00   \n",
       "17                         NaT                      NaT                  NaN   \n",
       "18                         NaT                      NaT                  NaT   \n",
       "19                         NaT                      NaT     19 days 00:00:00   \n",
       "20           236 days 00:00:00                      NaT    169 days 00:00:00   \n",
       "21                         NaT                      NaT                  NaN   \n",
       "22                         NaT                      NaT                  NaT   \n",
       "23                         NaT                      NaT                  NaT   \n",
       "24                         NaT                      NaT                  NaN   \n",
       "25                         NaT                      NaT                  NaT   \n",
       "26                         NaT                      NaT                  NaT   \n",
       "27                         NaT                      NaT                  NaT   \n",
       "28                         NaT                      NaT                  NaT   \n",
       "29                         NaT                      NaT                  NaN   \n",
       "...                        ...                      ...                  ...   \n",
       "77268                      NaT                      NaT                  NaN   \n",
       "77269                      NaT                      NaT                  NaT   \n",
       "77270                      NaT                      NaT                  NaN   \n",
       "77271                      NaT                      NaT                  NaT   \n",
       "77272                      NaT                      NaT    100 days 00:00:00   \n",
       "77273        281 days 00:00:00                      NaT      4 days 00:00:00   \n",
       "77274        546 days 00:00:00        646 days 00:00:00    177 days 00:00:00   \n",
       "77275                      NaT                      NaT                  NaT   \n",
       "77276                      NaT                      NaT                  NaN   \n",
       "77277                      NaT                      NaT                  NaT   \n",
       "77278                      NaT                      NaT      4 days 00:00:00   \n",
       "77279        141 days 00:00:00                      NaT    137 days 00:00:00   \n",
       "77280        214 days 00:00:00        218 days 00:00:00      0 days 00:00:00   \n",
       "77281        339 days 00:00:00        476 days 00:00:00     77 days 00:00:00   \n",
       "77282        704 days 00:00:00        704 days 00:00:00    262 days 00:00:00   \n",
       "77283                      NaT                      NaT                  NaT   \n",
       "77284                      NaT                      NaT                  NaN   \n",
       "77285                      NaT                      NaT                  NaT   \n",
       "77286                      NaT                      NaT    321 days 00:00:00   \n",
       "77287        717 days 00:00:00                      NaT     31 days 00:00:00   \n",
       "77288        416 days 00:00:00        737 days 00:00:00    365 days 00:00:00   \n",
       "77289        593 days 00:00:00        624 days 00:00:00     20 days 00:00:00   \n",
       "77290                      NaT                      NaT                  NaN   \n",
       "77291                      NaT                      NaT                  NaT   \n",
       "77292                      NaT                      NaT    187 days 00:00:00   \n",
       "77293                      NaT                      NaT                  NaN   \n",
       "77294                      NaT                      NaT                  NaT   \n",
       "77295                      NaT                      NaT      3 days 00:00:00   \n",
       "77296        348 days 00:00:00                      NaT      1 days 00:00:00   \n",
       "77297        710 days 00:00:00        713 days 00:00:00    344 days 00:00:00   \n",
       "\n",
       "      time_distance_shift2 time_distance_shift3 time_distance_to_next  times  \\\n",
       "0                      NaT                  NaT                   NaT      1   \n",
       "1                      NaT                  NaT                   NaT      1   \n",
       "2                      NaT                  NaT                   NaT      1   \n",
       "3                      NaN                  NaN      53 days 00:00:00      8   \n",
       "4                      NaN                  NaN      52 days 00:00:00      8   \n",
       "5                      NaT                  NaN      23 days 00:00:00      8   \n",
       "6         53 days 00:00:00                  NaT     117 days 00:00:00      8   \n",
       "7         52 days 00:00:00     53 days 00:00:00     354 days 00:00:00      8   \n",
       "8         23 days 00:00:00     52 days 00:00:00     123 days 00:00:00      8   \n",
       "9        117 days 00:00:00     23 days 00:00:00     183 days 00:00:00      8   \n",
       "10       354 days 00:00:00    117 days 00:00:00                   NaN      8   \n",
       "11                     NaT                  NaT                   NaT      1   \n",
       "12                     NaN                  NaN     287 days 00:00:00      5   \n",
       "13                     NaN                  NaN     146 days 00:00:00      5   \n",
       "14                     NaT                  NaN     219 days 00:00:00      5   \n",
       "15       287 days 00:00:00                  NaT       0 days 00:00:00      5   \n",
       "16       146 days 00:00:00    287 days 00:00:00                   NaN      5   \n",
       "17                     NaN                  NaN      19 days 00:00:00      4   \n",
       "18                     NaN                  NaN     169 days 00:00:00      4   \n",
       "19                     NaT                  NaN      48 days 00:00:00      4   \n",
       "20        19 days 00:00:00                  NaT                   NaN      4   \n",
       "21                     NaN                  NaN       0 days 00:00:00      2   \n",
       "22                     NaN                  NaN                   NaN      2   \n",
       "23                     NaT                  NaT                   NaT      1   \n",
       "24                     NaN                  NaN     275 days 00:00:00      2   \n",
       "25                     NaN                  NaN                   NaN      2   \n",
       "26                     NaT                  NaT                   NaT      1   \n",
       "27                     NaT                  NaT                   NaT      1   \n",
       "28                     NaT                  NaT                   NaT      1   \n",
       "29                     NaN                  NaN      40 days 00:00:00      4   \n",
       "...                    ...                  ...                   ...    ...   \n",
       "77268                  NaN                  NaN     282 days 00:00:00      2   \n",
       "77269                  NaN                  NaN                   NaN      2   \n",
       "77270                  NaN                  NaN     100 days 00:00:00      5   \n",
       "77271                  NaN                  NaN       4 days 00:00:00      5   \n",
       "77272                  NaT                  NaN     177 days 00:00:00      5   \n",
       "77273    100 days 00:00:00                  NaT     365 days 00:00:00      5   \n",
       "77274      4 days 00:00:00    100 days 00:00:00                   NaN      5   \n",
       "77275                  NaT                  NaT                   NaT      1   \n",
       "77276                  NaN                  NaN       4 days 00:00:00      7   \n",
       "77277                  NaN                  NaN     137 days 00:00:00      7   \n",
       "77278                  NaT                  NaN       0 days 00:00:00      7   \n",
       "77279      4 days 00:00:00                  NaT      77 days 00:00:00      7   \n",
       "77280    137 days 00:00:00      4 days 00:00:00     262 days 00:00:00      7   \n",
       "77281      0 days 00:00:00    137 days 00:00:00     365 days 00:00:00      7   \n",
       "77282     77 days 00:00:00      0 days 00:00:00                   NaN      7   \n",
       "77283                  NaT                  NaT                   NaT      1   \n",
       "77284                  NaN                  NaN     321 days 00:00:00      6   \n",
       "77285                  NaN                  NaN      31 days 00:00:00      6   \n",
       "77286                  NaT                  NaN     365 days 00:00:00      6   \n",
       "77287    321 days 00:00:00                  NaT      20 days 00:00:00      6   \n",
       "77288     31 days 00:00:00    321 days 00:00:00     208 days 00:00:00      6   \n",
       "77289    365 days 00:00:00     31 days 00:00:00                   NaN      6   \n",
       "77290                  NaN                  NaN     187 days 00:00:00      3   \n",
       "77291                  NaN                  NaN     163 days 00:00:00      3   \n",
       "77292                  NaT                  NaN                   NaN      3   \n",
       "77293                  NaN                  NaN       3 days 00:00:00      5   \n",
       "77294                  NaN                  NaN       1 days 00:00:00      5   \n",
       "77295                  NaT                  NaN     344 days 00:00:00      5   \n",
       "77296      3 days 00:00:00                  NaT     365 days 00:00:00      5   \n",
       "77297      1 days 00:00:00      3 days 00:00:00                   NaN      5   \n",
       "\n",
       "           total AW  total_aw_shift1  total_aw_shift2  \\\n",
       "0      0.000000e+00              NaN              NaN   \n",
       "1      6.920609e+08              NaN              NaN   \n",
       "2      0.000000e+00              NaN              NaN   \n",
       "3      2.506427e+08              NaN              NaN   \n",
       "4      7.282570e+07     2.506427e+08              NaN   \n",
       "5      5.437718e+07     7.282570e+07     2.506427e+08   \n",
       "6      8.299390e+07     5.437718e+07     7.282570e+07   \n",
       "7      1.740418e+08     8.299390e+07     5.437718e+07   \n",
       "8      1.449721e+08     1.740418e+08     8.299390e+07   \n",
       "9      3.492072e+08     1.449721e+08     1.740418e+08   \n",
       "10     9.844455e+07     3.492072e+08     1.449721e+08   \n",
       "11     9.100526e+06              NaN              NaN   \n",
       "12     6.008080e+05              NaN              NaN   \n",
       "13     6.009330e+05     6.008080e+05              NaN   \n",
       "14     1.710782e+07     6.009330e+05     6.008080e+05   \n",
       "15     6.141441e+08     1.710782e+07     6.009330e+05   \n",
       "16     1.820254e+07     6.141441e+08     1.710782e+07   \n",
       "17     1.312889e+10              NaN              NaN   \n",
       "18     8.012790e+05     1.312889e+10              NaN   \n",
       "19     3.416196e+10     8.012790e+05     1.312889e+10   \n",
       "20     6.015440e+08     3.416196e+10     8.012790e+05   \n",
       "21     1.820254e+07              NaN              NaN   \n",
       "22     1.502184e+07     1.820254e+07              NaN   \n",
       "23     6.078441e+08              NaN              NaN   \n",
       "24     4.623936e+08              NaN              NaN   \n",
       "25     3.766853e+08     4.623936e+08              NaN   \n",
       "26     1.964710e+08              NaN              NaN   \n",
       "27     2.569950e+08              NaN              NaN   \n",
       "28     1.040050e+06              NaN              NaN   \n",
       "29     0.000000e+00              NaN              NaN   \n",
       "...             ...              ...              ...   \n",
       "77268  1.262346e+09              NaN              NaN   \n",
       "77269  3.550296e+08     1.262346e+09              NaN   \n",
       "77270  2.756869e+08              NaN              NaN   \n",
       "77271  7.435043e+08     2.756869e+08              NaN   \n",
       "77272  1.143133e+08     7.435043e+08     2.756869e+08   \n",
       "77273  1.180864e+09     1.143133e+08     7.435043e+08   \n",
       "77274  7.704780e+07     1.180864e+09     1.143133e+08   \n",
       "77275  1.537540e+08              NaN              NaN   \n",
       "77276  1.011400e+08              NaN              NaN   \n",
       "77277  9.609190e+07     1.011400e+08              NaN   \n",
       "77278  2.009209e+08     9.609190e+07     1.011400e+08   \n",
       "77279  2.883962e+09     2.009209e+08     9.609190e+07   \n",
       "77280  2.806735e+08     2.883962e+09     2.009209e+08   \n",
       "77281  1.011400e+08     2.806735e+08     2.883962e+09   \n",
       "77282  3.003192e+07     1.011400e+08     2.806735e+08   \n",
       "77283  6.655190e+07              NaN              NaN   \n",
       "77284  1.526729e+08              NaN              NaN   \n",
       "77285  2.304808e+08     1.526729e+08              NaN   \n",
       "77286  5.375397e+07     2.304808e+08     1.526729e+08   \n",
       "77287  2.862720e+08     5.375397e+07     2.304808e+08   \n",
       "77288  2.204294e+08     2.862720e+08     5.375397e+07   \n",
       "77289  7.704780e+07     2.204294e+08     2.862720e+08   \n",
       "77290  9.999999e+07              NaN              NaN   \n",
       "77291  9.999999e+07     9.999999e+07              NaN   \n",
       "77292  9.999999e+07     9.999999e+07     9.999999e+07   \n",
       "77293  2.908894e+08              NaN              NaN   \n",
       "77294  1.665133e+08     2.908894e+08              NaN   \n",
       "77295  4.757098e+08     1.665133e+08     2.908894e+08   \n",
       "77296  1.314107e+08     4.757098e+08     1.665133e+08   \n",
       "77297  2.452708e+08     1.314107e+08     4.757098e+08   \n",
       "\n",
       "       trend_of_frequent_in_the_past  weather_come_in_last_1_month  \\\n",
       "0                           0.000000                         False   \n",
       "1                           0.000000                         False   \n",
       "2                           0.000000                         False   \n",
       "3                           0.000000                         False   \n",
       "4                           0.000000                         False   \n",
       "5                           1.000000                         False   \n",
       "6                           0.051665                          True   \n",
       "7                          -0.017250                         False   \n",
       "8                          -0.007385                         False   \n",
       "9                          -0.004513                         False   \n",
       "10                          0.000450                         False   \n",
       "11                          0.000000                         False   \n",
       "12                          0.000000                         False   \n",
       "13                          0.000000                         False   \n",
       "14                          0.007092                         False   \n",
       "15                          0.006838                         False   \n",
       "16                          0.008684                          True   \n",
       "17                          0.000000                         False   \n",
       "18                          0.000000                          True   \n",
       "19                         -0.006667                         False   \n",
       "20                         -0.002291                         False   \n",
       "21                          0.000000                         False   \n",
       "22                          0.000000                          True   \n",
       "23                          0.000000                         False   \n",
       "24                          0.000000                         False   \n",
       "25                          0.000000                         False   \n",
       "26                          0.000000                         False   \n",
       "27                          0.000000                         False   \n",
       "28                          0.000000                         False   \n",
       "29                          0.000000                         False   \n",
       "...                              ...                           ...   \n",
       "77268                       0.000000                         False   \n",
       "77269                       0.000000                         False   \n",
       "77270                       0.000000                         False   \n",
       "77271                       0.000000                         False   \n",
       "77272                       0.010417                          True   \n",
       "77273                      -0.005125                         False   \n",
       "77274                      -0.006891                         False   \n",
       "77275                       0.000000                         False   \n",
       "77276                       0.000000                         False   \n",
       "77277                       0.000000                          True   \n",
       "77278                      -0.007519                         False   \n",
       "77279                       0.000329                          True   \n",
       "77280                      -0.003195                         False   \n",
       "77281                      -0.006158                         False   \n",
       "77282                      -0.007629                         False   \n",
       "77283                       0.000000                         False   \n",
       "77284                       0.000000                         False   \n",
       "77285                       0.000000                         False   \n",
       "77286                       0.003448                         False   \n",
       "77287                      -0.000668                         False   \n",
       "77288                       0.002794                          True   \n",
       "77289                      -0.001155                         False   \n",
       "77290                       0.000000                         False   \n",
       "77291                       0.000000                         False   \n",
       "77292                       0.041667                         False   \n",
       "77293                       0.000000                         False   \n",
       "77294                       0.000000                          True   \n",
       "77295                       0.500000                          True   \n",
       "77296                      -0.004373                         False   \n",
       "77297                      -0.005740                         False   \n",
       "\n",
       "       weather_come_in_last_2_month  weather_come_in_last_3_month  \\\n",
       "0                             False                         False   \n",
       "1                             False                         False   \n",
       "2                             False                         False   \n",
       "3                             False                         False   \n",
       "4                              True                          True   \n",
       "5                              True                          True   \n",
       "6                              True                          True   \n",
       "7                             False                         False   \n",
       "8                             False                         False   \n",
       "9                             False                         False   \n",
       "10                            False                         False   \n",
       "11                            False                         False   \n",
       "12                            False                         False   \n",
       "13                            False                         False   \n",
       "14                            False                         False   \n",
       "15                            False                         False   \n",
       "16                             True                          True   \n",
       "17                            False                         False   \n",
       "18                             True                          True   \n",
       "19                            False                         False   \n",
       "20                             True                          True   \n",
       "21                            False                         False   \n",
       "22                             True                          True   \n",
       "23                            False                         False   \n",
       "24                            False                         False   \n",
       "25                            False                         False   \n",
       "26                            False                         False   \n",
       "27                            False                         False   \n",
       "28                            False                         False   \n",
       "29                            False                         False   \n",
       "...                             ...                           ...   \n",
       "77268                         False                         False   \n",
       "77269                         False                         False   \n",
       "77270                         False                         False   \n",
       "77271                         False                         False   \n",
       "77272                          True                          True   \n",
       "77273                         False                         False   \n",
       "77274                         False                         False   \n",
       "77275                         False                         False   \n",
       "77276                         False                         False   \n",
       "77277                          True                          True   \n",
       "77278                         False                         False   \n",
       "77279                          True                          True   \n",
       "77280                         False                          True   \n",
       "77281                         False                         False   \n",
       "77282                         False                         False   \n",
       "77283                         False                         False   \n",
       "77284                         False                         False   \n",
       "77285                         False                         False   \n",
       "77286                          True                          True   \n",
       "77287                         False                         False   \n",
       "77288                          True                          True   \n",
       "77289                         False                         False   \n",
       "77290                         False                         False   \n",
       "77291                         False                         False   \n",
       "77292                         False                         False   \n",
       "77293                         False                         False   \n",
       "77294                          True                          True   \n",
       "77295                          True                          True   \n",
       "77296                         False                         False   \n",
       "77297                         False                         False   \n",
       "\n",
       "       weather_come_in_last_4_month  weather_come_in_last_5_month  \\\n",
       "0                             False                         False   \n",
       "1                             False                         False   \n",
       "2                             False                         False   \n",
       "3                             False                         False   \n",
       "4                              True                          True   \n",
       "5                              True                          True   \n",
       "6                              True                          True   \n",
       "7                              True                          True   \n",
       "8                             False                         False   \n",
       "9                             False                          True   \n",
       "10                            False                         False   \n",
       "11                            False                         False   \n",
       "12                            False                         False   \n",
       "13                            False                         False   \n",
       "14                            False                          True   \n",
       "15                            False                         False   \n",
       "16                             True                          True   \n",
       "17                            False                         False   \n",
       "18                             True                          True   \n",
       "19                            False                         False   \n",
       "20                             True                          True   \n",
       "21                            False                         False   \n",
       "22                             True                          True   \n",
       "23                            False                         False   \n",
       "24                            False                         False   \n",
       "25                            False                         False   \n",
       "26                            False                         False   \n",
       "27                            False                         False   \n",
       "28                            False                         False   \n",
       "29                            False                         False   \n",
       "...                             ...                           ...   \n",
       "77268                         False                         False   \n",
       "77269                         False                         False   \n",
       "77270                         False                         False   \n",
       "77271                          True                          True   \n",
       "77272                          True                          True   \n",
       "77273                         False                         False   \n",
       "77274                         False                         False   \n",
       "77275                         False                         False   \n",
       "77276                         False                         False   \n",
       "77277                          True                          True   \n",
       "77278                         False                          True   \n",
       "77279                          True                          True   \n",
       "77280                          True                          True   \n",
       "77281                         False                         False   \n",
       "77282                         False                         False   \n",
       "77283                         False                         False   \n",
       "77284                         False                         False   \n",
       "77285                         False                         False   \n",
       "77286                          True                          True   \n",
       "77287                         False                         False   \n",
       "77288                          True                          True   \n",
       "77289                         False                         False   \n",
       "77290                         False                         False   \n",
       "77291                         False                         False   \n",
       "77292                         False                         False   \n",
       "77293                         False                         False   \n",
       "77294                          True                          True   \n",
       "77295                          True                          True   \n",
       "77296                         False                         False   \n",
       "77297                         False                         False   \n",
       "\n",
       "       weather_come_in_last_6_month  year  weather_come_in_1_month  \\\n",
       "0                             False  2014                    False   \n",
       "1                             False  2015                    False   \n",
       "2                             False  2014                    False   \n",
       "3                             False  2014                    False   \n",
       "4                              True  2014                    False   \n",
       "5                              True  2015                     True   \n",
       "6                              True  2015                    False   \n",
       "7                              True  2015                    False   \n",
       "8                             False  2016                    False   \n",
       "9                              True  2016                    False   \n",
       "10                            False  2017                    False   \n",
       "11                            False  2014                    False   \n",
       "12                            False  2014                    False   \n",
       "13                            False  2014                    False   \n",
       "14                             True  2015                    False   \n",
       "15                            False  2015                     True   \n",
       "16                             True  2015                    False   \n",
       "17                            False  2014                     True   \n",
       "18                             True  2014                    False   \n",
       "19                             True  2014                    False   \n",
       "20                             True  2015                    False   \n",
       "21                            False  2015                     True   \n",
       "22                             True  2015                    False   \n",
       "23                            False  2015                    False   \n",
       "24                            False  2016                    False   \n",
       "25                            False  2016                    False   \n",
       "26                            False  2017                    False   \n",
       "27                            False  2018                    False   \n",
       "28                            False  2018                    False   \n",
       "29                            False  2014                    False   \n",
       "...                             ...   ...                      ...   \n",
       "77268                         False  2014                    False   \n",
       "77269                         False  2015                    False   \n",
       "77270                         False  2014                    False   \n",
       "77271                          True  2014                     True   \n",
       "77272                          True  2014                    False   \n",
       "77273                          True  2015                    False   \n",
       "77274                         False  2017                    False   \n",
       "77275                         False  2016                    False   \n",
       "77276                         False  2015                     True   \n",
       "77277                          True  2015                    False   \n",
       "77278                          True  2015                     True   \n",
       "77279                          True  2015                    False   \n",
       "77280                          True  2015                    False   \n",
       "77281                         False  2016                    False   \n",
       "77282                         False  2018                    False   \n",
       "77283                         False  2017                    False   \n",
       "77284                         False  2014                    False   \n",
       "77285                         False  2015                    False   \n",
       "77286                          True  2015                    False   \n",
       "77287                         False  2016                     True   \n",
       "77288                          True  2016                    False   \n",
       "77289                         False  2017                    False   \n",
       "77290                         False  2016                    False   \n",
       "77291                         False  2016                    False   \n",
       "77292                          True  2017                    False   \n",
       "77293                         False  2014                     True   \n",
       "77294                          True  2014                     True   \n",
       "77295                          True  2014                    False   \n",
       "77296                         False  2015                    False   \n",
       "77297                         False  2017                    False   \n",
       "\n",
       "       weather_come_in_2_month  weather_come_in_3_month  \\\n",
       "0                        False                    False   \n",
       "1                        False                    False   \n",
       "2                        False                    False   \n",
       "3                         True                     True   \n",
       "4                         True                     True   \n",
       "5                         True                     True   \n",
       "6                        False                    False   \n",
       "7                        False                    False   \n",
       "8                        False                    False   \n",
       "9                        False                    False   \n",
       "10                       False                    False   \n",
       "11                       False                    False   \n",
       "12                       False                    False   \n",
       "13                       False                    False   \n",
       "14                       False                    False   \n",
       "15                        True                     True   \n",
       "16                       False                    False   \n",
       "17                        True                     True   \n",
       "18                       False                    False   \n",
       "19                        True                     True   \n",
       "20                       False                    False   \n",
       "21                        True                     True   \n",
       "22                       False                    False   \n",
       "23                       False                    False   \n",
       "24                       False                    False   \n",
       "25                       False                    False   \n",
       "26                       False                    False   \n",
       "27                       False                    False   \n",
       "28                       False                    False   \n",
       "29                        True                     True   \n",
       "...                        ...                      ...   \n",
       "77268                    False                    False   \n",
       "77269                    False                    False   \n",
       "77270                    False                    False   \n",
       "77271                     True                     True   \n",
       "77272                    False                    False   \n",
       "77273                    False                    False   \n",
       "77274                    False                    False   \n",
       "77275                    False                    False   \n",
       "77276                     True                     True   \n",
       "77277                    False                    False   \n",
       "77278                     True                     True   \n",
       "77279                    False                     True   \n",
       "77280                    False                    False   \n",
       "77281                    False                    False   \n",
       "77282                    False                    False   \n",
       "77283                    False                    False   \n",
       "77284                    False                    False   \n",
       "77285                     True                     True   \n",
       "77286                    False                    False   \n",
       "77287                     True                     True   \n",
       "77288                    False                    False   \n",
       "77289                    False                    False   \n",
       "77290                    False                    False   \n",
       "77291                    False                    False   \n",
       "77292                    False                    False   \n",
       "77293                     True                     True   \n",
       "77294                     True                     True   \n",
       "77295                    False                    False   \n",
       "77296                    False                    False   \n",
       "77297                    False                    False   \n",
       "\n",
       "       weather_come_in_4_month  weather_come_in_5_month  \\\n",
       "0                        False                    False   \n",
       "1                        False                    False   \n",
       "2                        False                    False   \n",
       "3                         True                     True   \n",
       "4                         True                     True   \n",
       "5                         True                     True   \n",
       "6                         True                     True   \n",
       "7                        False                    False   \n",
       "8                        False                     True   \n",
       "9                        False                    False   \n",
       "10                       False                    False   \n",
       "11                       False                    False   \n",
       "12                       False                    False   \n",
       "13                       False                     True   \n",
       "14                       False                    False   \n",
       "15                        True                     True   \n",
       "16                       False                    False   \n",
       "17                        True                     True   \n",
       "18                       False                    False   \n",
       "19                        True                     True   \n",
       "20                       False                    False   \n",
       "21                        True                     True   \n",
       "22                       False                    False   \n",
       "23                       False                    False   \n",
       "24                       False                    False   \n",
       "25                       False                    False   \n",
       "26                       False                    False   \n",
       "27                       False                    False   \n",
       "28                       False                    False   \n",
       "29                        True                     True   \n",
       "...                        ...                      ...   \n",
       "77268                    False                    False   \n",
       "77269                    False                    False   \n",
       "77270                     True                     True   \n",
       "77271                     True                     True   \n",
       "77272                    False                    False   \n",
       "77273                    False                    False   \n",
       "77274                    False                    False   \n",
       "77275                    False                    False   \n",
       "77276                     True                     True   \n",
       "77277                    False                     True   \n",
       "77278                     True                     True   \n",
       "77279                     True                     True   \n",
       "77280                    False                    False   \n",
       "77281                    False                    False   \n",
       "77282                    False                    False   \n",
       "77283                    False                    False   \n",
       "77284                    False                    False   \n",
       "77285                     True                     True   \n",
       "77286                    False                    False   \n",
       "77287                     True                     True   \n",
       "77288                    False                    False   \n",
       "77289                    False                    False   \n",
       "77290                    False                    False   \n",
       "77291                    False                    False   \n",
       "77292                    False                    False   \n",
       "77293                     True                     True   \n",
       "77294                     True                     True   \n",
       "77295                    False                    False   \n",
       "77296                    False                    False   \n",
       "77297                    False                    False   \n",
       "\n",
       "       weather_come_in_6_month  \n",
       "0                        False  \n",
       "1                        False  \n",
       "2                        False  \n",
       "3                         True  \n",
       "4                         True  \n",
       "5                         True  \n",
       "6                         True  \n",
       "7                        False  \n",
       "8                         True  \n",
       "9                        False  \n",
       "10                       False  \n",
       "11                       False  \n",
       "12                       False  \n",
       "13                        True  \n",
       "14                       False  \n",
       "15                        True  \n",
       "16                       False  \n",
       "17                        True  \n",
       "18                        True  \n",
       "19                        True  \n",
       "20                       False  \n",
       "21                        True  \n",
       "22                       False  \n",
       "23                       False  \n",
       "24                       False  \n",
       "25                       False  \n",
       "26                       False  \n",
       "27                       False  \n",
       "28                       False  \n",
       "29                        True  \n",
       "...                        ...  \n",
       "77268                    False  \n",
       "77269                    False  \n",
       "77270                     True  \n",
       "77271                     True  \n",
       "77272                     True  \n",
       "77273                    False  \n",
       "77274                    False  \n",
       "77275                    False  \n",
       "77276                     True  \n",
       "77277                     True  \n",
       "77278                     True  \n",
       "77279                     True  \n",
       "77280                    False  \n",
       "77281                    False  \n",
       "77282                    False  \n",
       "77283                    False  \n",
       "77284                    False  \n",
       "77285                     True  \n",
       "77286                    False  \n",
       "77287                     True  \n",
       "77288                    False  \n",
       "77289                    False  \n",
       "77290                    False  \n",
       "77291                     True  \n",
       "77292                    False  \n",
       "77293                     True  \n",
       "77294                     True  \n",
       "77295                    False  \n",
       "77296                    False  \n",
       "77297                    False  \n",
       "\n",
       "[77298 rows x 55 columns]"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2393"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df3 = tg\n",
    "del tg\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deal with NA value in df\n",
    "- Markencode\n",
    "    - set to unknown\n",
    "- Adressanredecode\n",
    "    - set to unknown\n",
    "- Motorcode\n",
    "    - set to unknown\n",
    "- Fahrzeugmodellnummer\n",
    "    - set to unknown\n",
    "- Modell\n",
    "    - set to unknown\n",
    "- Typ\n",
    "    - set to unknown\n",
    "- Getriebecode\n",
    "    - set to unknown\n",
    "- Gewicht\n",
    "    - set to unknown\n",
    "- Leistung\n",
    "    - set to unknown\n",
    "- Erstzulassungsdatum\n",
    "    - set to unknown\n",
    "- age_auto\n",
    "    - set to 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nd = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# fill na with unknown\n",
    "fs_to_un = ['Markencode', 'Adressanredecode', 'Motorcode', 'Fahrzeugmodellnummer', 'Modell',\n",
    "           'Typ', 'Getriebecode', 'Gewicht', 'Leistung (KW)', 'Erstzulassungsdatum']\n",
    "for col in fs_to_un:\n",
    "    nd[col] = nd[col].fillna('unknown')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nd['age_auto'] = nd['age_auto'].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "238"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = nd\n",
    "del nd\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Deal with NA value in df3\n",
    "firstly, drop all the features, which 'times' equal to 1\n",
    "- frequent_in_last_X_month: \n",
    "    - reason: for the first coming auto this feature doesn't exist\n",
    "    - fill na with 0\n",
    "- km_distance_from_last:\n",
    "    - reason: for the first coming auto this feature doesn't exist\n",
    "    - fill with mean, for the auto that only one time appear, fill with total mean\n",
    "- km_distance_to_next:\n",
    "    - reason: doesn't exist for the last data in a group\n",
    "    - fill with mean\n",
    "    - Notice: should not be used in the training\n",
    "- num_act_shift1, num_act_shift2:\n",
    "    - reason: doesn't exist for the first data in group\n",
    "    - fill with 0\n",
    "    - Notice: better not used\n",
    "- num_teile_shift1, num_teile_shift2:\n",
    "    - reason: doesn't exist for the first data in group\n",
    "    - fill with 0\n",
    "    - Notice: better not used\n",
    "- repair age:\n",
    "    - reason: for the first coming auto this feature doesn't exist\n",
    "    - fill with 0 timedelta\n",
    "- time_distance_from_last\n",
    "    - reason: for the first coming auto this feature doesn't exist\n",
    "    - fill with mean\n",
    "- time_distance_from_lastX, time_distance_shiftX\n",
    "    - reason: for the first coming auto this feature doesn't exist\n",
    "    - fill with mean\n",
    "    - Notice: better not used\n",
    "- total_aw_shiftX\n",
    "    - reason: for the first coming auto this feature doesn't exist\n",
    "    - fill with 0\n",
    "    -Notice: better not used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ndf = df3.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# drop times == 1\n",
    "ndf = ndf.drop(ndf[ndf['times'] == 1].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# fill with 0\n",
    "fs_fill_0 = ['frequent_in_last_1_month', 'frequent_in_last_2_month', 'frequent_in_last_3_month', \n",
    "            'frequent_in_last_4_month', 'frequent_in_last_5_month', 'frequent_in_last_6_month',\n",
    "            'num_act_shift1', 'num_act_shift2', 'num_teile_shift1', 'num_teile_shift2', 'total_aw_shift1',\n",
    "            'total_aw_shift2']\n",
    "for col in fs_fill_0:\n",
    "    ndf[col] = ndf[col].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# fill with 0 timedelta\n",
    "ndf['repair_age'] = ndf['repair_age'].fillna(timedelta(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ihuangyiran/anaconda2/envs/data_mining/lib/python3.6/site-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n",
      "/Users/ihuangyiran/anaconda2/envs/data_mining/lib/python3.6/site-packages/ipykernel_launcher.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  after removing the cwd from sys.path.\n"
     ]
    }
   ],
   "source": [
    "# fill with mean: km_distance_from_last, km_distance_to_next\n",
    "ndf['km_distance_from_last'][ndf['km_distance_from_last'].isna()] = ndf['mean_km_distance_in_group'][ndf['km_distance_from_last'].isna()]\n",
    "ndf['km_distance_from_last'] = ndf['km_distance_from_last'].fillna(ndf['km_distance_from_last'].mean()) # fill the rest with total mean\n",
    "ndf['km_distance_to_next'][ndf['km_distance_to_next'].isna()] = ndf['mean_km_distance_in_group'][ndf['km_distance_to_next'].isna()]\n",
    "ndf['km_distance_to_next'] = ndf['km_distance_to_next'].fillna(ndf['km_distance_to_next'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# fill with mean: time_distance_from_last\n",
    "# timedelta changed to int automatically, why???\n",
    "def set_f2_to_f1(x, feature1, feature2):\n",
    "    #print(x['time_distance_from_last'])\n",
    "    if type(x[feature1]) == type(pd.NaT):\n",
    "        x[feature1] = x[feature2]\n",
    "    return x\n",
    "ndf = ndf.apply(set_f2_to_f1, axis = 1, args = ['time_distance_from_last', 'mean_time_distance_in_group'])\n",
    "ndf = ndf.apply(set_f2_to_f1, axis = 1, args = ['time_distance_to_next', 'mean_time_distance_in_group'])\n",
    "#ndf['time_distance_from_last'][ndf['time_distance_from_last'].isna()] = ndf['mean_time_distance_in_group'][ndf['time_distance_from_last'].isna()]\n",
    "#ndf['time_distance_from_last'] = ndf['time_distance_from_last'].fillna(ndf['time_distance_from_last'].mean())\n",
    "#ndf['time_distance_to_next'][ndf['time_distance_to_next'].isna()] = ndf['mean_time_distance_in_group'][ndf['time_distance_to_next'].isna()]\n",
    "#ndf['time_distance_to_next'] = ndf['time_distance_to_next'].fillna(ndf['time_distance_to_next'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# fill with mean: time_distance_from_lastX, time_distance_shiftX, total_aw_shiftX\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# delete na items in time_distance_from_last\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# extract the feature, that we may will used in the training\n",
    "fs = ['Auftragsdatum', 'Fahrgestellnummer', 'frequent_in_2014', 'frequent_in_2015', 'frequent_in_2016',\n",
    "     'frequent_in_2017', 'frequent_in_2018', 'frequent_in_last_1_month', 'frequent_in_last_2_month',\n",
    "     'frequent_in_last_3_month', 'frequent_in_last_4_month', 'frequent_in_last_5_month', 'frequent_in_last_6_month',\n",
    "     'km_distance_from_last', 'max_km_distance_in_group', 'min_km_distance_in_group', 'mean_km_distance_in_group',\n",
    "     'mean_time_distance_in_group', 'max_time_distance_in_group', 'min_time_distance_in_group', \n",
    "     'num_act_shift1', 'num_act_shift2', 'num_teile_shift1', 'num_teile_shift2', 'repair_age',\n",
    "     'time_distance_from_last', 'total_aw_shift1', 'total_aw_shift2', 'trend_of_frequent_in_the_past',\n",
    "     'weather_come_in_last_1_month', 'weather_come_in_last_2_month', 'weather_come_in_last_3_month',\n",
    "     'weather_come_in_last_4_month', 'weather_come_in_last_5_month', 'weather_come_in_last_6_month',\n",
    "     'weather_come_in_1_month', 'weather_come_in_2_month', 'weather_come_in_3_month', 'weather_come_in_4_month',\n",
    "     'weather_come_in_5_month', 'weather_come_in_6_month']\n",
    "ndf = ndf[fs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# remember that, we still have feature in df, so we have to merge the df3 back to df.\n",
    "# we name it df2 here for test\n",
    "df2 = pd.merge(df, ndf, on = ['Fahrgestellnummer', 'Auftragsdatum'], how = 'right')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "79369"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# length of df2 may larger than length of df3, because the combination of 'Fahrgestellnummer' and 'Auftragsdatum' are\n",
    "# not unique in df\n",
    "len(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 79369 entries, 0 to 79368\n",
      "Data columns (total 63 columns):\n",
      "Auftragsnummer                   79369 non-null object\n",
      "num_act                          79369 non-null int64\n",
      "total AW                         79369 non-null float64\n",
      "Gruppe-Nr                        79369 non-null object\n",
      "num_teile                        79369 non-null int64\n",
      "KM-Stand                         79369 non-null float64\n",
      "Markencode                       79369 non-null object\n",
      "Lagerortcode                     79369 non-null object\n",
      "Auftragsdatum                    79369 non-null datetime64[ns]\n",
      "Adressanredecode                 79369 non-null object\n",
      "Motorcode                        79369 non-null object\n",
      "Fahrzeugmodellnummer             79369 non-null object\n",
      "Modell                           79369 non-null object\n",
      "Typ                              79369 non-null object\n",
      "Getriebecode                     79369 non-null object\n",
      "Gewicht                          79369 non-null object\n",
      "Leistung (KW)                    79369 non-null object\n",
      "Fahrgestellnummer                79369 non-null object\n",
      "Erstzulassungsdatum              79369 non-null object\n",
      "year                             79369 non-null object\n",
      "month                            79369 non-null object\n",
      "day                              79369 non-null object\n",
      "age_auto                         79369 non-null float64\n",
      "age_autohaus                     79369 non-null int64\n",
      "frequent_in_2014                 79369 non-null int64\n",
      "frequent_in_2015                 79369 non-null int64\n",
      "frequent_in_2016                 79369 non-null int64\n",
      "frequent_in_2017                 79369 non-null int64\n",
      "frequent_in_2018                 79369 non-null int64\n",
      "frequent_in_last_1_month         79369 non-null float64\n",
      "frequent_in_last_2_month         79369 non-null float64\n",
      "frequent_in_last_3_month         79369 non-null float64\n",
      "frequent_in_last_4_month         79369 non-null float64\n",
      "frequent_in_last_5_month         79369 non-null float64\n",
      "frequent_in_last_6_month         79369 non-null float64\n",
      "km_distance_from_last            79369 non-null float64\n",
      "max_km_distance_in_group         79369 non-null float64\n",
      "min_km_distance_in_group         79369 non-null float64\n",
      "mean_km_distance_in_group        79369 non-null float64\n",
      "mean_time_distance_in_group      79369 non-null timedelta64[ns]\n",
      "max_time_distance_in_group       79369 non-null timedelta64[ns]\n",
      "min_time_distance_in_group       79369 non-null timedelta64[ns]\n",
      "num_act_shift1                   79369 non-null float64\n",
      "num_act_shift2                   79369 non-null float64\n",
      "num_teile_shift1                 79369 non-null float64\n",
      "num_teile_shift2                 79369 non-null float64\n",
      "repair_age                       79369 non-null timedelta64[ns]\n",
      "time_distance_from_last          79369 non-null timedelta64[ns]\n",
      "total_aw_shift1                  79369 non-null float64\n",
      "total_aw_shift2                  79369 non-null float64\n",
      "trend_of_frequent_in_the_past    79369 non-null float64\n",
      "weather_come_in_last_1_month     79369 non-null bool\n",
      "weather_come_in_last_2_month     79369 non-null bool\n",
      "weather_come_in_last_3_month     79369 non-null bool\n",
      "weather_come_in_last_4_month     79369 non-null bool\n",
      "weather_come_in_last_5_month     79369 non-null bool\n",
      "weather_come_in_last_6_month     79369 non-null bool\n",
      "weather_come_in_1_month          79369 non-null bool\n",
      "weather_come_in_2_month          79369 non-null bool\n",
      "weather_come_in_3_month          79369 non-null bool\n",
      "weather_come_in_4_month          79369 non-null bool\n",
      "weather_come_in_5_month          79369 non-null bool\n",
      "weather_come_in_6_month          79369 non-null bool\n",
      "dtypes: bool(12), datetime64[ns](1), float64(20), int64(8), object(17), timedelta64[ns](5)\n",
      "memory usage: 32.4+ MB\n"
     ]
    }
   ],
   "source": [
    "df2.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Deal with outlier in df and clip\n",
    "!!!!!!!!!!!!!not jet\n",
    "e.g. als neuen wagen in attribute Lagerortcode\n",
    "- unknown value in Gewicht feature\n",
    "- unknown vlaue in Leistung (KW) feature\n",
    "- frequent should be numeric and >= 0\n",
    "- km distance should be numeric and >= 0\n",
    "- num_act and num_teile should be numeric and >= 0\n",
    "- repair_age should be numeric and >= 0\n",
    "- time distance should be numeric and >= 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ol = df2.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ihuangyiran/anaconda2/envs/data_mining/lib/python3.6/site-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "# Gewicht\n",
    "ol['Gewicht'][ol['Gewicht'] == 'unknown'] = '0,00'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ihuangyiran/anaconda2/envs/data_mining/lib/python3.6/site-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "# Leistung (KW)\n",
    "ol['Leistung (KW)'][ol['Leistung (KW)'] == 'unknown']  = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#df2.info()\n",
    "df2 = ol\n",
    "del ol\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#ol['Leistung (KW)'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## extract feature: mean_km_distance_in_total, mean_time_distance_in_total\n",
    "not jet !!!!!!!!\n",
    "- 有次来确定应该往后看几个月"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tt = df2.copy()\n",
    "tt['mean_km_distance_in_total'] = tt['km_distance_from_last'].mean()\n",
    "tt['mean_time_distance_in_total'] = tt['time_distance_from_last'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2 = tt\n",
    "del tt\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       142 days 03:04:44.851799\n",
       "1       142 days 03:04:44.851799\n",
       "2       142 days 03:04:44.851799\n",
       "3       142 days 03:04:44.851799\n",
       "4       142 days 03:04:44.851799\n",
       "5       142 days 03:04:44.851799\n",
       "6       142 days 03:04:44.851799\n",
       "7       142 days 03:04:44.851799\n",
       "8       142 days 03:04:44.851799\n",
       "9       142 days 03:04:44.851799\n",
       "10      142 days 03:04:44.851799\n",
       "11      142 days 03:04:44.851799\n",
       "12      142 days 03:04:44.851799\n",
       "13      142 days 03:04:44.851799\n",
       "14      142 days 03:04:44.851799\n",
       "15      142 days 03:04:44.851799\n",
       "16      142 days 03:04:44.851799\n",
       "17      142 days 03:04:44.851799\n",
       "18      142 days 03:04:44.851799\n",
       "19      142 days 03:04:44.851799\n",
       "20      142 days 03:04:44.851799\n",
       "21      142 days 03:04:44.851799\n",
       "22      142 days 03:04:44.851799\n",
       "23      142 days 03:04:44.851799\n",
       "24      142 days 03:04:44.851799\n",
       "25      142 days 03:04:44.851799\n",
       "26      142 days 03:04:44.851799\n",
       "27      142 days 03:04:44.851799\n",
       "28      142 days 03:04:44.851799\n",
       "29      142 days 03:04:44.851799\n",
       "                  ...           \n",
       "79339   142 days 03:04:44.851799\n",
       "79340   142 days 03:04:44.851799\n",
       "79341   142 days 03:04:44.851799\n",
       "79342   142 days 03:04:44.851799\n",
       "79343   142 days 03:04:44.851799\n",
       "79344   142 days 03:04:44.851799\n",
       "79345   142 days 03:04:44.851799\n",
       "79346   142 days 03:04:44.851799\n",
       "79347   142 days 03:04:44.851799\n",
       "79348   142 days 03:04:44.851799\n",
       "79349   142 days 03:04:44.851799\n",
       "79350   142 days 03:04:44.851799\n",
       "79351   142 days 03:04:44.851799\n",
       "79352   142 days 03:04:44.851799\n",
       "79353   142 days 03:04:44.851799\n",
       "79354   142 days 03:04:44.851799\n",
       "79355   142 days 03:04:44.851799\n",
       "79356   142 days 03:04:44.851799\n",
       "79357   142 days 03:04:44.851799\n",
       "79358   142 days 03:04:44.851799\n",
       "79359   142 days 03:04:44.851799\n",
       "79360   142 days 03:04:44.851799\n",
       "79361   142 days 03:04:44.851799\n",
       "79362   142 days 03:04:44.851799\n",
       "79363   142 days 03:04:44.851799\n",
       "79364   142 days 03:04:44.851799\n",
       "79365   142 days 03:04:44.851799\n",
       "79366   142 days 03:04:44.851799\n",
       "79367   142 days 03:04:44.851799\n",
       "79368   142 days 03:04:44.851799\n",
       "Name: mean_time_distance_in_total, Length: 79369, dtype: timedelta64[ns]"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2['mean_time_distance_in_total']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## feature vor discretization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "av = df2.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ihuangyiran/anaconda2/envs/data_mining/lib/python3.6/site-packages/pandas/core/indexing.py:194: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self._setitem_with_indexer(indexer, value)\n"
     ]
    }
   ],
   "source": [
    "# Adressanredecode\n",
    "av['Adressanredecode'].loc[(av['Adressanredecode'] != 'Firma') & (av['Adressanredecode'] != 'Herr') & (av['Adressanredecode'] != 'Frau')] = 'other'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ihuangyiran/anaconda2/envs/data_mining/lib/python3.6/site-packages/ipykernel_launcher.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  after removing the cwd from sys.path.\n"
     ]
    }
   ],
   "source": [
    "# Motorcode: distribute according to the first Ziffer\n",
    "av['Motorcode'] = av['Motorcode'].map(lambda x: str(x).lower()[0] if x else x)\n",
    "# Motorcode: change the Motorcode which start with a number\n",
    "av['Motorcode'][av['Motorcode'].map(lambda x: True if re.search('[0-9\\-]', x) else False)] = 'unknown'\n",
    "# Motorcode: change the Motorcode which account smaller than 100\n",
    "# combine items in Motor count < 100 will be set to other\n",
    "tmp = av.copy()\n",
    "tmp['count'] = 1\n",
    "motor = tmp[['Motorcode', 'count']]\n",
    "mc = motor.groupby('Motorcode', as_index = False).count()\n",
    "tmp = pd.merge(av, mc, how = 'left', on = 'Motorcode')\n",
    "tmp.loc[tmp['count'] < 100, 'Motorcode'] = 'others'\n",
    "tmp.drop('count', axis= 1, inplace= True)\n",
    "av = tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Fahrzeugmodellnummer\n",
    "tmp = av.copy()\n",
    "tmp['count'] = 1\n",
    "motor = tmp[['Fahrzeugmodellnummer', 'count']]\n",
    "mc = motor.groupby('Fahrzeugmodellnummer', as_index = False).count()\n",
    "tmp = pd.merge(av, mc, how = 'left', on = 'Fahrzeugmodellnummer')\n",
    "tmp.loc[tmp['count'] < 100, 'Fahrzeugmodellnummer'] = 'others'\n",
    "tmp.drop('count', axis= 1, inplace= True)\n",
    "av = tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# modell nummer\n",
    "tmp = av.copy()\n",
    "tmp['count'] = 1\n",
    "motor = tmp[['Modell', 'count']]\n",
    "mc = motor.groupby('Modell', as_index = False).count()\n",
    "tmp = pd.merge(av, mc, how = 'left', on = 'Modell')\n",
    "tmp.loc[tmp['count'] < 100, 'Modell'] = 'others'\n",
    "tmp.drop('count', axis= 1, inplace= True)\n",
    "av = tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# getriebecode\n",
    "tmp = av.copy()\n",
    "tmp['count'] = 1\n",
    "motor = tmp[['Getriebecode', 'count']]\n",
    "mc = motor.groupby('Getriebecode', as_index = False).count()\n",
    "tmp = pd.merge(av, mc, how = 'left', on = 'Getriebecode')\n",
    "tmp.loc[tmp['count'] < 100, 'Getriebecode'] = 'others'\n",
    "tmp.drop('count', axis= 1, inplace= True)\n",
    "av = tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Typ\n",
    "tmp = av.copy()\n",
    "tmp['count'] = 1\n",
    "motor = tmp[['Typ', 'count']]\n",
    "mc = motor.groupby('Typ', as_index = False).count()\n",
    "tmp = pd.merge(av, mc, how = 'left', on = 'Typ')\n",
    "tmp.loc[tmp['count'] < 100, 'Typ'] = 'others'\n",
    "tmp.drop('count', axis= 1, inplace= True)\n",
    "av = tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "c         61395\n",
       "b          8011\n",
       "a          4763\n",
       "u          2201\n",
       "d          1782\n",
       "z           804\n",
       "others      413\n",
       "Name: Motorcode, dtype: int64"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "av['Motorcode'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "355"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2 = av\n",
    "del av\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 79369 entries, 0 to 79368\n",
      "Data columns (total 65 columns):\n",
      "Auftragsnummer                   79369 non-null object\n",
      "num_act                          79369 non-null int64\n",
      "total AW                         79369 non-null float64\n",
      "Gruppe-Nr                        79369 non-null object\n",
      "num_teile                        79369 non-null int64\n",
      "KM-Stand                         79369 non-null float64\n",
      "Markencode                       79369 non-null object\n",
      "Lagerortcode                     79369 non-null object\n",
      "Auftragsdatum                    79369 non-null datetime64[ns]\n",
      "Adressanredecode                 79369 non-null object\n",
      "Motorcode                        79369 non-null object\n",
      "Fahrzeugmodellnummer             79369 non-null object\n",
      "Modell                           79369 non-null object\n",
      "Typ                              79369 non-null object\n",
      "Getriebecode                     79369 non-null object\n",
      "Gewicht                          79369 non-null object\n",
      "Leistung (KW)                    79369 non-null object\n",
      "Fahrgestellnummer                79369 non-null object\n",
      "Erstzulassungsdatum              79369 non-null object\n",
      "year                             79369 non-null object\n",
      "month                            79369 non-null object\n",
      "day                              79369 non-null object\n",
      "age_auto                         79369 non-null float64\n",
      "age_autohaus                     79369 non-null int64\n",
      "frequent_in_2014                 79369 non-null int64\n",
      "frequent_in_2015                 79369 non-null int64\n",
      "frequent_in_2016                 79369 non-null int64\n",
      "frequent_in_2017                 79369 non-null int64\n",
      "frequent_in_2018                 79369 non-null int64\n",
      "frequent_in_last_1_month         79369 non-null float64\n",
      "frequent_in_last_2_month         79369 non-null float64\n",
      "frequent_in_last_3_month         79369 non-null float64\n",
      "frequent_in_last_4_month         79369 non-null float64\n",
      "frequent_in_last_5_month         79369 non-null float64\n",
      "frequent_in_last_6_month         79369 non-null float64\n",
      "km_distance_from_last            79369 non-null float64\n",
      "max_km_distance_in_group         79369 non-null float64\n",
      "min_km_distance_in_group         79369 non-null float64\n",
      "mean_km_distance_in_group        79369 non-null float64\n",
      "mean_time_distance_in_group      79369 non-null timedelta64[ns]\n",
      "max_time_distance_in_group       79369 non-null timedelta64[ns]\n",
      "min_time_distance_in_group       79369 non-null timedelta64[ns]\n",
      "num_act_shift1                   79369 non-null float64\n",
      "num_act_shift2                   79369 non-null float64\n",
      "num_teile_shift1                 79369 non-null float64\n",
      "num_teile_shift2                 79369 non-null float64\n",
      "repair_age                       79369 non-null timedelta64[ns]\n",
      "time_distance_from_last          79369 non-null timedelta64[ns]\n",
      "total_aw_shift1                  79369 non-null float64\n",
      "total_aw_shift2                  79369 non-null float64\n",
      "trend_of_frequent_in_the_past    79369 non-null float64\n",
      "weather_come_in_last_1_month     79369 non-null bool\n",
      "weather_come_in_last_2_month     79369 non-null bool\n",
      "weather_come_in_last_3_month     79369 non-null bool\n",
      "weather_come_in_last_4_month     79369 non-null bool\n",
      "weather_come_in_last_5_month     79369 non-null bool\n",
      "weather_come_in_last_6_month     79369 non-null bool\n",
      "weather_come_in_1_month          79369 non-null bool\n",
      "weather_come_in_2_month          79369 non-null bool\n",
      "weather_come_in_3_month          79369 non-null bool\n",
      "weather_come_in_4_month          79369 non-null bool\n",
      "weather_come_in_5_month          79369 non-null bool\n",
      "weather_come_in_6_month          79369 non-null bool\n",
      "mean_km_distance_in_total        79369 non-null float64\n",
      "mean_time_distance_in_total      79369 non-null timedelta64[ns]\n",
      "dtypes: bool(12), datetime64[ns](1), float64(21), int64(8), object(17), timedelta64[ns](6)\n",
      "memory usage: 33.6+ MB\n"
     ]
    }
   ],
   "source": [
    "df2.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2    17671\n",
       "4    17534\n",
       "3    16977\n",
       "1    16812\n",
       "5    10375\n",
       "Name: age_autohaus, dtype: int64"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2['age_autohaus'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoding\n",
    "- OrdinalEncoder\n",
    "- OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ec = df2.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "features_to_ordinal = ['Markencode', 'Fahrzeugmodellnummer', 'Modell', 'Getriebecode']\n",
    "ordinal_encoder = OrdinalEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "34"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# select faetures\n",
    "sf = ec[features_to_ordinal]\n",
    "# name\n",
    "name_ordinal_encode = [f + '_ordinal_encode' for f in features_to_ordinal]\n",
    "# fit and transform\n",
    "ordinal_encoder.fit(sf)\n",
    "tmp = ordinal_encoder.transform(sf)\n",
    "# create Dataframe\n",
    "tmp = pd.DataFrame(tmp, columns = name_ordinal_encode)\n",
    "# combine\n",
    "ec = pd.concat([ec, tmp], axis = 1)\n",
    "# del and gc\n",
    "del sf, tmp, features_to_ordinal, ordinal_encoder\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "features_to_onehot = ['Lagerortcode', 'Adressanredecode', 'Motorcode', 'age_autohaus', 'Typ']\n",
    "onehot_encoder = OneHotEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for feature in features_to_onehot:\n",
    "    sf = ec[feature]\n",
    "    tmp = onehot_encoder.fit_transform(sf.values.reshape(-1, 1)).toarray()\n",
    "    _, l = tmp.shape\n",
    "    columns = [feature + '_onehot_' + str(i) for i in range(l)]\n",
    "    tmp = pd.DataFrame(tmp, columns = columns)\n",
    "    ec = pd.concat([ec, tmp], axis = 1)\n",
    "# del and gc\n",
    "del sf, tmp, columns\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 79369 entries, 0 to 79368\n",
      "Data columns (total 97 columns):\n",
      "Auftragsnummer                         79369 non-null object\n",
      "num_act                                79369 non-null int64\n",
      "total AW                               79369 non-null float64\n",
      "Gruppe-Nr                              79369 non-null object\n",
      "num_teile                              79369 non-null int64\n",
      "KM-Stand                               79369 non-null float64\n",
      "Markencode                             79369 non-null object\n",
      "Lagerortcode                           79369 non-null object\n",
      "Auftragsdatum                          79369 non-null datetime64[ns]\n",
      "Adressanredecode                       79369 non-null object\n",
      "Motorcode                              79369 non-null object\n",
      "Fahrzeugmodellnummer                   79369 non-null object\n",
      "Modell                                 79369 non-null object\n",
      "Typ                                    79369 non-null object\n",
      "Getriebecode                           79369 non-null object\n",
      "Gewicht                                79369 non-null object\n",
      "Leistung (KW)                          79369 non-null object\n",
      "Fahrgestellnummer                      79369 non-null object\n",
      "Erstzulassungsdatum                    79369 non-null object\n",
      "year                                   79369 non-null object\n",
      "month                                  79369 non-null object\n",
      "day                                    79369 non-null object\n",
      "age_auto                               79369 non-null float64\n",
      "age_autohaus                           79369 non-null int64\n",
      "frequent_in_2014                       79369 non-null int64\n",
      "frequent_in_2015                       79369 non-null int64\n",
      "frequent_in_2016                       79369 non-null int64\n",
      "frequent_in_2017                       79369 non-null int64\n",
      "frequent_in_2018                       79369 non-null int64\n",
      "frequent_in_last_1_month               79369 non-null float64\n",
      "frequent_in_last_2_month               79369 non-null float64\n",
      "frequent_in_last_3_month               79369 non-null float64\n",
      "frequent_in_last_4_month               79369 non-null float64\n",
      "frequent_in_last_5_month               79369 non-null float64\n",
      "frequent_in_last_6_month               79369 non-null float64\n",
      "km_distance_from_last                  79369 non-null float64\n",
      "max_km_distance_in_group               79369 non-null float64\n",
      "min_km_distance_in_group               79369 non-null float64\n",
      "mean_km_distance_in_group              79369 non-null float64\n",
      "mean_time_distance_in_group            79369 non-null timedelta64[ns]\n",
      "max_time_distance_in_group             79369 non-null timedelta64[ns]\n",
      "min_time_distance_in_group             79369 non-null timedelta64[ns]\n",
      "num_act_shift1                         79369 non-null float64\n",
      "num_act_shift2                         79369 non-null float64\n",
      "num_teile_shift1                       79369 non-null float64\n",
      "num_teile_shift2                       79369 non-null float64\n",
      "repair_age                             79369 non-null timedelta64[ns]\n",
      "time_distance_from_last                79369 non-null timedelta64[ns]\n",
      "total_aw_shift1                        79369 non-null float64\n",
      "total_aw_shift2                        79369 non-null float64\n",
      "trend_of_frequent_in_the_past          79369 non-null float64\n",
      "weather_come_in_last_1_month           79369 non-null bool\n",
      "weather_come_in_last_2_month           79369 non-null bool\n",
      "weather_come_in_last_3_month           79369 non-null bool\n",
      "weather_come_in_last_4_month           79369 non-null bool\n",
      "weather_come_in_last_5_month           79369 non-null bool\n",
      "weather_come_in_last_6_month           79369 non-null bool\n",
      "weather_come_in_1_month                79369 non-null bool\n",
      "weather_come_in_2_month                79369 non-null bool\n",
      "weather_come_in_3_month                79369 non-null bool\n",
      "weather_come_in_4_month                79369 non-null bool\n",
      "weather_come_in_5_month                79369 non-null bool\n",
      "weather_come_in_6_month                79369 non-null bool\n",
      "mean_km_distance_in_total              79369 non-null float64\n",
      "mean_time_distance_in_total            79369 non-null timedelta64[ns]\n",
      "Markencode_ordinal_encode              79369 non-null float64\n",
      "Fahrzeugmodellnummer_ordinal_encode    79369 non-null float64\n",
      "Modell_ordinal_encode                  79369 non-null float64\n",
      "Getriebecode_ordinal_encode            79369 non-null float64\n",
      "Lagerortcode_onehot_0                  79369 non-null float64\n",
      "Lagerortcode_onehot_1                  79369 non-null float64\n",
      "Lagerortcode_onehot_2                  79369 non-null float64\n",
      "Lagerortcode_onehot_3                  79369 non-null float64\n",
      "Lagerortcode_onehot_4                  79369 non-null float64\n",
      "Lagerortcode_onehot_5                  79369 non-null float64\n",
      "Lagerortcode_onehot_6                  79369 non-null float64\n",
      "Lagerortcode_onehot_7                  79369 non-null float64\n",
      "Lagerortcode_onehot_8                  79369 non-null float64\n",
      "Adressanredecode_onehot_0              79369 non-null float64\n",
      "Adressanredecode_onehot_1              79369 non-null float64\n",
      "Adressanredecode_onehot_2              79369 non-null float64\n",
      "Adressanredecode_onehot_3              79369 non-null float64\n",
      "Motorcode_onehot_0                     79369 non-null float64\n",
      "Motorcode_onehot_1                     79369 non-null float64\n",
      "Motorcode_onehot_2                     79369 non-null float64\n",
      "Motorcode_onehot_3                     79369 non-null float64\n",
      "Motorcode_onehot_4                     79369 non-null float64\n",
      "Motorcode_onehot_5                     79369 non-null float64\n",
      "Motorcode_onehot_6                     79369 non-null float64\n",
      "age_autohaus_onehot_0                  79369 non-null float64\n",
      "age_autohaus_onehot_1                  79369 non-null float64\n",
      "age_autohaus_onehot_2                  79369 non-null float64\n",
      "age_autohaus_onehot_3                  79369 non-null float64\n",
      "age_autohaus_onehot_4                  79369 non-null float64\n",
      "Typ_onehot_0                           79369 non-null float64\n",
      "Typ_onehot_1                           79369 non-null float64\n",
      "Typ_onehot_2                           79369 non-null float64\n",
      "dtypes: bool(12), datetime64[ns](1), float64(53), int64(8), object(17), timedelta64[ns](6)\n",
      "memory usage: 53.0+ MB\n"
     ]
    }
   ],
   "source": [
    "ec.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "629"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2 = ec\n",
    "del ec\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Auftragsnummer',\n",
       " 'num_act',\n",
       " 'total AW',\n",
       " 'Gruppe-Nr',\n",
       " 'num_teile',\n",
       " 'KM-Stand',\n",
       " 'Markencode',\n",
       " 'Lagerortcode',\n",
       " 'Auftragsdatum',\n",
       " 'Adressanredecode',\n",
       " 'Motorcode',\n",
       " 'Fahrzeugmodellnummer',\n",
       " 'Modell',\n",
       " 'Typ',\n",
       " 'Getriebecode',\n",
       " 'Gewicht',\n",
       " 'Leistung (KW)',\n",
       " 'Fahrgestellnummer',\n",
       " 'Erstzulassungsdatum',\n",
       " 'year',\n",
       " 'month',\n",
       " 'day',\n",
       " 'age_auto',\n",
       " 'age_autohaus',\n",
       " 'frequent_in_2014',\n",
       " 'frequent_in_2015',\n",
       " 'frequent_in_2016',\n",
       " 'frequent_in_2017',\n",
       " 'frequent_in_2018',\n",
       " 'frequent_in_last_1_month',\n",
       " 'frequent_in_last_2_month',\n",
       " 'frequent_in_last_3_month',\n",
       " 'frequent_in_last_4_month',\n",
       " 'frequent_in_last_5_month',\n",
       " 'frequent_in_last_6_month',\n",
       " 'km_distance_from_last',\n",
       " 'max_km_distance_in_group',\n",
       " 'min_km_distance_in_group',\n",
       " 'mean_km_distance_in_group',\n",
       " 'mean_time_distance_in_group',\n",
       " 'max_time_distance_in_group',\n",
       " 'min_time_distance_in_group',\n",
       " 'num_act_shift1',\n",
       " 'num_act_shift2',\n",
       " 'num_teile_shift1',\n",
       " 'num_teile_shift2',\n",
       " 'repair_age',\n",
       " 'time_distance_from_last',\n",
       " 'total_aw_shift1',\n",
       " 'total_aw_shift2',\n",
       " 'trend_of_frequent_in_the_past',\n",
       " 'weather_come_in_last_1_month',\n",
       " 'weather_come_in_last_2_month',\n",
       " 'weather_come_in_last_3_month',\n",
       " 'weather_come_in_last_4_month',\n",
       " 'weather_come_in_last_5_month',\n",
       " 'weather_come_in_last_6_month',\n",
       " 'weather_come_in_1_month',\n",
       " 'weather_come_in_2_month',\n",
       " 'weather_come_in_3_month',\n",
       " 'weather_come_in_4_month',\n",
       " 'weather_come_in_5_month',\n",
       " 'weather_come_in_6_month',\n",
       " 'mean_km_distance_in_total',\n",
       " 'mean_time_distance_in_total',\n",
       " 'Markencode_ordinal_encode',\n",
       " 'Fahrzeugmodellnummer_ordinal_encode',\n",
       " 'Modell_ordinal_encode',\n",
       " 'Getriebecode_ordinal_encode',\n",
       " 'Lagerortcode_onehot_0',\n",
       " 'Lagerortcode_onehot_1',\n",
       " 'Lagerortcode_onehot_2',\n",
       " 'Lagerortcode_onehot_3',\n",
       " 'Lagerortcode_onehot_4',\n",
       " 'Lagerortcode_onehot_5',\n",
       " 'Lagerortcode_onehot_6',\n",
       " 'Lagerortcode_onehot_7',\n",
       " 'Lagerortcode_onehot_8',\n",
       " 'Adressanredecode_onehot_0',\n",
       " 'Adressanredecode_onehot_1',\n",
       " 'Adressanredecode_onehot_2',\n",
       " 'Adressanredecode_onehot_3',\n",
       " 'Motorcode_onehot_0',\n",
       " 'Motorcode_onehot_1',\n",
       " 'Motorcode_onehot_2',\n",
       " 'Motorcode_onehot_3',\n",
       " 'Motorcode_onehot_4',\n",
       " 'Motorcode_onehot_5',\n",
       " 'Motorcode_onehot_6',\n",
       " 'age_autohaus_onehot_0',\n",
       " 'age_autohaus_onehot_1',\n",
       " 'age_autohaus_onehot_2',\n",
       " 'age_autohaus_onehot_3',\n",
       " 'age_autohaus_onehot_4',\n",
       " 'Typ_onehot_0',\n",
       " 'Typ_onehot_1',\n",
       " 'Typ_onehot_2']"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(df2.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2    17671\n",
       "4    17534\n",
       "3    16977\n",
       "1    16812\n",
       "5    10375\n",
       "Name: age_autohaus, dtype: int64"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2['age_autohaus'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discretization\n",
    "not tested jet\n",
    "\n",
    "- 直接等深分箱吗？？？bining\n",
    "- 聚类？？？\n",
    "- 平滑处理？？？\n",
    "- MDLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fc = df2.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Gewicht\n",
    "fc['Gewicht'] = fc['Gewicht'].map(lambda x: re.sub(',', '.', x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# binning, number of bin???????\n",
    "# use onehot to do the encode, would be better???\n",
    "# when n_bins larger als 3, it throws ecxeption: bins must be monotonically increasing or decreasing ???\n",
    "n_bins = 3\n",
    "bin_width = KBinsDiscretizer(n_bins = n_bins, encode = 'onehot', strategy = 'uniform')\n",
    "bin_deep = KBinsDiscretizer(n_bins = n_bins, encode = 'onehot', strategy = 'quantile')\n",
    "bin_kmeans = KBinsDiscretizer(n_bins = n_bins, encode = 'onehot', strategy = 'kmeans')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# features to do the binning\n",
    "features_to_binning = ['num_act', 'total AW', 'KM-Stand', 'Gewicht', 'Leistung (KW)', 'age_auto',\n",
    "                       'Markencode_ordinal_encode', 'Fahrzeugmodellnummer_ordinal_encode', 'Modell_ordinal_encode']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 79369 entries, 0 to 79368\n",
      "Data columns (total 9 columns):\n",
      "num_act                                79369 non-null int64\n",
      "total AW                               79369 non-null float64\n",
      "KM-Stand                               79369 non-null float64\n",
      "Gewicht                                79369 non-null object\n",
      "Leistung (KW)                          79369 non-null object\n",
      "age_auto                               79369 non-null float64\n",
      "Markencode_ordinal_encode              79369 non-null float64\n",
      "Fahrzeugmodellnummer_ordinal_encode    79369 non-null float64\n",
      "Modell_ordinal_encode                  79369 non-null float64\n",
      "dtypes: float64(6), int64(1), object(2)\n",
      "memory usage: 6.1+ MB\n"
     ]
    }
   ],
   "source": [
    "#fc['Modell_ordinal_encode'].value_counts()\n",
    "tmp = fc[features_to_binning]\n",
    "tmp.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ihuangyiran/anaconda2/envs/data_mining/lib/python3.6/site-packages/sklearn/cluster/k_means_.py:968: ConvergenceWarning: Number of distinct clusters (2) found smaller than n_clusters (3). Possibly due to duplicate points in X.\n",
      "  return_n_iter=True)\n"
     ]
    }
   ],
   "source": [
    "# bining with ordinal encoder\n",
    "# select faetures\n",
    "tmp = fc[features_to_binning]\n",
    "# set name\n",
    "name_width = [f + '_bin_width' for f in features_to_binning]\n",
    "name_deep = [f + '_bin_deep' for f in features_to_binning]\n",
    "name_kmeans = [f + '_bin_kmeans' for f in features_to_binning]\n",
    "# fit and transform\n",
    "bin_width.fit(tmp)\n",
    "bin_deep.fit(tmp)\n",
    "bin_kmeans.fit(tmp)\n",
    "tmp_w = bin_width.transform(tmp)\n",
    "tmp_d = bin_deep.transform(tmp)\n",
    "tmp_k = bin_kmeans.transform(tmp)\n",
    "# columns names\n",
    "cols_name_width = []\n",
    "cols_name_deep = []\n",
    "cols_name_kmean = []\n",
    "for feature in features_to_binning:\n",
    "    for i in range(n_bins):\n",
    "        cols_name_width.append(feature + '_onehot_bin_width_' + str(i))\n",
    "        cols_name_deep.append(feature + '_onehot_bin_deep_' + str(i))\n",
    "        cols_name_kmean.append(feature + '_onehot_bin_kmean_'+ str(i))\n",
    "# transform to Dataframe\n",
    "tmp_w = pd.DataFrame(tmp_w.toarray(), columns = cols_name_width)\n",
    "tmp_d = pd.DataFrame(tmp_d.toarray(), columns = cols_name_deep)\n",
    "tmp_k = pd.DataFrame(tmp_k.toarray(), columns = cols_name_kmean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "35"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# concatenate\n",
    "fc = pd.concat([fc, tmp_w, tmp_d, tmp_k], axis = 1)\n",
    "# del and gc\n",
    "del tmp, tmp_w, tmp_d, tmp_k\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 79369 entries, 0 to 79368\n",
      "Columns: 178 entries, Auftragsnummer to Modell_ordinal_encode_onehot_bin_kmean_2\n",
      "dtypes: bool(12), datetime64[ns](1), float64(134), int64(8), object(17), timedelta64[ns](6)\n",
      "memory usage: 102.0+ MB\n"
     ]
    }
   ],
   "source": [
    "fc.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 出现很多cutpoint在边界的情况，不知道是怎么回事，，而且分段也只是分成两段，难道是有什么参数要设置，介绍上并没有说啊？？？？？？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#features_to_mdlp = ['num_act', 'total AW', 'KM-Stand', 'Gewicht', 'Leistung (KW)', \n",
    "#                       'Markencode_ordinal_encode', 'Fahrzeugmodellnummer_ordinal_encode', 'Modell_ordinal_encode',\n",
    "#                      'Typ_ordinal_encode']\n",
    "features_to_mdlp = ['num_act', 'total AW', 'KM-Stand', 'Gewicht', 'Leistung (KW)', \n",
    "                    'km_distance_from_last', 'num_teile', 'time_distance_from_last']\n",
    "label = 'weather_come_in_4_month'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 添加dis的属性：km_distance_from_last, time_distance_from_last， mean_km_distance_in_group\n",
    "fc['time_distance_from_last'] = fc['time_distance_from_last'].map(lambda x: x.days)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#fc[features_to_mdlp]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ihuangyiran/anaconda2/envs/data_mining/lib/python3.6/site-packages/ipykernel_launcher.py:52: FutureWarning: convert_objects is deprecated.  To re-infer data dtypes for object columns, use DataFrame.infer_objects()\n",
      "For all other conversions use the data-type specific converters pd.to_datetime, pd.to_timedelta and pd.to_numeric.\n"
     ]
    }
   ],
   "source": [
    "out_path_data, out_path_bins, return_bins, class_label, features = None, None, False, None, None\n",
    "features = features_to_mdlp\n",
    "class_label = label\n",
    "discretizer = MDLP_Discretizer(dataset=fc, class_label=class_label, features=features, out_path_data=out_path_data, out_path_bins=out_path_bins)\n",
    "tmp = discretizer.apply_cutpoints(out_data_path=out_path_data, out_bins_path=out_path_bins)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 这个tmp是分段后的结果，里面包含了所有的属性，但是，原属性并不是就没有用了，比如，year就可以用来区分划分训练和测试集，所以，最后是把，改变了的属性提取出来，然后和原属性进行结合！！！！另外结果还需要再次encoding，这次可以使用ordinal的方法。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tmp.to_pickle('discretizer.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "features_to_encode = ['num_act', 'total AW', 'KM-Stand', 'Gewicht', 'Leistung (KW)', \n",
    "                    'km_distance_from_last', 'num_teile', 'time_distance_from_last']\n",
    "ordinal_encoder = OrdinalEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "541"
      ]
     },
     "execution_count": 263,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# select faetures\n",
    "sf = df[features_to_encode]\n",
    "# name\n",
    "name_ordinal_encode = [f + '_ordinal_encode_after_mdlp' for f in features_to_encode]\n",
    "# fit and transform\n",
    "ordinal_encoder.fit(sf)\n",
    "tmp = ordinal_encoder.transform(sf)\n",
    "# create Dataframe\n",
    "tmp = pd.DataFrame(tmp, columns = name_ordinal_encode)\n",
    "# combine\n",
    "df = pd.concat([df, tmp], axis = 1)\n",
    "# del and gc\n",
    "del sf, tmp, features_to_encode, ordinal_encoder\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#sns.kdeplot(fc['total AW'][fc['weather_come_in_1_month'] == True].map(lambda x: math.ceil(x)))\n",
    "#sns.kdeplot(fc['total AW'][fc['weather_come_in_1_month'] == False].map(lambda x: math.ceil(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#fc.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## standardization and QuantileTransformer, PowerTransformer\n",
    "not tested jet!!! spring\n",
    "- StandardScaler\n",
    "- MinMaxScaler\n",
    "- QuantileTransformer: it smooths out unusual distributions and is less influenced by outliers than scaling methods. It does, however, distort correlations and distances within and across features.\n",
    "- PowerTransformer: Apply a power transform featurewise to make data more Gaussian-like.\n",
    "- difference between Standardization and normalization ???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_pickle('./discretizer.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Auftragsnummer',\n",
       " 'num_act',\n",
       " 'total AW',\n",
       " 'Gruppe-Nr',\n",
       " 'num_teile',\n",
       " 'KM-Stand',\n",
       " 'Markencode',\n",
       " 'Lagerortcode',\n",
       " 'Auftragsdatum',\n",
       " 'Adressanredecode',\n",
       " 'Motorcode',\n",
       " 'Fahrzeugmodellnummer',\n",
       " 'Modell',\n",
       " 'Typ',\n",
       " 'Getriebecode',\n",
       " 'Gewicht',\n",
       " 'Leistung (KW)',\n",
       " 'Fahrgestellnummer',\n",
       " 'Erstzulassungsdatum',\n",
       " 'year',\n",
       " 'month',\n",
       " 'day',\n",
       " 'age_auto',\n",
       " 'age_autohaus',\n",
       " 'frequent_in_2014',\n",
       " 'frequent_in_2015',\n",
       " 'frequent_in_2016',\n",
       " 'frequent_in_2017',\n",
       " 'frequent_in_2018',\n",
       " 'frequent_in_last_1_month',\n",
       " 'frequent_in_last_2_month',\n",
       " 'frequent_in_last_3_month',\n",
       " 'frequent_in_last_4_month',\n",
       " 'frequent_in_last_5_month',\n",
       " 'frequent_in_last_6_month',\n",
       " 'km_distance_from_last',\n",
       " 'max_km_distance_in_group',\n",
       " 'min_km_distance_in_group',\n",
       " 'mean_km_distance_in_group',\n",
       " 'mean_time_distance_in_group',\n",
       " 'max_time_distance_in_group',\n",
       " 'min_time_distance_in_group',\n",
       " 'num_act_shift1',\n",
       " 'num_act_shift2',\n",
       " 'num_teile_shift1',\n",
       " 'num_teile_shift2',\n",
       " 'repair_age',\n",
       " 'time_distance_from_last',\n",
       " 'total_aw_shift1',\n",
       " 'total_aw_shift2',\n",
       " 'trend_of_frequent_in_the_past',\n",
       " 'weather_come_in_last_1_month',\n",
       " 'weather_come_in_last_2_month',\n",
       " 'weather_come_in_last_3_month',\n",
       " 'weather_come_in_last_4_month',\n",
       " 'weather_come_in_last_5_month',\n",
       " 'weather_come_in_last_6_month',\n",
       " 'weather_come_in_1_month',\n",
       " 'weather_come_in_2_month',\n",
       " 'weather_come_in_3_month',\n",
       " 'weather_come_in_4_month',\n",
       " 'weather_come_in_4_month',\n",
       " 'weather_come_in_5_month',\n",
       " 'weather_come_in_6_month',\n",
       " 'mean_km_distance_in_total',\n",
       " 'mean_time_distance_in_total',\n",
       " 'Markencode_ordinal_encode',\n",
       " 'Fahrzeugmodellnummer_ordinal_encode',\n",
       " 'Modell_ordinal_encode',\n",
       " 'Getriebecode_ordinal_encode',\n",
       " 'Lagerortcode_onehot_0',\n",
       " 'Lagerortcode_onehot_1',\n",
       " 'Lagerortcode_onehot_2',\n",
       " 'Lagerortcode_onehot_3',\n",
       " 'Lagerortcode_onehot_4',\n",
       " 'Lagerortcode_onehot_5',\n",
       " 'Lagerortcode_onehot_6',\n",
       " 'Lagerortcode_onehot_7',\n",
       " 'Lagerortcode_onehot_8',\n",
       " 'Adressanredecode_onehot_0',\n",
       " 'Adressanredecode_onehot_1',\n",
       " 'Adressanredecode_onehot_2',\n",
       " 'Adressanredecode_onehot_3',\n",
       " 'Motorcode_onehot_0',\n",
       " 'Motorcode_onehot_1',\n",
       " 'Motorcode_onehot_2',\n",
       " 'Motorcode_onehot_3',\n",
       " 'Motorcode_onehot_4',\n",
       " 'Motorcode_onehot_5',\n",
       " 'Motorcode_onehot_6',\n",
       " 'age_autohaus_onehot_0',\n",
       " 'age_autohaus_onehot_1',\n",
       " 'age_autohaus_onehot_2',\n",
       " 'age_autohaus_onehot_3',\n",
       " 'age_autohaus_onehot_4',\n",
       " 'Typ_onehot_0',\n",
       " 'Typ_onehot_1',\n",
       " 'Typ_onehot_2',\n",
       " 'num_act_onehot_bin_width_0',\n",
       " 'num_act_onehot_bin_width_1',\n",
       " 'num_act_onehot_bin_width_2',\n",
       " 'total AW_onehot_bin_width_0',\n",
       " 'total AW_onehot_bin_width_1',\n",
       " 'total AW_onehot_bin_width_2',\n",
       " 'KM-Stand_onehot_bin_width_0',\n",
       " 'KM-Stand_onehot_bin_width_1',\n",
       " 'KM-Stand_onehot_bin_width_2',\n",
       " 'Gewicht_onehot_bin_width_0',\n",
       " 'Gewicht_onehot_bin_width_1',\n",
       " 'Gewicht_onehot_bin_width_2',\n",
       " 'Leistung (KW)_onehot_bin_width_0',\n",
       " 'Leistung (KW)_onehot_bin_width_1',\n",
       " 'Leistung (KW)_onehot_bin_width_2',\n",
       " 'age_auto_onehot_bin_width_0',\n",
       " 'age_auto_onehot_bin_width_1',\n",
       " 'age_auto_onehot_bin_width_2',\n",
       " 'Markencode_ordinal_encode_onehot_bin_width_0',\n",
       " 'Markencode_ordinal_encode_onehot_bin_width_1',\n",
       " 'Markencode_ordinal_encode_onehot_bin_width_2',\n",
       " 'Fahrzeugmodellnummer_ordinal_encode_onehot_bin_width_0',\n",
       " 'Fahrzeugmodellnummer_ordinal_encode_onehot_bin_width_1',\n",
       " 'Fahrzeugmodellnummer_ordinal_encode_onehot_bin_width_2',\n",
       " 'Modell_ordinal_encode_onehot_bin_width_0',\n",
       " 'Modell_ordinal_encode_onehot_bin_width_1',\n",
       " 'Modell_ordinal_encode_onehot_bin_width_2',\n",
       " 'num_act_onehot_bin_deep_0',\n",
       " 'num_act_onehot_bin_deep_1',\n",
       " 'num_act_onehot_bin_deep_2',\n",
       " 'total AW_onehot_bin_deep_0',\n",
       " 'total AW_onehot_bin_deep_1',\n",
       " 'total AW_onehot_bin_deep_2',\n",
       " 'KM-Stand_onehot_bin_deep_0',\n",
       " 'KM-Stand_onehot_bin_deep_1',\n",
       " 'KM-Stand_onehot_bin_deep_2',\n",
       " 'Gewicht_onehot_bin_deep_0',\n",
       " 'Gewicht_onehot_bin_deep_1',\n",
       " 'Gewicht_onehot_bin_deep_2',\n",
       " 'Leistung (KW)_onehot_bin_deep_0',\n",
       " 'Leistung (KW)_onehot_bin_deep_1',\n",
       " 'Leistung (KW)_onehot_bin_deep_2',\n",
       " 'age_auto_onehot_bin_deep_0',\n",
       " 'age_auto_onehot_bin_deep_1',\n",
       " 'age_auto_onehot_bin_deep_2',\n",
       " 'Markencode_ordinal_encode_onehot_bin_deep_0',\n",
       " 'Markencode_ordinal_encode_onehot_bin_deep_1',\n",
       " 'Markencode_ordinal_encode_onehot_bin_deep_2',\n",
       " 'Fahrzeugmodellnummer_ordinal_encode_onehot_bin_deep_0',\n",
       " 'Fahrzeugmodellnummer_ordinal_encode_onehot_bin_deep_1',\n",
       " 'Fahrzeugmodellnummer_ordinal_encode_onehot_bin_deep_2',\n",
       " 'Modell_ordinal_encode_onehot_bin_deep_0',\n",
       " 'Modell_ordinal_encode_onehot_bin_deep_1',\n",
       " 'Modell_ordinal_encode_onehot_bin_deep_2',\n",
       " 'num_act_onehot_bin_kmean_0',\n",
       " 'num_act_onehot_bin_kmean_1',\n",
       " 'num_act_onehot_bin_kmean_2',\n",
       " 'total AW_onehot_bin_kmean_0',\n",
       " 'total AW_onehot_bin_kmean_1',\n",
       " 'total AW_onehot_bin_kmean_2',\n",
       " 'KM-Stand_onehot_bin_kmean_0',\n",
       " 'KM-Stand_onehot_bin_kmean_1',\n",
       " 'KM-Stand_onehot_bin_kmean_2',\n",
       " 'Gewicht_onehot_bin_kmean_0',\n",
       " 'Gewicht_onehot_bin_kmean_1',\n",
       " 'Gewicht_onehot_bin_kmean_2',\n",
       " 'Leistung (KW)_onehot_bin_kmean_0',\n",
       " 'Leistung (KW)_onehot_bin_kmean_1',\n",
       " 'Leistung (KW)_onehot_bin_kmean_2',\n",
       " 'age_auto_onehot_bin_kmean_0',\n",
       " 'age_auto_onehot_bin_kmean_1',\n",
       " 'age_auto_onehot_bin_kmean_2',\n",
       " 'Markencode_ordinal_encode_onehot_bin_kmean_0',\n",
       " 'Markencode_ordinal_encode_onehot_bin_kmean_1',\n",
       " 'Markencode_ordinal_encode_onehot_bin_kmean_2',\n",
       " 'Fahrzeugmodellnummer_ordinal_encode_onehot_bin_kmean_0',\n",
       " 'Fahrzeugmodellnummer_ordinal_encode_onehot_bin_kmean_1',\n",
       " 'Fahrzeugmodellnummer_ordinal_encode_onehot_bin_kmean_2',\n",
       " 'Modell_ordinal_encode_onehot_bin_kmean_0',\n",
       " 'Modell_ordinal_encode_onehot_bin_kmean_1',\n",
       " 'Modell_ordinal_encode_onehot_bin_kmean_2',\n",
       " 'weather_come_in_n_month']"
      ]
     },
     "execution_count": 261,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df['weather_come_in_n_month'] = df['weather_come_in_4_month'].iloc[:, 0]\n",
    "df['trend_of_frequent_in_the_past'] = df['trend_of_frequent_in_the_past'].map(lambda x: 1 if x>0 else (-1 if x<0 else 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# train and test data distribution\n",
    "test = df[df['year'] == '2018']\n",
    "train = df[df['year'] != '2018']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train = train[['frequent_in_last_3_month', 'frequent_in_last_4_month', 'frequent_in_last_5_month',\n",
    "               'mean_km_distance_in_group',\n",
    "               'trend_of_frequent_in_the_past',\n",
    "               'km_distance_from_last_ordinal_encode_after_mdlp', 'time_distance_from_last_ordinal_encode_after_mdlp',\n",
    "               'num_act_ordinal_encode_after_mdlp', 'total AW_ordinal_encode_after_mdlp',\n",
    "               'KM-Stand_ordinal_encode_after_mdlp', 'Leistung (KW)_ordinal_encode_after_mdlp',\n",
    "               'num_teile_ordinal_encode_after_mdlp']]\n",
    "Y_train = train['weather_come_in_n_month']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "true_test = test[test['weather_come_in_n_month'] == True] #4457\n",
    "false_test = test[test['weather_come_in_n_month'] == False] # 5918\n",
    "test = pd.concat([true_test.sample(4000), false_test.sample(4000)], axis = 0)\n",
    "X_test = test[['frequent_in_last_3_month', 'frequent_in_last_4_month', 'frequent_in_last_5_month',\n",
    "               'mean_km_distance_in_group',\n",
    "               'trend_of_frequent_in_the_past',\n",
    "               'km_distance_from_last_ordinal_encode_after_mdlp', 'time_distance_from_last_ordinal_encode_after_mdlp',\n",
    "               'num_act_ordinal_encode_after_mdlp', 'total AW_ordinal_encode_after_mdlp',\n",
    "               'KM-Stand_ordinal_encode_after_mdlp', 'Leistung (KW)_ordinal_encode_after_mdlp',\n",
    "               'num_teile_ordinal_encode_after_mdlp']]\n",
    "Y_test = test['weather_come_in_n_month']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# only do it for neural network training\n",
    "std_scaler = StandardScaler(copy = True, with_mean = True, with_std = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ihuangyiran/anaconda2/envs/data_mining/lib/python3.6/site-packages/sklearn/preprocessing/data.py:617: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  return self.partial_fit(X, y)\n",
      "/Users/ihuangyiran/anaconda2/envs/data_mining/lib/python3.6/site-packages/ipykernel_launcher.py:8: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "127"
      ]
     },
     "execution_count": 280,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# select features\n",
    "#sf = X_train[features_to_scale]\n",
    "sf = X_train\n",
    "# name \n",
    "name_std_scale = [f + '_std_scale' for f in features_to_scale]\n",
    "# fit and transform\n",
    "std_scaler.fit(sf)\n",
    "tmp = std_scaler.transform(sf)\n",
    "# rename\n",
    "tmp = pd.DataFrame(tmp, columns = X_train.columns)\n",
    "# combine\n",
    "#X_train = pd.concat([X_train, tmp], axis = 1)\n",
    "X_train = tmp\n",
    "# del and gc\n",
    "del sf, tmp\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ihuangyiran/anaconda2/envs/data_mining/lib/python3.6/site-packages/ipykernel_launcher.py:2: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 281,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# standardization of test data\n",
    "tmp = std_scaler.transform(X_test)\n",
    "X_test = pd.DataFrame(tmp, columns = X_test.columns)\n",
    "del tmp\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.67664283, -0.74510802, -0.81091908, ...,  0.36062281,\n",
       "        -0.63924577,  1.00888404],\n",
       "       [-0.67664283, -0.74510802, -0.81091908, ...,  0.36062281,\n",
       "        -0.63924577, -0.79426456],\n",
       "       [ 0.62191063,  0.41137499,  0.22936486, ...,  0.36062281,\n",
       "        -0.63924577,  1.00888404],\n",
       "       ..., \n",
       "       [-0.67664283, -0.74510802, -0.81091908, ...,  0.36062281,\n",
       "        -0.63924577, -0.79426456],\n",
       "       [-0.67664283, -0.74510802, -0.81091908, ...,  0.36062281,\n",
       "        -0.63924577, -0.79426456],\n",
       "       [ 0.62191063,  0.41137499,  0.22936486, ...,  0.36062281,\n",
       "        -0.63924577, -0.79426456]])"
      ]
     },
     "execution_count": 291,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#quan_scaler = QuantileTransformer()\n",
    "X_train.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# select features\n",
    "#sf = sd[features_to_quantile]\n",
    "# name \n",
    "#name_quan_scale = [f + 'quan_scale' for f in features_to_quantile]\n",
    "# fit and transform\n",
    "#quan_scaler.fit(sf)\n",
    "#tmp = quan_scaler.transform(sf)\n",
    "# rename\n",
    "#tmp = pd.DataFrame(tmp, columns = name_quan_scale)\n",
    "# combine\n",
    "#sd = pd.concat([sd, tmp], axis = 1)\n",
    "# del and gc\n",
    "#del sf, tmp\n",
    "#gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#features_to_power = ['total AW']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#power_scaler = PowerTransformer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# select features\n",
    "#sf = sd[features_to_power]\n",
    "# name\n",
    "#name_power_scale = [f + 'power_scale' for f in features_to_power]\n",
    "# fit and transform\n",
    "#power_scaler.fit(sf)\n",
    "#tmp = power_scaler.transform(sf)\n",
    "# rename\n",
    "#tmp = pd.DataFrame(tmp, columns = name_power_scale)\n",
    "# combine\n",
    "#sd = pd.concat([sd, tmp], axis = 1)\n",
    "# del and gc\n",
    "#del sf, tmp\n",
    "#gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0 [0/68994 (0%)]\tLoss: 0.547318\n",
      "Train Epoch: 0 [1000/68994 (1%)]\tLoss: 0.277469\n",
      "Train Epoch: 0 [2000/68994 (3%)]\tLoss: 0.247449\n",
      "Train Epoch: 0 [3000/68994 (4%)]\tLoss: 0.243995\n",
      "Train Epoch: 0 [4000/68994 (6%)]\tLoss: 0.229557\n",
      "Train Epoch: 0 [5000/68994 (7%)]\tLoss: 0.233058\n",
      "Train Epoch: 0 [6000/68994 (9%)]\tLoss: 0.229515\n",
      "Train Epoch: 0 [7000/68994 (10%)]\tLoss: 0.235232\n",
      "Train Epoch: 0 [8000/68994 (12%)]\tLoss: 0.232125\n",
      "Train Epoch: 0 [9000/68994 (13%)]\tLoss: 0.230110\n",
      "Train Epoch: 0 [10000/68994 (14%)]\tLoss: 0.219798\n",
      "Train Epoch: 0 [11000/68994 (16%)]\tLoss: 0.239115\n",
      "Train Epoch: 0 [12000/68994 (17%)]\tLoss: 0.253360\n",
      "Train Epoch: 0 [13000/68994 (19%)]\tLoss: 0.226722\n",
      "Train Epoch: 0 [14000/68994 (20%)]\tLoss: 0.197300\n",
      "Train Epoch: 0 [15000/68994 (22%)]\tLoss: 0.234722\n",
      "Train Epoch: 0 [16000/68994 (23%)]\tLoss: 0.242722\n",
      "Train Epoch: 0 [17000/68994 (25%)]\tLoss: 0.259582\n",
      "Train Epoch: 0 [18000/68994 (26%)]\tLoss: 0.199165\n",
      "Train Epoch: 0 [19000/68994 (28%)]\tLoss: 0.203221\n",
      "Train Epoch: 0 [20000/68994 (29%)]\tLoss: 0.238761\n",
      "Train Epoch: 0 [21000/68994 (30%)]\tLoss: 0.232423\n",
      "Train Epoch: 0 [22000/68994 (32%)]\tLoss: 0.249720\n",
      "Train Epoch: 0 [23000/68994 (33%)]\tLoss: 0.221857\n",
      "Train Epoch: 0 [24000/68994 (35%)]\tLoss: 0.212658\n",
      "Train Epoch: 0 [25000/68994 (36%)]\tLoss: 0.217061\n",
      "Train Epoch: 0 [26000/68994 (38%)]\tLoss: 0.223851\n",
      "Train Epoch: 0 [27000/68994 (39%)]\tLoss: 0.209761\n",
      "Train Epoch: 0 [28000/68994 (41%)]\tLoss: 0.213420\n",
      "Train Epoch: 0 [29000/68994 (42%)]\tLoss: 0.225518\n",
      "Train Epoch: 0 [30000/68994 (43%)]\tLoss: 0.203092\n",
      "Train Epoch: 0 [31000/68994 (45%)]\tLoss: 0.221731\n",
      "Train Epoch: 0 [32000/68994 (46%)]\tLoss: 0.218634\n",
      "Train Epoch: 0 [33000/68994 (48%)]\tLoss: 0.223375\n",
      "Train Epoch: 0 [34000/68994 (49%)]\tLoss: 0.252231\n",
      "Train Epoch: 0 [35000/68994 (51%)]\tLoss: 0.214525\n",
      "Train Epoch: 0 [36000/68994 (52%)]\tLoss: 0.196829\n",
      "Train Epoch: 0 [37000/68994 (54%)]\tLoss: 0.197714\n",
      "Train Epoch: 0 [38000/68994 (55%)]\tLoss: 0.236005\n",
      "Train Epoch: 0 [39000/68994 (57%)]\tLoss: 0.203012\n",
      "Train Epoch: 0 [40000/68994 (58%)]\tLoss: 0.205462\n",
      "Train Epoch: 0 [41000/68994 (59%)]\tLoss: 0.217665\n",
      "Train Epoch: 0 [42000/68994 (61%)]\tLoss: 0.213181\n",
      "Train Epoch: 0 [43000/68994 (62%)]\tLoss: 0.190818\n",
      "Train Epoch: 0 [44000/68994 (64%)]\tLoss: 0.236085\n",
      "Train Epoch: 0 [45000/68994 (65%)]\tLoss: 0.215047\n",
      "Train Epoch: 0 [46000/68994 (67%)]\tLoss: 0.184073\n",
      "Train Epoch: 0 [47000/68994 (68%)]\tLoss: 0.223926\n",
      "Train Epoch: 0 [48000/68994 (70%)]\tLoss: 0.268000\n",
      "Train Epoch: 0 [49000/68994 (71%)]\tLoss: 0.220444\n",
      "Train Epoch: 0 [50000/68994 (72%)]\tLoss: 0.245069\n",
      "Train Epoch: 0 [51000/68994 (74%)]\tLoss: 0.207163\n",
      "Train Epoch: 0 [52000/68994 (75%)]\tLoss: 0.198969\n",
      "Train Epoch: 0 [53000/68994 (77%)]\tLoss: 0.213686\n",
      "Train Epoch: 0 [54000/68994 (78%)]\tLoss: 0.234048\n",
      "Train Epoch: 0 [55000/68994 (80%)]\tLoss: 0.228149\n",
      "Train Epoch: 0 [56000/68994 (81%)]\tLoss: 0.187610\n",
      "Train Epoch: 0 [57000/68994 (83%)]\tLoss: 0.231494\n",
      "Train Epoch: 0 [58000/68994 (84%)]\tLoss: 0.219013\n",
      "Train Epoch: 0 [59000/68994 (86%)]\tLoss: 0.234737\n",
      "Train Epoch: 0 [60000/68994 (87%)]\tLoss: 0.190757\n",
      "Train Epoch: 0 [61000/68994 (88%)]\tLoss: 0.223619\n",
      "Train Epoch: 0 [62000/68994 (90%)]\tLoss: 0.235299\n",
      "Train Epoch: 0 [63000/68994 (91%)]\tLoss: 0.233495\n",
      "Train Epoch: 0 [64000/68994 (93%)]\tLoss: 0.208462\n",
      "Train Epoch: 0 [65000/68994 (94%)]\tLoss: 0.204346\n",
      "Train Epoch: 0 [66000/68994 (96%)]\tLoss: 0.246115\n",
      "Train Epoch: 0 [67000/68994 (97%)]\tLoss: 0.213559\n",
      "Train Epoch: 0 [68000/68994 (99%)]\tLoss: 0.229897\n",
      "====> Epoch: 0 Average train loss: 0.2240\n",
      "====> Epoch: 0 Average test loss: 0.2157 Average hit rate: 65.0000\n",
      "Train Epoch: 1 [0/68994 (0%)]\tLoss: 0.210196\n",
      "Train Epoch: 1 [1000/68994 (1%)]\tLoss: 0.254968\n",
      "Train Epoch: 1 [2000/68994 (3%)]\tLoss: 0.240345\n",
      "Train Epoch: 1 [3000/68994 (4%)]\tLoss: 0.216624\n",
      "Train Epoch: 1 [4000/68994 (6%)]\tLoss: 0.207289\n",
      "Train Epoch: 1 [5000/68994 (7%)]\tLoss: 0.233774\n",
      "Train Epoch: 1 [6000/68994 (9%)]\tLoss: 0.245200\n",
      "Train Epoch: 1 [7000/68994 (10%)]\tLoss: 0.212071\n",
      "Train Epoch: 1 [8000/68994 (12%)]\tLoss: 0.234407\n",
      "Train Epoch: 1 [9000/68994 (13%)]\tLoss: 0.226203\n",
      "Train Epoch: 1 [10000/68994 (14%)]\tLoss: 0.204650\n",
      "Train Epoch: 1 [11000/68994 (16%)]\tLoss: 0.241914\n",
      "Train Epoch: 1 [12000/68994 (17%)]\tLoss: 0.227950\n",
      "Train Epoch: 1 [13000/68994 (19%)]\tLoss: 0.213965\n",
      "Train Epoch: 1 [14000/68994 (20%)]\tLoss: 0.220731\n",
      "Train Epoch: 1 [15000/68994 (22%)]\tLoss: 0.201307\n",
      "Train Epoch: 1 [16000/68994 (23%)]\tLoss: 0.193588\n",
      "Train Epoch: 1 [17000/68994 (25%)]\tLoss: 0.220955\n",
      "Train Epoch: 1 [18000/68994 (26%)]\tLoss: 0.202893\n",
      "Train Epoch: 1 [19000/68994 (28%)]\tLoss: 0.209705\n",
      "Train Epoch: 1 [20000/68994 (29%)]\tLoss: 0.181427\n",
      "Train Epoch: 1 [21000/68994 (30%)]\tLoss: 0.185243\n",
      "Train Epoch: 1 [22000/68994 (32%)]\tLoss: 0.235616\n",
      "Train Epoch: 1 [23000/68994 (33%)]\tLoss: 0.236446\n",
      "Train Epoch: 1 [24000/68994 (35%)]\tLoss: 0.218341\n",
      "Train Epoch: 1 [25000/68994 (36%)]\tLoss: 0.211786\n",
      "Train Epoch: 1 [26000/68994 (38%)]\tLoss: 0.232749\n",
      "Train Epoch: 1 [27000/68994 (39%)]\tLoss: 0.221880\n",
      "Train Epoch: 1 [28000/68994 (41%)]\tLoss: 0.218155\n",
      "Train Epoch: 1 [29000/68994 (42%)]\tLoss: 0.255487\n",
      "Train Epoch: 1 [30000/68994 (43%)]\tLoss: 0.230037\n",
      "Train Epoch: 1 [31000/68994 (45%)]\tLoss: 0.227449\n",
      "Train Epoch: 1 [32000/68994 (46%)]\tLoss: 0.217646\n",
      "Train Epoch: 1 [33000/68994 (48%)]\tLoss: 0.215868\n",
      "Train Epoch: 1 [34000/68994 (49%)]\tLoss: 0.206084\n",
      "Train Epoch: 1 [35000/68994 (51%)]\tLoss: 0.208108\n",
      "Train Epoch: 1 [36000/68994 (52%)]\tLoss: 0.232235\n",
      "Train Epoch: 1 [37000/68994 (54%)]\tLoss: 0.212441\n",
      "Train Epoch: 1 [38000/68994 (55%)]\tLoss: 0.199488\n",
      "Train Epoch: 1 [39000/68994 (57%)]\tLoss: 0.211023\n",
      "Train Epoch: 1 [40000/68994 (58%)]\tLoss: 0.214677\n",
      "Train Epoch: 1 [41000/68994 (59%)]\tLoss: 0.188858\n",
      "Train Epoch: 1 [42000/68994 (61%)]\tLoss: 0.206443\n",
      "Train Epoch: 1 [43000/68994 (62%)]\tLoss: 0.202478\n",
      "Train Epoch: 1 [44000/68994 (64%)]\tLoss: 0.212396\n",
      "Train Epoch: 1 [45000/68994 (65%)]\tLoss: 0.228894\n",
      "Train Epoch: 1 [46000/68994 (67%)]\tLoss: 0.226183\n",
      "Train Epoch: 1 [47000/68994 (68%)]\tLoss: 0.235980\n",
      "Train Epoch: 1 [48000/68994 (70%)]\tLoss: 0.208553\n",
      "Train Epoch: 1 [49000/68994 (71%)]\tLoss: 0.216437\n",
      "Train Epoch: 1 [50000/68994 (72%)]\tLoss: 0.219670\n",
      "Train Epoch: 1 [51000/68994 (74%)]\tLoss: 0.217652\n",
      "Train Epoch: 1 [52000/68994 (75%)]\tLoss: 0.212637\n",
      "Train Epoch: 1 [53000/68994 (77%)]\tLoss: 0.218141\n",
      "Train Epoch: 1 [54000/68994 (78%)]\tLoss: 0.214934\n",
      "Train Epoch: 1 [55000/68994 (80%)]\tLoss: 0.222083\n",
      "Train Epoch: 1 [56000/68994 (81%)]\tLoss: 0.224458\n",
      "Train Epoch: 1 [57000/68994 (83%)]\tLoss: 0.209415\n",
      "Train Epoch: 1 [58000/68994 (84%)]\tLoss: 0.192652\n",
      "Train Epoch: 1 [59000/68994 (86%)]\tLoss: 0.186903\n",
      "Train Epoch: 1 [60000/68994 (87%)]\tLoss: 0.202274\n",
      "Train Epoch: 1 [61000/68994 (88%)]\tLoss: 0.190892\n",
      "Train Epoch: 1 [62000/68994 (90%)]\tLoss: 0.224380\n",
      "Train Epoch: 1 [63000/68994 (91%)]\tLoss: 0.211425\n",
      "Train Epoch: 1 [64000/68994 (93%)]\tLoss: 0.202853\n",
      "Train Epoch: 1 [65000/68994 (94%)]\tLoss: 0.202275\n",
      "Train Epoch: 1 [66000/68994 (96%)]\tLoss: 0.226514\n",
      "Train Epoch: 1 [67000/68994 (97%)]\tLoss: 0.230348\n",
      "Train Epoch: 1 [68000/68994 (99%)]\tLoss: 0.203747\n",
      "====> Epoch: 1 Average train loss: 0.2147\n",
      "====> Epoch: 1 Average test loss: 0.2158 Average hit rate: 65.0000\n",
      "Train Epoch: 2 [0/68994 (0%)]\tLoss: 0.218026\n",
      "Train Epoch: 2 [1000/68994 (1%)]\tLoss: 0.209841\n",
      "Train Epoch: 2 [2000/68994 (3%)]\tLoss: 0.207228\n",
      "Train Epoch: 2 [3000/68994 (4%)]\tLoss: 0.213099\n",
      "Train Epoch: 2 [4000/68994 (6%)]\tLoss: 0.217246\n",
      "Train Epoch: 2 [5000/68994 (7%)]\tLoss: 0.190762\n",
      "Train Epoch: 2 [6000/68994 (9%)]\tLoss: 0.198431\n",
      "Train Epoch: 2 [7000/68994 (10%)]\tLoss: 0.208980\n",
      "Train Epoch: 2 [8000/68994 (12%)]\tLoss: 0.202781\n",
      "Train Epoch: 2 [9000/68994 (13%)]\tLoss: 0.216758\n",
      "Train Epoch: 2 [10000/68994 (14%)]\tLoss: 0.215484\n",
      "Train Epoch: 2 [11000/68994 (16%)]\tLoss: 0.177542\n",
      "Train Epoch: 2 [12000/68994 (17%)]\tLoss: 0.210655\n",
      "Train Epoch: 2 [13000/68994 (19%)]\tLoss: 0.209011\n",
      "Train Epoch: 2 [14000/68994 (20%)]\tLoss: 0.165828\n",
      "Train Epoch: 2 [15000/68994 (22%)]\tLoss: 0.203024\n",
      "Train Epoch: 2 [16000/68994 (23%)]\tLoss: 0.191838\n",
      "Train Epoch: 2 [17000/68994 (25%)]\tLoss: 0.194758\n",
      "Train Epoch: 2 [18000/68994 (26%)]\tLoss: 0.249587\n",
      "Train Epoch: 2 [19000/68994 (28%)]\tLoss: 0.222346\n",
      "Train Epoch: 2 [20000/68994 (29%)]\tLoss: 0.187542\n",
      "Train Epoch: 2 [21000/68994 (30%)]\tLoss: 0.222137\n",
      "Train Epoch: 2 [22000/68994 (32%)]\tLoss: 0.223553\n",
      "Train Epoch: 2 [23000/68994 (33%)]\tLoss: 0.240947\n",
      "Train Epoch: 2 [24000/68994 (35%)]\tLoss: 0.218660\n",
      "Train Epoch: 2 [25000/68994 (36%)]\tLoss: 0.201136\n",
      "Train Epoch: 2 [26000/68994 (38%)]\tLoss: 0.198854\n",
      "Train Epoch: 2 [27000/68994 (39%)]\tLoss: 0.212716\n",
      "Train Epoch: 2 [28000/68994 (41%)]\tLoss: 0.182169\n",
      "Train Epoch: 2 [29000/68994 (42%)]\tLoss: 0.217317\n",
      "Train Epoch: 2 [30000/68994 (43%)]\tLoss: 0.219438\n",
      "Train Epoch: 2 [31000/68994 (45%)]\tLoss: 0.218203\n",
      "Train Epoch: 2 [32000/68994 (46%)]\tLoss: 0.206268\n",
      "Train Epoch: 2 [33000/68994 (48%)]\tLoss: 0.245020\n",
      "Train Epoch: 2 [34000/68994 (49%)]\tLoss: 0.223472\n",
      "Train Epoch: 2 [35000/68994 (51%)]\tLoss: 0.217332\n",
      "Train Epoch: 2 [36000/68994 (52%)]\tLoss: 0.217209\n",
      "Train Epoch: 2 [37000/68994 (54%)]\tLoss: 0.210910\n",
      "Train Epoch: 2 [38000/68994 (55%)]\tLoss: 0.215368\n",
      "Train Epoch: 2 [39000/68994 (57%)]\tLoss: 0.250394\n",
      "Train Epoch: 2 [40000/68994 (58%)]\tLoss: 0.218915\n",
      "Train Epoch: 2 [41000/68994 (59%)]\tLoss: 0.179200\n",
      "Train Epoch: 2 [42000/68994 (61%)]\tLoss: 0.194042\n",
      "Train Epoch: 2 [43000/68994 (62%)]\tLoss: 0.208023\n",
      "Train Epoch: 2 [44000/68994 (64%)]\tLoss: 0.219892\n",
      "Train Epoch: 2 [45000/68994 (65%)]\tLoss: 0.237555\n",
      "Train Epoch: 2 [46000/68994 (67%)]\tLoss: 0.204842\n",
      "Train Epoch: 2 [47000/68994 (68%)]\tLoss: 0.214025\n",
      "Train Epoch: 2 [48000/68994 (70%)]\tLoss: 0.208985\n",
      "Train Epoch: 2 [49000/68994 (71%)]\tLoss: 0.232366\n",
      "Train Epoch: 2 [50000/68994 (72%)]\tLoss: 0.217658\n",
      "Train Epoch: 2 [51000/68994 (74%)]\tLoss: 0.215358\n",
      "Train Epoch: 2 [52000/68994 (75%)]\tLoss: 0.207626\n",
      "Train Epoch: 2 [53000/68994 (77%)]\tLoss: 0.228234\n",
      "Train Epoch: 2 [54000/68994 (78%)]\tLoss: 0.180287\n",
      "Train Epoch: 2 [55000/68994 (80%)]\tLoss: 0.214213\n",
      "Train Epoch: 2 [56000/68994 (81%)]\tLoss: 0.227526\n",
      "Train Epoch: 2 [57000/68994 (83%)]\tLoss: 0.209370\n",
      "Train Epoch: 2 [58000/68994 (84%)]\tLoss: 0.196230\n",
      "Train Epoch: 2 [59000/68994 (86%)]\tLoss: 0.214542\n",
      "Train Epoch: 2 [60000/68994 (87%)]\tLoss: 0.235002\n",
      "Train Epoch: 2 [61000/68994 (88%)]\tLoss: 0.188820\n",
      "Train Epoch: 2 [62000/68994 (90%)]\tLoss: 0.203543\n",
      "Train Epoch: 2 [63000/68994 (91%)]\tLoss: 0.203976\n",
      "Train Epoch: 2 [64000/68994 (93%)]\tLoss: 0.188434\n",
      "Train Epoch: 2 [65000/68994 (94%)]\tLoss: 0.177943\n",
      "Train Epoch: 2 [66000/68994 (96%)]\tLoss: 0.208788\n",
      "Train Epoch: 2 [67000/68994 (97%)]\tLoss: 0.202884\n",
      "Train Epoch: 2 [68000/68994 (99%)]\tLoss: 0.233307\n",
      "====> Epoch: 2 Average train loss: 0.2128\n",
      "====> Epoch: 2 Average test loss: 0.2097 Average hit rate: 66.0000\n",
      "Train Epoch: 3 [0/68994 (0%)]\tLoss: 0.226240\n",
      "Train Epoch: 3 [1000/68994 (1%)]\tLoss: 0.217188\n",
      "Train Epoch: 3 [2000/68994 (3%)]\tLoss: 0.201791\n",
      "Train Epoch: 3 [3000/68994 (4%)]\tLoss: 0.200344\n",
      "Train Epoch: 3 [4000/68994 (6%)]\tLoss: 0.202381\n",
      "Train Epoch: 3 [5000/68994 (7%)]\tLoss: 0.197257\n",
      "Train Epoch: 3 [6000/68994 (9%)]\tLoss: 0.211187\n",
      "Train Epoch: 3 [7000/68994 (10%)]\tLoss: 0.210519\n",
      "Train Epoch: 3 [8000/68994 (12%)]\tLoss: 0.218299\n",
      "Train Epoch: 3 [9000/68994 (13%)]\tLoss: 0.184121\n",
      "Train Epoch: 3 [10000/68994 (14%)]\tLoss: 0.176868\n",
      "Train Epoch: 3 [11000/68994 (16%)]\tLoss: 0.215844\n",
      "Train Epoch: 3 [12000/68994 (17%)]\tLoss: 0.221592\n",
      "Train Epoch: 3 [13000/68994 (19%)]\tLoss: 0.215316\n",
      "Train Epoch: 3 [14000/68994 (20%)]\tLoss: 0.236131\n",
      "Train Epoch: 3 [15000/68994 (22%)]\tLoss: 0.246033\n",
      "Train Epoch: 3 [16000/68994 (23%)]\tLoss: 0.223777\n",
      "Train Epoch: 3 [17000/68994 (25%)]\tLoss: 0.189899\n",
      "Train Epoch: 3 [18000/68994 (26%)]\tLoss: 0.196488\n",
      "Train Epoch: 3 [19000/68994 (28%)]\tLoss: 0.280574\n",
      "Train Epoch: 3 [20000/68994 (29%)]\tLoss: 0.190613\n",
      "Train Epoch: 3 [21000/68994 (30%)]\tLoss: 0.234659\n",
      "Train Epoch: 3 [22000/68994 (32%)]\tLoss: 0.234830\n",
      "Train Epoch: 3 [23000/68994 (33%)]\tLoss: 0.215257\n",
      "Train Epoch: 3 [24000/68994 (35%)]\tLoss: 0.210321\n",
      "Train Epoch: 3 [25000/68994 (36%)]\tLoss: 0.182515\n",
      "Train Epoch: 3 [26000/68994 (38%)]\tLoss: 0.216219\n",
      "Train Epoch: 3 [27000/68994 (39%)]\tLoss: 0.199316\n",
      "Train Epoch: 3 [28000/68994 (41%)]\tLoss: 0.204896\n",
      "Train Epoch: 3 [29000/68994 (42%)]\tLoss: 0.201670\n",
      "Train Epoch: 3 [30000/68994 (43%)]\tLoss: 0.220721\n",
      "Train Epoch: 3 [31000/68994 (45%)]\tLoss: 0.233440\n",
      "Train Epoch: 3 [32000/68994 (46%)]\tLoss: 0.212755\n",
      "Train Epoch: 3 [33000/68994 (48%)]\tLoss: 0.226930\n",
      "Train Epoch: 3 [34000/68994 (49%)]\tLoss: 0.218979\n",
      "Train Epoch: 3 [35000/68994 (51%)]\tLoss: 0.231508\n",
      "Train Epoch: 3 [36000/68994 (52%)]\tLoss: 0.197789\n",
      "Train Epoch: 3 [37000/68994 (54%)]\tLoss: 0.204113\n",
      "Train Epoch: 3 [38000/68994 (55%)]\tLoss: 0.202009\n",
      "Train Epoch: 3 [39000/68994 (57%)]\tLoss: 0.207954\n",
      "Train Epoch: 3 [40000/68994 (58%)]\tLoss: 0.230133\n",
      "Train Epoch: 3 [41000/68994 (59%)]\tLoss: 0.210344\n",
      "Train Epoch: 3 [42000/68994 (61%)]\tLoss: 0.196009\n",
      "Train Epoch: 3 [43000/68994 (62%)]\tLoss: 0.230932\n",
      "Train Epoch: 3 [44000/68994 (64%)]\tLoss: 0.236733\n",
      "Train Epoch: 3 [45000/68994 (65%)]\tLoss: 0.206935\n",
      "Train Epoch: 3 [46000/68994 (67%)]\tLoss: 0.192795\n",
      "Train Epoch: 3 [47000/68994 (68%)]\tLoss: 0.199957\n",
      "Train Epoch: 3 [48000/68994 (70%)]\tLoss: 0.249415\n",
      "Train Epoch: 3 [49000/68994 (71%)]\tLoss: 0.222859\n",
      "Train Epoch: 3 [50000/68994 (72%)]\tLoss: 0.203923\n",
      "Train Epoch: 3 [51000/68994 (74%)]\tLoss: 0.227524\n",
      "Train Epoch: 3 [52000/68994 (75%)]\tLoss: 0.216272\n",
      "Train Epoch: 3 [53000/68994 (77%)]\tLoss: 0.224173\n",
      "Train Epoch: 3 [54000/68994 (78%)]\tLoss: 0.213314\n",
      "Train Epoch: 3 [55000/68994 (80%)]\tLoss: 0.206874\n",
      "Train Epoch: 3 [56000/68994 (81%)]\tLoss: 0.218424\n",
      "Train Epoch: 3 [57000/68994 (83%)]\tLoss: 0.217254\n",
      "Train Epoch: 3 [58000/68994 (84%)]\tLoss: 0.191770\n",
      "Train Epoch: 3 [59000/68994 (86%)]\tLoss: 0.168807\n",
      "Train Epoch: 3 [60000/68994 (87%)]\tLoss: 0.205179\n",
      "Train Epoch: 3 [61000/68994 (88%)]\tLoss: 0.238066\n",
      "Train Epoch: 3 [62000/68994 (90%)]\tLoss: 0.225494\n",
      "Train Epoch: 3 [63000/68994 (91%)]\tLoss: 0.184519\n",
      "Train Epoch: 3 [64000/68994 (93%)]\tLoss: 0.198405\n",
      "Train Epoch: 3 [65000/68994 (94%)]\tLoss: 0.204760\n",
      "Train Epoch: 3 [66000/68994 (96%)]\tLoss: 0.206414\n",
      "Train Epoch: 3 [67000/68994 (97%)]\tLoss: 0.204585\n",
      "Train Epoch: 3 [68000/68994 (99%)]\tLoss: 0.197527\n",
      "====> Epoch: 3 Average train loss: 0.2118\n",
      "====> Epoch: 3 Average test loss: 0.2077 Average hit rate: 66.0000\n",
      "Train Epoch: 4 [0/68994 (0%)]\tLoss: 0.209531\n",
      "Train Epoch: 4 [1000/68994 (1%)]\tLoss: 0.225299\n",
      "Train Epoch: 4 [2000/68994 (3%)]\tLoss: 0.188377\n",
      "Train Epoch: 4 [3000/68994 (4%)]\tLoss: 0.242548\n",
      "Train Epoch: 4 [4000/68994 (6%)]\tLoss: 0.210452\n",
      "Train Epoch: 4 [5000/68994 (7%)]\tLoss: 0.218645\n",
      "Train Epoch: 4 [6000/68994 (9%)]\tLoss: 0.172432\n",
      "Train Epoch: 4 [7000/68994 (10%)]\tLoss: 0.220378\n",
      "Train Epoch: 4 [8000/68994 (12%)]\tLoss: 0.210381\n",
      "Train Epoch: 4 [9000/68994 (13%)]\tLoss: 0.217206\n",
      "Train Epoch: 4 [10000/68994 (14%)]\tLoss: 0.198112\n",
      "Train Epoch: 4 [11000/68994 (16%)]\tLoss: 0.219427\n",
      "Train Epoch: 4 [12000/68994 (17%)]\tLoss: 0.235088\n",
      "Train Epoch: 4 [13000/68994 (19%)]\tLoss: 0.229777\n",
      "Train Epoch: 4 [14000/68994 (20%)]\tLoss: 0.214010\n",
      "Train Epoch: 4 [15000/68994 (22%)]\tLoss: 0.202955\n",
      "Train Epoch: 4 [16000/68994 (23%)]\tLoss: 0.221707\n",
      "Train Epoch: 4 [17000/68994 (25%)]\tLoss: 0.191392\n",
      "Train Epoch: 4 [18000/68994 (26%)]\tLoss: 0.192161\n",
      "Train Epoch: 4 [19000/68994 (28%)]\tLoss: 0.207571\n",
      "Train Epoch: 4 [20000/68994 (29%)]\tLoss: 0.205655\n",
      "Train Epoch: 4 [21000/68994 (30%)]\tLoss: 0.220640\n",
      "Train Epoch: 4 [22000/68994 (32%)]\tLoss: 0.219287\n",
      "Train Epoch: 4 [23000/68994 (33%)]\tLoss: 0.252916\n",
      "Train Epoch: 4 [24000/68994 (35%)]\tLoss: 0.191875\n",
      "Train Epoch: 4 [25000/68994 (36%)]\tLoss: 0.218476\n",
      "Train Epoch: 4 [26000/68994 (38%)]\tLoss: 0.202381\n",
      "Train Epoch: 4 [27000/68994 (39%)]\tLoss: 0.252302\n",
      "Train Epoch: 4 [28000/68994 (41%)]\tLoss: 0.211645\n",
      "Train Epoch: 4 [29000/68994 (42%)]\tLoss: 0.218905\n",
      "Train Epoch: 4 [30000/68994 (43%)]\tLoss: 0.218900\n",
      "Train Epoch: 4 [31000/68994 (45%)]\tLoss: 0.177180\n",
      "Train Epoch: 4 [32000/68994 (46%)]\tLoss: 0.228218\n",
      "Train Epoch: 4 [33000/68994 (48%)]\tLoss: 0.215087\n",
      "Train Epoch: 4 [34000/68994 (49%)]\tLoss: 0.199420\n",
      "Train Epoch: 4 [35000/68994 (51%)]\tLoss: 0.198551\n",
      "Train Epoch: 4 [36000/68994 (52%)]\tLoss: 0.220826\n",
      "Train Epoch: 4 [37000/68994 (54%)]\tLoss: 0.211928\n",
      "Train Epoch: 4 [38000/68994 (55%)]\tLoss: 0.221454\n",
      "Train Epoch: 4 [39000/68994 (57%)]\tLoss: 0.230191\n",
      "Train Epoch: 4 [40000/68994 (58%)]\tLoss: 0.216983\n",
      "Train Epoch: 4 [41000/68994 (59%)]\tLoss: 0.206792\n",
      "Train Epoch: 4 [42000/68994 (61%)]\tLoss: 0.213959\n",
      "Train Epoch: 4 [43000/68994 (62%)]\tLoss: 0.218133\n",
      "Train Epoch: 4 [44000/68994 (64%)]\tLoss: 0.202253\n",
      "Train Epoch: 4 [45000/68994 (65%)]\tLoss: 0.219657\n",
      "Train Epoch: 4 [46000/68994 (67%)]\tLoss: 0.204837\n",
      "Train Epoch: 4 [47000/68994 (68%)]\tLoss: 0.221655\n",
      "Train Epoch: 4 [48000/68994 (70%)]\tLoss: 0.193612\n",
      "Train Epoch: 4 [49000/68994 (71%)]\tLoss: 0.200538\n",
      "Train Epoch: 4 [50000/68994 (72%)]\tLoss: 0.180492\n",
      "Train Epoch: 4 [51000/68994 (74%)]\tLoss: 0.215959\n",
      "Train Epoch: 4 [52000/68994 (75%)]\tLoss: 0.200325\n",
      "Train Epoch: 4 [53000/68994 (77%)]\tLoss: 0.215022\n",
      "Train Epoch: 4 [54000/68994 (78%)]\tLoss: 0.224744\n",
      "Train Epoch: 4 [55000/68994 (80%)]\tLoss: 0.242108\n",
      "Train Epoch: 4 [56000/68994 (81%)]\tLoss: 0.194301\n",
      "Train Epoch: 4 [57000/68994 (83%)]\tLoss: 0.197734\n",
      "Train Epoch: 4 [58000/68994 (84%)]\tLoss: 0.227809\n",
      "Train Epoch: 4 [59000/68994 (86%)]\tLoss: 0.230558\n",
      "Train Epoch: 4 [60000/68994 (87%)]\tLoss: 0.209278\n",
      "Train Epoch: 4 [61000/68994 (88%)]\tLoss: 0.191652\n",
      "Train Epoch: 4 [62000/68994 (90%)]\tLoss: 0.199825\n",
      "Train Epoch: 4 [63000/68994 (91%)]\tLoss: 0.218191\n",
      "Train Epoch: 4 [64000/68994 (93%)]\tLoss: 0.231178\n",
      "Train Epoch: 4 [65000/68994 (94%)]\tLoss: 0.248269\n",
      "Train Epoch: 4 [66000/68994 (96%)]\tLoss: 0.198842\n",
      "Train Epoch: 4 [67000/68994 (97%)]\tLoss: 0.228495\n",
      "Train Epoch: 4 [68000/68994 (99%)]\tLoss: 0.155443\n",
      "====> Epoch: 4 Average train loss: 0.2116\n",
      "====> Epoch: 4 Average test loss: 0.2103 Average hit rate: 66.0000\n",
      "Train Epoch: 5 [0/68994 (0%)]\tLoss: 0.237911\n",
      "Train Epoch: 5 [1000/68994 (1%)]\tLoss: 0.240907\n",
      "Train Epoch: 5 [2000/68994 (3%)]\tLoss: 0.204466\n",
      "Train Epoch: 5 [3000/68994 (4%)]\tLoss: 0.212151\n",
      "Train Epoch: 5 [4000/68994 (6%)]\tLoss: 0.225542\n",
      "Train Epoch: 5 [5000/68994 (7%)]\tLoss: 0.182968\n",
      "Train Epoch: 5 [6000/68994 (9%)]\tLoss: 0.196345\n",
      "Train Epoch: 5 [7000/68994 (10%)]\tLoss: 0.208752\n",
      "Train Epoch: 5 [8000/68994 (12%)]\tLoss: 0.225316\n",
      "Train Epoch: 5 [9000/68994 (13%)]\tLoss: 0.197114\n",
      "Train Epoch: 5 [10000/68994 (14%)]\tLoss: 0.232698\n",
      "Train Epoch: 5 [11000/68994 (16%)]\tLoss: 0.212967\n",
      "Train Epoch: 5 [12000/68994 (17%)]\tLoss: 0.228095\n",
      "Train Epoch: 5 [13000/68994 (19%)]\tLoss: 0.192796\n",
      "Train Epoch: 5 [14000/68994 (20%)]\tLoss: 0.214679\n",
      "Train Epoch: 5 [15000/68994 (22%)]\tLoss: 0.194340\n",
      "Train Epoch: 5 [16000/68994 (23%)]\tLoss: 0.217149\n",
      "Train Epoch: 5 [17000/68994 (25%)]\tLoss: 0.209238\n",
      "Train Epoch: 5 [18000/68994 (26%)]\tLoss: 0.204131\n",
      "Train Epoch: 5 [19000/68994 (28%)]\tLoss: 0.229085\n",
      "Train Epoch: 5 [20000/68994 (29%)]\tLoss: 0.204747\n",
      "Train Epoch: 5 [21000/68994 (30%)]\tLoss: 0.210015\n",
      "Train Epoch: 5 [22000/68994 (32%)]\tLoss: 0.238675\n",
      "Train Epoch: 5 [23000/68994 (33%)]\tLoss: 0.224293\n",
      "Train Epoch: 5 [24000/68994 (35%)]\tLoss: 0.196459\n",
      "Train Epoch: 5 [25000/68994 (36%)]\tLoss: 0.201915\n",
      "Train Epoch: 5 [26000/68994 (38%)]\tLoss: 0.206062\n",
      "Train Epoch: 5 [27000/68994 (39%)]\tLoss: 0.220937\n",
      "Train Epoch: 5 [28000/68994 (41%)]\tLoss: 0.209778\n",
      "Train Epoch: 5 [29000/68994 (42%)]\tLoss: 0.198612\n",
      "Train Epoch: 5 [30000/68994 (43%)]\tLoss: 0.207460\n",
      "Train Epoch: 5 [31000/68994 (45%)]\tLoss: 0.209514\n",
      "Train Epoch: 5 [32000/68994 (46%)]\tLoss: 0.191928\n",
      "Train Epoch: 5 [33000/68994 (48%)]\tLoss: 0.188717\n",
      "Train Epoch: 5 [34000/68994 (49%)]\tLoss: 0.195964\n",
      "Train Epoch: 5 [35000/68994 (51%)]\tLoss: 0.203931\n",
      "Train Epoch: 5 [36000/68994 (52%)]\tLoss: 0.197314\n",
      "Train Epoch: 5 [37000/68994 (54%)]\tLoss: 0.214626\n",
      "Train Epoch: 5 [38000/68994 (55%)]\tLoss: 0.234507\n",
      "Train Epoch: 5 [39000/68994 (57%)]\tLoss: 0.205617\n",
      "Train Epoch: 5 [40000/68994 (58%)]\tLoss: 0.202170\n",
      "Train Epoch: 5 [41000/68994 (59%)]\tLoss: 0.195629\n",
      "Train Epoch: 5 [42000/68994 (61%)]\tLoss: 0.216593\n",
      "Train Epoch: 5 [43000/68994 (62%)]\tLoss: 0.192811\n",
      "Train Epoch: 5 [44000/68994 (64%)]\tLoss: 0.221327\n",
      "Train Epoch: 5 [45000/68994 (65%)]\tLoss: 0.202453\n",
      "Train Epoch: 5 [46000/68994 (67%)]\tLoss: 0.206352\n",
      "Train Epoch: 5 [47000/68994 (68%)]\tLoss: 0.216152\n",
      "Train Epoch: 5 [48000/68994 (70%)]\tLoss: 0.210455\n",
      "Train Epoch: 5 [49000/68994 (71%)]\tLoss: 0.236945\n",
      "Train Epoch: 5 [50000/68994 (72%)]\tLoss: 0.223699\n",
      "Train Epoch: 5 [51000/68994 (74%)]\tLoss: 0.202999\n",
      "Train Epoch: 5 [52000/68994 (75%)]\tLoss: 0.187530\n",
      "Train Epoch: 5 [53000/68994 (77%)]\tLoss: 0.255506\n",
      "Train Epoch: 5 [54000/68994 (78%)]\tLoss: 0.246142\n",
      "Train Epoch: 5 [55000/68994 (80%)]\tLoss: 0.234265\n",
      "Train Epoch: 5 [56000/68994 (81%)]\tLoss: 0.194107\n",
      "Train Epoch: 5 [57000/68994 (83%)]\tLoss: 0.190385\n",
      "Train Epoch: 5 [58000/68994 (84%)]\tLoss: 0.209119\n",
      "Train Epoch: 5 [59000/68994 (86%)]\tLoss: 0.226052\n",
      "Train Epoch: 5 [60000/68994 (87%)]\tLoss: 0.213295\n",
      "Train Epoch: 5 [61000/68994 (88%)]\tLoss: 0.224359\n",
      "Train Epoch: 5 [62000/68994 (90%)]\tLoss: 0.208683\n",
      "Train Epoch: 5 [63000/68994 (91%)]\tLoss: 0.199925\n",
      "Train Epoch: 5 [64000/68994 (93%)]\tLoss: 0.229466\n",
      "Train Epoch: 5 [65000/68994 (94%)]\tLoss: 0.223179\n",
      "Train Epoch: 5 [66000/68994 (96%)]\tLoss: 0.194471\n",
      "Train Epoch: 5 [67000/68994 (97%)]\tLoss: 0.200434\n",
      "Train Epoch: 5 [68000/68994 (99%)]\tLoss: 0.205019\n",
      "====> Epoch: 5 Average train loss: 0.2111\n",
      "====> Epoch: 5 Average test loss: 0.2106 Average hit rate: 65.0000\n",
      "Train Epoch: 6 [0/68994 (0%)]\tLoss: 0.204635\n",
      "Train Epoch: 6 [1000/68994 (1%)]\tLoss: 0.242453\n",
      "Train Epoch: 6 [2000/68994 (3%)]\tLoss: 0.227994\n",
      "Train Epoch: 6 [3000/68994 (4%)]\tLoss: 0.220024\n",
      "Train Epoch: 6 [4000/68994 (6%)]\tLoss: 0.221312\n",
      "Train Epoch: 6 [5000/68994 (7%)]\tLoss: 0.217524\n",
      "Train Epoch: 6 [6000/68994 (9%)]\tLoss: 0.200761\n",
      "Train Epoch: 6 [7000/68994 (10%)]\tLoss: 0.224932\n",
      "Train Epoch: 6 [8000/68994 (12%)]\tLoss: 0.206665\n",
      "Train Epoch: 6 [9000/68994 (13%)]\tLoss: 0.210497\n",
      "Train Epoch: 6 [10000/68994 (14%)]\tLoss: 0.200736\n",
      "Train Epoch: 6 [11000/68994 (16%)]\tLoss: 0.163051\n",
      "Train Epoch: 6 [12000/68994 (17%)]\tLoss: 0.193817\n",
      "Train Epoch: 6 [13000/68994 (19%)]\tLoss: 0.224848\n",
      "Train Epoch: 6 [14000/68994 (20%)]\tLoss: 0.224389\n",
      "Train Epoch: 6 [15000/68994 (22%)]\tLoss: 0.203501\n",
      "Train Epoch: 6 [16000/68994 (23%)]\tLoss: 0.202483\n",
      "Train Epoch: 6 [17000/68994 (25%)]\tLoss: 0.212615\n",
      "Train Epoch: 6 [18000/68994 (26%)]\tLoss: 0.201964\n",
      "Train Epoch: 6 [19000/68994 (28%)]\tLoss: 0.195891\n",
      "Train Epoch: 6 [20000/68994 (29%)]\tLoss: 0.186039\n",
      "Train Epoch: 6 [21000/68994 (30%)]\tLoss: 0.205497\n",
      "Train Epoch: 6 [22000/68994 (32%)]\tLoss: 0.226782\n",
      "Train Epoch: 6 [23000/68994 (33%)]\tLoss: 0.213172\n",
      "Train Epoch: 6 [24000/68994 (35%)]\tLoss: 0.197319\n",
      "Train Epoch: 6 [25000/68994 (36%)]\tLoss: 0.169758\n",
      "Train Epoch: 6 [26000/68994 (38%)]\tLoss: 0.229387\n",
      "Train Epoch: 6 [27000/68994 (39%)]\tLoss: 0.220804\n",
      "Train Epoch: 6 [28000/68994 (41%)]\tLoss: 0.219703\n",
      "Train Epoch: 6 [29000/68994 (42%)]\tLoss: 0.231617\n",
      "Train Epoch: 6 [30000/68994 (43%)]\tLoss: 0.217704\n",
      "Train Epoch: 6 [31000/68994 (45%)]\tLoss: 0.199889\n",
      "Train Epoch: 6 [32000/68994 (46%)]\tLoss: 0.218143\n",
      "Train Epoch: 6 [33000/68994 (48%)]\tLoss: 0.207515\n",
      "Train Epoch: 6 [34000/68994 (49%)]\tLoss: 0.218689\n",
      "Train Epoch: 6 [35000/68994 (51%)]\tLoss: 0.206715\n",
      "Train Epoch: 6 [36000/68994 (52%)]\tLoss: 0.226165\n",
      "Train Epoch: 6 [37000/68994 (54%)]\tLoss: 0.216261\n",
      "Train Epoch: 6 [38000/68994 (55%)]\tLoss: 0.253380\n",
      "Train Epoch: 6 [39000/68994 (57%)]\tLoss: 0.201817\n",
      "Train Epoch: 6 [40000/68994 (58%)]\tLoss: 0.185990\n",
      "Train Epoch: 6 [41000/68994 (59%)]\tLoss: 0.208959\n",
      "Train Epoch: 6 [42000/68994 (61%)]\tLoss: 0.226729\n",
      "Train Epoch: 6 [43000/68994 (62%)]\tLoss: 0.219034\n",
      "Train Epoch: 6 [44000/68994 (64%)]\tLoss: 0.202637\n",
      "Train Epoch: 6 [45000/68994 (65%)]\tLoss: 0.198859\n",
      "Train Epoch: 6 [46000/68994 (67%)]\tLoss: 0.225654\n",
      "Train Epoch: 6 [47000/68994 (68%)]\tLoss: 0.216237\n",
      "Train Epoch: 6 [48000/68994 (70%)]\tLoss: 0.240255\n",
      "Train Epoch: 6 [49000/68994 (71%)]\tLoss: 0.211264\n",
      "Train Epoch: 6 [50000/68994 (72%)]\tLoss: 0.195476\n",
      "Train Epoch: 6 [51000/68994 (74%)]\tLoss: 0.197772\n",
      "Train Epoch: 6 [52000/68994 (75%)]\tLoss: 0.209231\n",
      "Train Epoch: 6 [53000/68994 (77%)]\tLoss: 0.209946\n",
      "Train Epoch: 6 [54000/68994 (78%)]\tLoss: 0.236505\n",
      "Train Epoch: 6 [55000/68994 (80%)]\tLoss: 0.199936\n",
      "Train Epoch: 6 [56000/68994 (81%)]\tLoss: 0.227921\n",
      "Train Epoch: 6 [57000/68994 (83%)]\tLoss: 0.209590\n",
      "Train Epoch: 6 [58000/68994 (84%)]\tLoss: 0.207522\n",
      "Train Epoch: 6 [59000/68994 (86%)]\tLoss: 0.215743\n",
      "Train Epoch: 6 [60000/68994 (87%)]\tLoss: 0.208754\n",
      "Train Epoch: 6 [61000/68994 (88%)]\tLoss: 0.205522\n",
      "Train Epoch: 6 [62000/68994 (90%)]\tLoss: 0.209107\n",
      "Train Epoch: 6 [63000/68994 (91%)]\tLoss: 0.212827\n",
      "Train Epoch: 6 [64000/68994 (93%)]\tLoss: 0.216240\n",
      "Train Epoch: 6 [65000/68994 (94%)]\tLoss: 0.213122\n",
      "Train Epoch: 6 [66000/68994 (96%)]\tLoss: 0.223246\n",
      "Train Epoch: 6 [67000/68994 (97%)]\tLoss: 0.215621\n",
      "Train Epoch: 6 [68000/68994 (99%)]\tLoss: 0.216856\n",
      "====> Epoch: 6 Average train loss: 0.2109\n",
      "====> Epoch: 6 Average test loss: 0.2133 Average hit rate: 64.0000\n",
      "Train Epoch: 7 [0/68994 (0%)]\tLoss: 0.251118\n",
      "Train Epoch: 7 [1000/68994 (1%)]\tLoss: 0.234709\n",
      "Train Epoch: 7 [2000/68994 (3%)]\tLoss: 0.187658\n",
      "Train Epoch: 7 [3000/68994 (4%)]\tLoss: 0.194745\n",
      "Train Epoch: 7 [4000/68994 (6%)]\tLoss: 0.214057\n",
      "Train Epoch: 7 [5000/68994 (7%)]\tLoss: 0.224233\n",
      "Train Epoch: 7 [6000/68994 (9%)]\tLoss: 0.214608\n",
      "Train Epoch: 7 [7000/68994 (10%)]\tLoss: 0.208258\n",
      "Train Epoch: 7 [8000/68994 (12%)]\tLoss: 0.215508\n",
      "Train Epoch: 7 [9000/68994 (13%)]\tLoss: 0.197164\n",
      "Train Epoch: 7 [10000/68994 (14%)]\tLoss: 0.191665\n",
      "Train Epoch: 7 [11000/68994 (16%)]\tLoss: 0.197300\n",
      "Train Epoch: 7 [12000/68994 (17%)]\tLoss: 0.202611\n",
      "Train Epoch: 7 [13000/68994 (19%)]\tLoss: 0.220165\n",
      "Train Epoch: 7 [14000/68994 (20%)]\tLoss: 0.199714\n",
      "Train Epoch: 7 [15000/68994 (22%)]\tLoss: 0.204056\n",
      "Train Epoch: 7 [16000/68994 (23%)]\tLoss: 0.204058\n",
      "Train Epoch: 7 [17000/68994 (25%)]\tLoss: 0.215636\n",
      "Train Epoch: 7 [18000/68994 (26%)]\tLoss: 0.195912\n",
      "Train Epoch: 7 [19000/68994 (28%)]\tLoss: 0.212312\n",
      "Train Epoch: 7 [20000/68994 (29%)]\tLoss: 0.207186\n",
      "Train Epoch: 7 [21000/68994 (30%)]\tLoss: 0.209356\n",
      "Train Epoch: 7 [22000/68994 (32%)]\tLoss: 0.215714\n",
      "Train Epoch: 7 [23000/68994 (33%)]\tLoss: 0.230678\n",
      "Train Epoch: 7 [24000/68994 (35%)]\tLoss: 0.202597\n",
      "Train Epoch: 7 [25000/68994 (36%)]\tLoss: 0.194236\n",
      "Train Epoch: 7 [26000/68994 (38%)]\tLoss: 0.225876\n",
      "Train Epoch: 7 [27000/68994 (39%)]\tLoss: 0.245172\n",
      "Train Epoch: 7 [28000/68994 (41%)]\tLoss: 0.202234\n",
      "Train Epoch: 7 [29000/68994 (42%)]\tLoss: 0.215153\n",
      "Train Epoch: 7 [30000/68994 (43%)]\tLoss: 0.213776\n",
      "Train Epoch: 7 [31000/68994 (45%)]\tLoss: 0.215962\n",
      "Train Epoch: 7 [32000/68994 (46%)]\tLoss: 0.228596\n",
      "Train Epoch: 7 [33000/68994 (48%)]\tLoss: 0.194232\n",
      "Train Epoch: 7 [34000/68994 (49%)]\tLoss: 0.217939\n",
      "Train Epoch: 7 [35000/68994 (51%)]\tLoss: 0.214680\n",
      "Train Epoch: 7 [36000/68994 (52%)]\tLoss: 0.217024\n",
      "Train Epoch: 7 [37000/68994 (54%)]\tLoss: 0.183939\n",
      "Train Epoch: 7 [38000/68994 (55%)]\tLoss: 0.212294\n",
      "Train Epoch: 7 [39000/68994 (57%)]\tLoss: 0.222853\n",
      "Train Epoch: 7 [40000/68994 (58%)]\tLoss: 0.190004\n",
      "Train Epoch: 7 [41000/68994 (59%)]\tLoss: 0.243009\n",
      "Train Epoch: 7 [42000/68994 (61%)]\tLoss: 0.211728\n",
      "Train Epoch: 7 [43000/68994 (62%)]\tLoss: 0.223330\n",
      "Train Epoch: 7 [44000/68994 (64%)]\tLoss: 0.201772\n",
      "Train Epoch: 7 [45000/68994 (65%)]\tLoss: 0.228611\n",
      "Train Epoch: 7 [46000/68994 (67%)]\tLoss: 0.231612\n",
      "Train Epoch: 7 [47000/68994 (68%)]\tLoss: 0.240434\n",
      "Train Epoch: 7 [48000/68994 (70%)]\tLoss: 0.194227\n",
      "Train Epoch: 7 [49000/68994 (71%)]\tLoss: 0.214144\n",
      "Train Epoch: 7 [50000/68994 (72%)]\tLoss: 0.218782\n",
      "Train Epoch: 7 [51000/68994 (74%)]\tLoss: 0.232760\n",
      "Train Epoch: 7 [52000/68994 (75%)]\tLoss: 0.226835\n",
      "Train Epoch: 7 [53000/68994 (77%)]\tLoss: 0.208477\n",
      "Train Epoch: 7 [54000/68994 (78%)]\tLoss: 0.231782\n",
      "Train Epoch: 7 [55000/68994 (80%)]\tLoss: 0.213148\n",
      "Train Epoch: 7 [56000/68994 (81%)]\tLoss: 0.207450\n",
      "Train Epoch: 7 [57000/68994 (83%)]\tLoss: 0.217438\n",
      "Train Epoch: 7 [58000/68994 (84%)]\tLoss: 0.202104\n",
      "Train Epoch: 7 [59000/68994 (86%)]\tLoss: 0.192729\n",
      "Train Epoch: 7 [60000/68994 (87%)]\tLoss: 0.216011\n",
      "Train Epoch: 7 [61000/68994 (88%)]\tLoss: 0.206292\n",
      "Train Epoch: 7 [62000/68994 (90%)]\tLoss: 0.198738\n",
      "Train Epoch: 7 [63000/68994 (91%)]\tLoss: 0.205583\n",
      "Train Epoch: 7 [64000/68994 (93%)]\tLoss: 0.216827\n",
      "Train Epoch: 7 [65000/68994 (94%)]\tLoss: 0.234941\n",
      "Train Epoch: 7 [66000/68994 (96%)]\tLoss: 0.192593\n",
      "Train Epoch: 7 [67000/68994 (97%)]\tLoss: 0.215750\n",
      "Train Epoch: 7 [68000/68994 (99%)]\tLoss: 0.230926\n",
      "====> Epoch: 7 Average train loss: 0.2106\n",
      "====> Epoch: 7 Average test loss: 0.2105 Average hit rate: 65.0000\n",
      "Train Epoch: 8 [0/68994 (0%)]\tLoss: 0.210798\n",
      "Train Epoch: 8 [1000/68994 (1%)]\tLoss: 0.214542\n",
      "Train Epoch: 8 [2000/68994 (3%)]\tLoss: 0.213032\n",
      "Train Epoch: 8 [3000/68994 (4%)]\tLoss: 0.230750\n",
      "Train Epoch: 8 [4000/68994 (6%)]\tLoss: 0.225497\n",
      "Train Epoch: 8 [5000/68994 (7%)]\tLoss: 0.228281\n",
      "Train Epoch: 8 [6000/68994 (9%)]\tLoss: 0.207397\n",
      "Train Epoch: 8 [7000/68994 (10%)]\tLoss: 0.206319\n",
      "Train Epoch: 8 [8000/68994 (12%)]\tLoss: 0.203774\n",
      "Train Epoch: 8 [9000/68994 (13%)]\tLoss: 0.230652\n",
      "Train Epoch: 8 [10000/68994 (14%)]\tLoss: 0.185698\n",
      "Train Epoch: 8 [11000/68994 (16%)]\tLoss: 0.227605\n",
      "Train Epoch: 8 [12000/68994 (17%)]\tLoss: 0.223419\n",
      "Train Epoch: 8 [13000/68994 (19%)]\tLoss: 0.181553\n",
      "Train Epoch: 8 [14000/68994 (20%)]\tLoss: 0.215452\n",
      "Train Epoch: 8 [15000/68994 (22%)]\tLoss: 0.199441\n",
      "Train Epoch: 8 [16000/68994 (23%)]\tLoss: 0.207021\n",
      "Train Epoch: 8 [17000/68994 (25%)]\tLoss: 0.189482\n",
      "Train Epoch: 8 [18000/68994 (26%)]\tLoss: 0.190018\n",
      "Train Epoch: 8 [19000/68994 (28%)]\tLoss: 0.201091\n",
      "Train Epoch: 8 [20000/68994 (29%)]\tLoss: 0.224452\n",
      "Train Epoch: 8 [21000/68994 (30%)]\tLoss: 0.224031\n",
      "Train Epoch: 8 [22000/68994 (32%)]\tLoss: 0.219973\n",
      "Train Epoch: 8 [23000/68994 (33%)]\tLoss: 0.197757\n",
      "Train Epoch: 8 [24000/68994 (35%)]\tLoss: 0.216240\n",
      "Train Epoch: 8 [25000/68994 (36%)]\tLoss: 0.218444\n",
      "Train Epoch: 8 [26000/68994 (38%)]\tLoss: 0.216737\n",
      "Train Epoch: 8 [27000/68994 (39%)]\tLoss: 0.244219\n",
      "Train Epoch: 8 [28000/68994 (41%)]\tLoss: 0.225502\n",
      "Train Epoch: 8 [29000/68994 (42%)]\tLoss: 0.218082\n",
      "Train Epoch: 8 [30000/68994 (43%)]\tLoss: 0.206565\n",
      "Train Epoch: 8 [31000/68994 (45%)]\tLoss: 0.230606\n",
      "Train Epoch: 8 [32000/68994 (46%)]\tLoss: 0.198976\n",
      "Train Epoch: 8 [33000/68994 (48%)]\tLoss: 0.206673\n",
      "Train Epoch: 8 [34000/68994 (49%)]\tLoss: 0.196729\n",
      "Train Epoch: 8 [35000/68994 (51%)]\tLoss: 0.219593\n",
      "Train Epoch: 8 [36000/68994 (52%)]\tLoss: 0.212712\n",
      "Train Epoch: 8 [37000/68994 (54%)]\tLoss: 0.227952\n",
      "Train Epoch: 8 [38000/68994 (55%)]\tLoss: 0.198456\n",
      "Train Epoch: 8 [39000/68994 (57%)]\tLoss: 0.220297\n",
      "Train Epoch: 8 [40000/68994 (58%)]\tLoss: 0.217262\n",
      "Train Epoch: 8 [41000/68994 (59%)]\tLoss: 0.209243\n",
      "Train Epoch: 8 [42000/68994 (61%)]\tLoss: 0.215871\n",
      "Train Epoch: 8 [43000/68994 (62%)]\tLoss: 0.203162\n",
      "Train Epoch: 8 [44000/68994 (64%)]\tLoss: 0.188333\n",
      "Train Epoch: 8 [45000/68994 (65%)]\tLoss: 0.219240\n",
      "Train Epoch: 8 [46000/68994 (67%)]\tLoss: 0.241537\n",
      "Train Epoch: 8 [47000/68994 (68%)]\tLoss: 0.215855\n",
      "Train Epoch: 8 [48000/68994 (70%)]\tLoss: 0.224340\n",
      "Train Epoch: 8 [49000/68994 (71%)]\tLoss: 0.188600\n",
      "Train Epoch: 8 [50000/68994 (72%)]\tLoss: 0.222631\n",
      "Train Epoch: 8 [51000/68994 (74%)]\tLoss: 0.212034\n",
      "Train Epoch: 8 [52000/68994 (75%)]\tLoss: 0.237434\n",
      "Train Epoch: 8 [53000/68994 (77%)]\tLoss: 0.246328\n",
      "Train Epoch: 8 [54000/68994 (78%)]\tLoss: 0.229600\n",
      "Train Epoch: 8 [55000/68994 (80%)]\tLoss: 0.208037\n",
      "Train Epoch: 8 [56000/68994 (81%)]\tLoss: 0.202496\n",
      "Train Epoch: 8 [57000/68994 (83%)]\tLoss: 0.219834\n",
      "Train Epoch: 8 [58000/68994 (84%)]\tLoss: 0.192541\n",
      "Train Epoch: 8 [59000/68994 (86%)]\tLoss: 0.184290\n",
      "Train Epoch: 8 [60000/68994 (87%)]\tLoss: 0.215866\n",
      "Train Epoch: 8 [61000/68994 (88%)]\tLoss: 0.204597\n",
      "Train Epoch: 8 [62000/68994 (90%)]\tLoss: 0.181865\n",
      "Train Epoch: 8 [63000/68994 (91%)]\tLoss: 0.216846\n",
      "Train Epoch: 8 [64000/68994 (93%)]\tLoss: 0.201941\n",
      "Train Epoch: 8 [65000/68994 (94%)]\tLoss: 0.180704\n",
      "Train Epoch: 8 [66000/68994 (96%)]\tLoss: 0.201424\n",
      "Train Epoch: 8 [67000/68994 (97%)]\tLoss: 0.221468\n",
      "Train Epoch: 8 [68000/68994 (99%)]\tLoss: 0.198698\n",
      "====> Epoch: 8 Average train loss: 0.2103\n",
      "====> Epoch: 8 Average test loss: 0.2102 Average hit rate: 65.0000\n",
      "Train Epoch: 9 [0/68994 (0%)]\tLoss: 0.211029\n",
      "Train Epoch: 9 [1000/68994 (1%)]\tLoss: 0.213417\n",
      "Train Epoch: 9 [2000/68994 (3%)]\tLoss: 0.227965\n",
      "Train Epoch: 9 [3000/68994 (4%)]\tLoss: 0.239559\n",
      "Train Epoch: 9 [4000/68994 (6%)]\tLoss: 0.211187\n",
      "Train Epoch: 9 [5000/68994 (7%)]\tLoss: 0.198338\n",
      "Train Epoch: 9 [6000/68994 (9%)]\tLoss: 0.216763\n",
      "Train Epoch: 9 [7000/68994 (10%)]\tLoss: 0.207097\n",
      "Train Epoch: 9 [8000/68994 (12%)]\tLoss: 0.189497\n",
      "Train Epoch: 9 [9000/68994 (13%)]\tLoss: 0.222591\n",
      "Train Epoch: 9 [10000/68994 (14%)]\tLoss: 0.221756\n",
      "Train Epoch: 9 [11000/68994 (16%)]\tLoss: 0.182584\n",
      "Train Epoch: 9 [12000/68994 (17%)]\tLoss: 0.211102\n",
      "Train Epoch: 9 [13000/68994 (19%)]\tLoss: 0.208051\n",
      "Train Epoch: 9 [14000/68994 (20%)]\tLoss: 0.204703\n",
      "Train Epoch: 9 [15000/68994 (22%)]\tLoss: 0.229313\n",
      "Train Epoch: 9 [16000/68994 (23%)]\tLoss: 0.225577\n",
      "Train Epoch: 9 [17000/68994 (25%)]\tLoss: 0.226380\n",
      "Train Epoch: 9 [18000/68994 (26%)]\tLoss: 0.194497\n",
      "Train Epoch: 9 [19000/68994 (28%)]\tLoss: 0.181347\n",
      "Train Epoch: 9 [20000/68994 (29%)]\tLoss: 0.212911\n",
      "Train Epoch: 9 [21000/68994 (30%)]\tLoss: 0.200019\n",
      "Train Epoch: 9 [22000/68994 (32%)]\tLoss: 0.212672\n",
      "Train Epoch: 9 [23000/68994 (33%)]\tLoss: 0.193640\n",
      "Train Epoch: 9 [24000/68994 (35%)]\tLoss: 0.256328\n",
      "Train Epoch: 9 [25000/68994 (36%)]\tLoss: 0.207213\n",
      "Train Epoch: 9 [26000/68994 (38%)]\tLoss: 0.191386\n",
      "Train Epoch: 9 [27000/68994 (39%)]\tLoss: 0.223178\n",
      "Train Epoch: 9 [28000/68994 (41%)]\tLoss: 0.236656\n",
      "Train Epoch: 9 [29000/68994 (42%)]\tLoss: 0.191335\n",
      "Train Epoch: 9 [30000/68994 (43%)]\tLoss: 0.208303\n",
      "Train Epoch: 9 [31000/68994 (45%)]\tLoss: 0.229302\n",
      "Train Epoch: 9 [32000/68994 (46%)]\tLoss: 0.232298\n",
      "Train Epoch: 9 [33000/68994 (48%)]\tLoss: 0.217736\n",
      "Train Epoch: 9 [34000/68994 (49%)]\tLoss: 0.209642\n",
      "Train Epoch: 9 [35000/68994 (51%)]\tLoss: 0.224938\n",
      "Train Epoch: 9 [36000/68994 (52%)]\tLoss: 0.211907\n",
      "Train Epoch: 9 [37000/68994 (54%)]\tLoss: 0.197813\n",
      "Train Epoch: 9 [38000/68994 (55%)]\tLoss: 0.204568\n",
      "Train Epoch: 9 [39000/68994 (57%)]\tLoss: 0.220468\n",
      "Train Epoch: 9 [40000/68994 (58%)]\tLoss: 0.185361\n",
      "Train Epoch: 9 [41000/68994 (59%)]\tLoss: 0.207058\n",
      "Train Epoch: 9 [42000/68994 (61%)]\tLoss: 0.216556\n",
      "Train Epoch: 9 [43000/68994 (62%)]\tLoss: 0.155495\n",
      "Train Epoch: 9 [44000/68994 (64%)]\tLoss: 0.247578\n",
      "Train Epoch: 9 [45000/68994 (65%)]\tLoss: 0.222333\n",
      "Train Epoch: 9 [46000/68994 (67%)]\tLoss: 0.214750\n",
      "Train Epoch: 9 [47000/68994 (68%)]\tLoss: 0.200946\n",
      "Train Epoch: 9 [48000/68994 (70%)]\tLoss: 0.234442\n",
      "Train Epoch: 9 [49000/68994 (71%)]\tLoss: 0.220395\n",
      "Train Epoch: 9 [50000/68994 (72%)]\tLoss: 0.203075\n",
      "Train Epoch: 9 [51000/68994 (74%)]\tLoss: 0.236188\n",
      "Train Epoch: 9 [52000/68994 (75%)]\tLoss: 0.196451\n",
      "Train Epoch: 9 [53000/68994 (77%)]\tLoss: 0.194042\n",
      "Train Epoch: 9 [54000/68994 (78%)]\tLoss: 0.184830\n",
      "Train Epoch: 9 [55000/68994 (80%)]\tLoss: 0.189611\n",
      "Train Epoch: 9 [56000/68994 (81%)]\tLoss: 0.203673\n",
      "Train Epoch: 9 [57000/68994 (83%)]\tLoss: 0.203883\n",
      "Train Epoch: 9 [58000/68994 (84%)]\tLoss: 0.237109\n",
      "Train Epoch: 9 [59000/68994 (86%)]\tLoss: 0.213307\n",
      "Train Epoch: 9 [60000/68994 (87%)]\tLoss: 0.177509\n",
      "Train Epoch: 9 [61000/68994 (88%)]\tLoss: 0.241189\n",
      "Train Epoch: 9 [62000/68994 (90%)]\tLoss: 0.205034\n",
      "Train Epoch: 9 [63000/68994 (91%)]\tLoss: 0.230263\n",
      "Train Epoch: 9 [64000/68994 (93%)]\tLoss: 0.222020\n",
      "Train Epoch: 9 [65000/68994 (94%)]\tLoss: 0.202653\n",
      "Train Epoch: 9 [66000/68994 (96%)]\tLoss: 0.209764\n",
      "Train Epoch: 9 [67000/68994 (97%)]\tLoss: 0.198478\n",
      "Train Epoch: 9 [68000/68994 (99%)]\tLoss: 0.226912\n",
      "====> Epoch: 9 Average train loss: 0.2102\n",
      "====> Epoch: 9 Average test loss: 0.2094 Average hit rate: 66.0000\n",
      "Train Epoch: 10 [0/68994 (0%)]\tLoss: 0.212042\n",
      "Train Epoch: 10 [1000/68994 (1%)]\tLoss: 0.223066\n",
      "Train Epoch: 10 [2000/68994 (3%)]\tLoss: 0.173589\n",
      "Train Epoch: 10 [3000/68994 (4%)]\tLoss: 0.206298\n",
      "Train Epoch: 10 [4000/68994 (6%)]\tLoss: 0.194100\n",
      "Train Epoch: 10 [5000/68994 (7%)]\tLoss: 0.200034\n",
      "Train Epoch: 10 [6000/68994 (9%)]\tLoss: 0.229987\n",
      "Train Epoch: 10 [7000/68994 (10%)]\tLoss: 0.200815\n",
      "Train Epoch: 10 [8000/68994 (12%)]\tLoss: 0.203203\n",
      "Train Epoch: 10 [9000/68994 (13%)]\tLoss: 0.182644\n",
      "Train Epoch: 10 [10000/68994 (14%)]\tLoss: 0.219158\n",
      "Train Epoch: 10 [11000/68994 (16%)]\tLoss: 0.198492\n",
      "Train Epoch: 10 [12000/68994 (17%)]\tLoss: 0.197439\n",
      "Train Epoch: 10 [13000/68994 (19%)]\tLoss: 0.208300\n",
      "Train Epoch: 10 [14000/68994 (20%)]\tLoss: 0.201636\n",
      "Train Epoch: 10 [15000/68994 (22%)]\tLoss: 0.237490\n",
      "Train Epoch: 10 [16000/68994 (23%)]\tLoss: 0.232167\n",
      "Train Epoch: 10 [17000/68994 (25%)]\tLoss: 0.219551\n",
      "Train Epoch: 10 [18000/68994 (26%)]\tLoss: 0.188913\n",
      "Train Epoch: 10 [19000/68994 (28%)]\tLoss: 0.222293\n",
      "Train Epoch: 10 [20000/68994 (29%)]\tLoss: 0.208065\n",
      "Train Epoch: 10 [21000/68994 (30%)]\tLoss: 0.190686\n",
      "Train Epoch: 10 [22000/68994 (32%)]\tLoss: 0.182350\n",
      "Train Epoch: 10 [23000/68994 (33%)]\tLoss: 0.232934\n",
      "Train Epoch: 10 [24000/68994 (35%)]\tLoss: 0.203036\n",
      "Train Epoch: 10 [25000/68994 (36%)]\tLoss: 0.213497\n",
      "Train Epoch: 10 [26000/68994 (38%)]\tLoss: 0.212575\n",
      "Train Epoch: 10 [27000/68994 (39%)]\tLoss: 0.207951\n",
      "Train Epoch: 10 [28000/68994 (41%)]\tLoss: 0.219104\n",
      "Train Epoch: 10 [29000/68994 (42%)]\tLoss: 0.246189\n",
      "Train Epoch: 10 [30000/68994 (43%)]\tLoss: 0.187684\n",
      "Train Epoch: 10 [31000/68994 (45%)]\tLoss: 0.249039\n",
      "Train Epoch: 10 [32000/68994 (46%)]\tLoss: 0.204337\n",
      "Train Epoch: 10 [33000/68994 (48%)]\tLoss: 0.219025\n",
      "Train Epoch: 10 [34000/68994 (49%)]\tLoss: 0.214291\n",
      "Train Epoch: 10 [35000/68994 (51%)]\tLoss: 0.211929\n",
      "Train Epoch: 10 [36000/68994 (52%)]\tLoss: 0.214839\n",
      "Train Epoch: 10 [37000/68994 (54%)]\tLoss: 0.203843\n",
      "Train Epoch: 10 [38000/68994 (55%)]\tLoss: 0.189561\n",
      "Train Epoch: 10 [39000/68994 (57%)]\tLoss: 0.220767\n",
      "Train Epoch: 10 [40000/68994 (58%)]\tLoss: 0.238774\n",
      "Train Epoch: 10 [41000/68994 (59%)]\tLoss: 0.194210\n",
      "Train Epoch: 10 [42000/68994 (61%)]\tLoss: 0.208454\n",
      "Train Epoch: 10 [43000/68994 (62%)]\tLoss: 0.232348\n",
      "Train Epoch: 10 [44000/68994 (64%)]\tLoss: 0.193378\n",
      "Train Epoch: 10 [45000/68994 (65%)]\tLoss: 0.210985\n",
      "Train Epoch: 10 [46000/68994 (67%)]\tLoss: 0.212321\n",
      "Train Epoch: 10 [47000/68994 (68%)]\tLoss: 0.219874\n",
      "Train Epoch: 10 [48000/68994 (70%)]\tLoss: 0.184497\n",
      "Train Epoch: 10 [49000/68994 (71%)]\tLoss: 0.193620\n",
      "Train Epoch: 10 [50000/68994 (72%)]\tLoss: 0.247377\n",
      "Train Epoch: 10 [51000/68994 (74%)]\tLoss: 0.222746\n",
      "Train Epoch: 10 [52000/68994 (75%)]\tLoss: 0.219495\n",
      "Train Epoch: 10 [53000/68994 (77%)]\tLoss: 0.234192\n",
      "Train Epoch: 10 [54000/68994 (78%)]\tLoss: 0.234112\n",
      "Train Epoch: 10 [55000/68994 (80%)]\tLoss: 0.208348\n",
      "Train Epoch: 10 [56000/68994 (81%)]\tLoss: 0.210561\n",
      "Train Epoch: 10 [57000/68994 (83%)]\tLoss: 0.222875\n",
      "Train Epoch: 10 [58000/68994 (84%)]\tLoss: 0.205720\n",
      "Train Epoch: 10 [59000/68994 (86%)]\tLoss: 0.194244\n",
      "Train Epoch: 10 [60000/68994 (87%)]\tLoss: 0.223585\n",
      "Train Epoch: 10 [61000/68994 (88%)]\tLoss: 0.203566\n",
      "Train Epoch: 10 [62000/68994 (90%)]\tLoss: 0.199112\n",
      "Train Epoch: 10 [63000/68994 (91%)]\tLoss: 0.240908\n",
      "Train Epoch: 10 [64000/68994 (93%)]\tLoss: 0.236289\n",
      "Train Epoch: 10 [65000/68994 (94%)]\tLoss: 0.190532\n",
      "Train Epoch: 10 [66000/68994 (96%)]\tLoss: 0.193628\n",
      "Train Epoch: 10 [67000/68994 (97%)]\tLoss: 0.213936\n",
      "Train Epoch: 10 [68000/68994 (99%)]\tLoss: 0.210861\n",
      "====> Epoch: 10 Average train loss: 0.2103\n",
      "====> Epoch: 10 Average test loss: 0.2133 Average hit rate: 65.0000\n",
      "Train Epoch: 11 [0/68994 (0%)]\tLoss: 0.243379\n",
      "Train Epoch: 11 [1000/68994 (1%)]\tLoss: 0.230029\n",
      "Train Epoch: 11 [2000/68994 (3%)]\tLoss: 0.218847\n",
      "Train Epoch: 11 [3000/68994 (4%)]\tLoss: 0.232407\n",
      "Train Epoch: 11 [4000/68994 (6%)]\tLoss: 0.224605\n",
      "Train Epoch: 11 [5000/68994 (7%)]\tLoss: 0.209330\n",
      "Train Epoch: 11 [6000/68994 (9%)]\tLoss: 0.212432\n",
      "Train Epoch: 11 [7000/68994 (10%)]\tLoss: 0.188161\n",
      "Train Epoch: 11 [8000/68994 (12%)]\tLoss: 0.176347\n",
      "Train Epoch: 11 [9000/68994 (13%)]\tLoss: 0.201385\n",
      "Train Epoch: 11 [10000/68994 (14%)]\tLoss: 0.196160\n",
      "Train Epoch: 11 [11000/68994 (16%)]\tLoss: 0.197729\n",
      "Train Epoch: 11 [12000/68994 (17%)]\tLoss: 0.211081\n",
      "Train Epoch: 11 [13000/68994 (19%)]\tLoss: 0.196764\n",
      "Train Epoch: 11 [14000/68994 (20%)]\tLoss: 0.200609\n",
      "Train Epoch: 11 [15000/68994 (22%)]\tLoss: 0.227495\n",
      "Train Epoch: 11 [16000/68994 (23%)]\tLoss: 0.229684\n",
      "Train Epoch: 11 [17000/68994 (25%)]\tLoss: 0.187034\n",
      "Train Epoch: 11 [18000/68994 (26%)]\tLoss: 0.191908\n",
      "Train Epoch: 11 [19000/68994 (28%)]\tLoss: 0.220306\n",
      "Train Epoch: 11 [20000/68994 (29%)]\tLoss: 0.203175\n",
      "Train Epoch: 11 [21000/68994 (30%)]\tLoss: 0.226338\n",
      "Train Epoch: 11 [22000/68994 (32%)]\tLoss: 0.195029\n",
      "Train Epoch: 11 [23000/68994 (33%)]\tLoss: 0.214568\n",
      "Train Epoch: 11 [24000/68994 (35%)]\tLoss: 0.211695\n",
      "Train Epoch: 11 [25000/68994 (36%)]\tLoss: 0.206509\n",
      "Train Epoch: 11 [26000/68994 (38%)]\tLoss: 0.202333\n",
      "Train Epoch: 11 [27000/68994 (39%)]\tLoss: 0.212354\n",
      "Train Epoch: 11 [28000/68994 (41%)]\tLoss: 0.228361\n",
      "Train Epoch: 11 [29000/68994 (42%)]\tLoss: 0.218222\n",
      "Train Epoch: 11 [30000/68994 (43%)]\tLoss: 0.201444\n",
      "Train Epoch: 11 [31000/68994 (45%)]\tLoss: 0.191468\n",
      "Train Epoch: 11 [32000/68994 (46%)]\tLoss: 0.213632\n",
      "Train Epoch: 11 [33000/68994 (48%)]\tLoss: 0.224104\n",
      "Train Epoch: 11 [34000/68994 (49%)]\tLoss: 0.189591\n",
      "Train Epoch: 11 [35000/68994 (51%)]\tLoss: 0.244908\n",
      "Train Epoch: 11 [36000/68994 (52%)]\tLoss: 0.207264\n",
      "Train Epoch: 11 [37000/68994 (54%)]\tLoss: 0.187876\n",
      "Train Epoch: 11 [38000/68994 (55%)]\tLoss: 0.227140\n",
      "Train Epoch: 11 [39000/68994 (57%)]\tLoss: 0.227450\n",
      "Train Epoch: 11 [40000/68994 (58%)]\tLoss: 0.226426\n",
      "Train Epoch: 11 [41000/68994 (59%)]\tLoss: 0.202797\n",
      "Train Epoch: 11 [42000/68994 (61%)]\tLoss: 0.235405\n",
      "Train Epoch: 11 [43000/68994 (62%)]\tLoss: 0.223445\n",
      "Train Epoch: 11 [44000/68994 (64%)]\tLoss: 0.211155\n",
      "Train Epoch: 11 [45000/68994 (65%)]\tLoss: 0.213283\n",
      "Train Epoch: 11 [46000/68994 (67%)]\tLoss: 0.198847\n",
      "Train Epoch: 11 [47000/68994 (68%)]\tLoss: 0.230757\n",
      "Train Epoch: 11 [48000/68994 (70%)]\tLoss: 0.184716\n",
      "Train Epoch: 11 [49000/68994 (71%)]\tLoss: 0.226761\n",
      "Train Epoch: 11 [50000/68994 (72%)]\tLoss: 0.192357\n",
      "Train Epoch: 11 [51000/68994 (74%)]\tLoss: 0.208662\n",
      "Train Epoch: 11 [52000/68994 (75%)]\tLoss: 0.184824\n",
      "Train Epoch: 11 [53000/68994 (77%)]\tLoss: 0.180533\n",
      "Train Epoch: 11 [54000/68994 (78%)]\tLoss: 0.209440\n",
      "Train Epoch: 11 [55000/68994 (80%)]\tLoss: 0.169670\n",
      "Train Epoch: 11 [56000/68994 (81%)]\tLoss: 0.205028\n",
      "Train Epoch: 11 [57000/68994 (83%)]\tLoss: 0.206792\n",
      "Train Epoch: 11 [58000/68994 (84%)]\tLoss: 0.202262\n",
      "Train Epoch: 11 [59000/68994 (86%)]\tLoss: 0.168902\n",
      "Train Epoch: 11 [60000/68994 (87%)]\tLoss: 0.191489\n",
      "Train Epoch: 11 [61000/68994 (88%)]\tLoss: 0.240518\n",
      "Train Epoch: 11 [62000/68994 (90%)]\tLoss: 0.200563\n",
      "Train Epoch: 11 [63000/68994 (91%)]\tLoss: 0.226861\n",
      "Train Epoch: 11 [64000/68994 (93%)]\tLoss: 0.213213\n",
      "Train Epoch: 11 [65000/68994 (94%)]\tLoss: 0.216023\n",
      "Train Epoch: 11 [66000/68994 (96%)]\tLoss: 0.211730\n",
      "Train Epoch: 11 [67000/68994 (97%)]\tLoss: 0.218591\n",
      "Train Epoch: 11 [68000/68994 (99%)]\tLoss: 0.198710\n",
      "====> Epoch: 11 Average train loss: 0.2101\n",
      "====> Epoch: 11 Average test loss: 0.2115 Average hit rate: 65.0000\n",
      "Train Epoch: 12 [0/68994 (0%)]\tLoss: 0.215782\n",
      "Train Epoch: 12 [1000/68994 (1%)]\tLoss: 0.199476\n",
      "Train Epoch: 12 [2000/68994 (3%)]\tLoss: 0.206637\n",
      "Train Epoch: 12 [3000/68994 (4%)]\tLoss: 0.207954\n",
      "Train Epoch: 12 [4000/68994 (6%)]\tLoss: 0.197626\n",
      "Train Epoch: 12 [5000/68994 (7%)]\tLoss: 0.200406\n",
      "Train Epoch: 12 [6000/68994 (9%)]\tLoss: 0.213339\n",
      "Train Epoch: 12 [7000/68994 (10%)]\tLoss: 0.221399\n",
      "Train Epoch: 12 [8000/68994 (12%)]\tLoss: 0.206515\n",
      "Train Epoch: 12 [9000/68994 (13%)]\tLoss: 0.220937\n",
      "Train Epoch: 12 [10000/68994 (14%)]\tLoss: 0.220624\n",
      "Train Epoch: 12 [11000/68994 (16%)]\tLoss: 0.194826\n",
      "Train Epoch: 12 [12000/68994 (17%)]\tLoss: 0.231587\n",
      "Train Epoch: 12 [13000/68994 (19%)]\tLoss: 0.193795\n",
      "Train Epoch: 12 [14000/68994 (20%)]\tLoss: 0.204126\n",
      "Train Epoch: 12 [15000/68994 (22%)]\tLoss: 0.228724\n",
      "Train Epoch: 12 [16000/68994 (23%)]\tLoss: 0.220052\n",
      "Train Epoch: 12 [17000/68994 (25%)]\tLoss: 0.223356\n",
      "Train Epoch: 12 [18000/68994 (26%)]\tLoss: 0.217879\n",
      "Train Epoch: 12 [19000/68994 (28%)]\tLoss: 0.244653\n",
      "Train Epoch: 12 [20000/68994 (29%)]\tLoss: 0.232048\n",
      "Train Epoch: 12 [21000/68994 (30%)]\tLoss: 0.200886\n",
      "Train Epoch: 12 [22000/68994 (32%)]\tLoss: 0.225232\n",
      "Train Epoch: 12 [23000/68994 (33%)]\tLoss: 0.209495\n",
      "Train Epoch: 12 [24000/68994 (35%)]\tLoss: 0.232164\n",
      "Train Epoch: 12 [25000/68994 (36%)]\tLoss: 0.221508\n",
      "Train Epoch: 12 [26000/68994 (38%)]\tLoss: 0.223594\n",
      "Train Epoch: 12 [27000/68994 (39%)]\tLoss: 0.200991\n",
      "Train Epoch: 12 [28000/68994 (41%)]\tLoss: 0.229889\n",
      "Train Epoch: 12 [29000/68994 (42%)]\tLoss: 0.215075\n",
      "Train Epoch: 12 [30000/68994 (43%)]\tLoss: 0.187457\n",
      "Train Epoch: 12 [31000/68994 (45%)]\tLoss: 0.215136\n",
      "Train Epoch: 12 [32000/68994 (46%)]\tLoss: 0.197935\n",
      "Train Epoch: 12 [33000/68994 (48%)]\tLoss: 0.191987\n",
      "Train Epoch: 12 [34000/68994 (49%)]\tLoss: 0.230083\n",
      "Train Epoch: 12 [35000/68994 (51%)]\tLoss: 0.200942\n",
      "Train Epoch: 12 [36000/68994 (52%)]\tLoss: 0.215307\n",
      "Train Epoch: 12 [37000/68994 (54%)]\tLoss: 0.211223\n",
      "Train Epoch: 12 [38000/68994 (55%)]\tLoss: 0.214866\n",
      "Train Epoch: 12 [39000/68994 (57%)]\tLoss: 0.187968\n",
      "Train Epoch: 12 [40000/68994 (58%)]\tLoss: 0.213429\n",
      "Train Epoch: 12 [41000/68994 (59%)]\tLoss: 0.212759\n",
      "Train Epoch: 12 [42000/68994 (61%)]\tLoss: 0.207072\n",
      "Train Epoch: 12 [43000/68994 (62%)]\tLoss: 0.244367\n",
      "Train Epoch: 12 [44000/68994 (64%)]\tLoss: 0.211673\n",
      "Train Epoch: 12 [45000/68994 (65%)]\tLoss: 0.219290\n",
      "Train Epoch: 12 [46000/68994 (67%)]\tLoss: 0.216838\n",
      "Train Epoch: 12 [47000/68994 (68%)]\tLoss: 0.210574\n",
      "Train Epoch: 12 [48000/68994 (70%)]\tLoss: 0.246489\n",
      "Train Epoch: 12 [49000/68994 (71%)]\tLoss: 0.215935\n",
      "Train Epoch: 12 [50000/68994 (72%)]\tLoss: 0.224896\n",
      "Train Epoch: 12 [51000/68994 (74%)]\tLoss: 0.204667\n",
      "Train Epoch: 12 [52000/68994 (75%)]\tLoss: 0.220481\n",
      "Train Epoch: 12 [53000/68994 (77%)]\tLoss: 0.215891\n",
      "Train Epoch: 12 [54000/68994 (78%)]\tLoss: 0.248064\n",
      "Train Epoch: 12 [55000/68994 (80%)]\tLoss: 0.183498\n",
      "Train Epoch: 12 [56000/68994 (81%)]\tLoss: 0.204111\n",
      "Train Epoch: 12 [57000/68994 (83%)]\tLoss: 0.197853\n",
      "Train Epoch: 12 [58000/68994 (84%)]\tLoss: 0.190419\n",
      "Train Epoch: 12 [59000/68994 (86%)]\tLoss: 0.201135\n",
      "Train Epoch: 12 [60000/68994 (87%)]\tLoss: 0.234242\n",
      "Train Epoch: 12 [61000/68994 (88%)]\tLoss: 0.177074\n",
      "Train Epoch: 12 [62000/68994 (90%)]\tLoss: 0.205970\n",
      "Train Epoch: 12 [63000/68994 (91%)]\tLoss: 0.202925\n",
      "Train Epoch: 12 [64000/68994 (93%)]\tLoss: 0.220222\n",
      "Train Epoch: 12 [65000/68994 (94%)]\tLoss: 0.202931\n",
      "Train Epoch: 12 [66000/68994 (96%)]\tLoss: 0.203917\n",
      "Train Epoch: 12 [67000/68994 (97%)]\tLoss: 0.196588\n",
      "Train Epoch: 12 [68000/68994 (99%)]\tLoss: 0.190935\n",
      "====> Epoch: 12 Average train loss: 0.2099\n",
      "====> Epoch: 12 Average test loss: 0.2095 Average hit rate: 66.0000\n",
      "Train Epoch: 13 [0/68994 (0%)]\tLoss: 0.193214\n",
      "Train Epoch: 13 [1000/68994 (1%)]\tLoss: 0.216259\n",
      "Train Epoch: 13 [2000/68994 (3%)]\tLoss: 0.206437\n",
      "Train Epoch: 13 [3000/68994 (4%)]\tLoss: 0.183572\n",
      "Train Epoch: 13 [4000/68994 (6%)]\tLoss: 0.180876\n",
      "Train Epoch: 13 [5000/68994 (7%)]\tLoss: 0.214798\n",
      "Train Epoch: 13 [6000/68994 (9%)]\tLoss: 0.204623\n",
      "Train Epoch: 13 [7000/68994 (10%)]\tLoss: 0.198817\n",
      "Train Epoch: 13 [8000/68994 (12%)]\tLoss: 0.196933\n",
      "Train Epoch: 13 [9000/68994 (13%)]\tLoss: 0.195247\n",
      "Train Epoch: 13 [10000/68994 (14%)]\tLoss: 0.193171\n",
      "Train Epoch: 13 [11000/68994 (16%)]\tLoss: 0.208132\n",
      "Train Epoch: 13 [12000/68994 (17%)]\tLoss: 0.217077\n",
      "Train Epoch: 13 [13000/68994 (19%)]\tLoss: 0.246903\n",
      "Train Epoch: 13 [14000/68994 (20%)]\tLoss: 0.200022\n",
      "Train Epoch: 13 [15000/68994 (22%)]\tLoss: 0.207510\n",
      "Train Epoch: 13 [16000/68994 (23%)]\tLoss: 0.200605\n",
      "Train Epoch: 13 [17000/68994 (25%)]\tLoss: 0.199029\n",
      "Train Epoch: 13 [18000/68994 (26%)]\tLoss: 0.219463\n",
      "Train Epoch: 13 [19000/68994 (28%)]\tLoss: 0.208219\n",
      "Train Epoch: 13 [20000/68994 (29%)]\tLoss: 0.228870\n",
      "Train Epoch: 13 [21000/68994 (30%)]\tLoss: 0.209543\n",
      "Train Epoch: 13 [22000/68994 (32%)]\tLoss: 0.193590\n",
      "Train Epoch: 13 [23000/68994 (33%)]\tLoss: 0.195849\n",
      "Train Epoch: 13 [24000/68994 (35%)]\tLoss: 0.177688\n",
      "Train Epoch: 13 [25000/68994 (36%)]\tLoss: 0.215414\n",
      "Train Epoch: 13 [26000/68994 (38%)]\tLoss: 0.214019\n",
      "Train Epoch: 13 [27000/68994 (39%)]\tLoss: 0.191119\n",
      "Train Epoch: 13 [28000/68994 (41%)]\tLoss: 0.169655\n",
      "Train Epoch: 13 [29000/68994 (42%)]\tLoss: 0.172601\n",
      "Train Epoch: 13 [30000/68994 (43%)]\tLoss: 0.212374\n",
      "Train Epoch: 13 [31000/68994 (45%)]\tLoss: 0.219120\n",
      "Train Epoch: 13 [32000/68994 (46%)]\tLoss: 0.246988\n",
      "Train Epoch: 13 [33000/68994 (48%)]\tLoss: 0.223314\n",
      "Train Epoch: 13 [34000/68994 (49%)]\tLoss: 0.204894\n",
      "Train Epoch: 13 [35000/68994 (51%)]\tLoss: 0.210161\n",
      "Train Epoch: 13 [36000/68994 (52%)]\tLoss: 0.188746\n",
      "Train Epoch: 13 [37000/68994 (54%)]\tLoss: 0.219634\n",
      "Train Epoch: 13 [38000/68994 (55%)]\tLoss: 0.213254\n",
      "Train Epoch: 13 [39000/68994 (57%)]\tLoss: 0.214954\n",
      "Train Epoch: 13 [40000/68994 (58%)]\tLoss: 0.207465\n",
      "Train Epoch: 13 [41000/68994 (59%)]\tLoss: 0.212379\n",
      "Train Epoch: 13 [42000/68994 (61%)]\tLoss: 0.193766\n",
      "Train Epoch: 13 [43000/68994 (62%)]\tLoss: 0.203004\n",
      "Train Epoch: 13 [44000/68994 (64%)]\tLoss: 0.217209\n",
      "Train Epoch: 13 [45000/68994 (65%)]\tLoss: 0.225107\n",
      "Train Epoch: 13 [46000/68994 (67%)]\tLoss: 0.212333\n",
      "Train Epoch: 13 [47000/68994 (68%)]\tLoss: 0.233840\n",
      "Train Epoch: 13 [48000/68994 (70%)]\tLoss: 0.200779\n",
      "Train Epoch: 13 [49000/68994 (71%)]\tLoss: 0.228265\n",
      "Train Epoch: 13 [50000/68994 (72%)]\tLoss: 0.236460\n",
      "Train Epoch: 13 [51000/68994 (74%)]\tLoss: 0.197729\n",
      "Train Epoch: 13 [52000/68994 (75%)]\tLoss: 0.228915\n",
      "Train Epoch: 13 [53000/68994 (77%)]\tLoss: 0.196773\n",
      "Train Epoch: 13 [54000/68994 (78%)]\tLoss: 0.228001\n",
      "Train Epoch: 13 [55000/68994 (80%)]\tLoss: 0.222826\n",
      "Train Epoch: 13 [56000/68994 (81%)]\tLoss: 0.193246\n",
      "Train Epoch: 13 [57000/68994 (83%)]\tLoss: 0.221140\n",
      "Train Epoch: 13 [58000/68994 (84%)]\tLoss: 0.182476\n",
      "Train Epoch: 13 [59000/68994 (86%)]\tLoss: 0.197889\n",
      "Train Epoch: 13 [60000/68994 (87%)]\tLoss: 0.195210\n",
      "Train Epoch: 13 [61000/68994 (88%)]\tLoss: 0.224196\n",
      "Train Epoch: 13 [62000/68994 (90%)]\tLoss: 0.217220\n",
      "Train Epoch: 13 [63000/68994 (91%)]\tLoss: 0.203497\n",
      "Train Epoch: 13 [64000/68994 (93%)]\tLoss: 0.204372\n",
      "Train Epoch: 13 [65000/68994 (94%)]\tLoss: 0.188572\n",
      "Train Epoch: 13 [66000/68994 (96%)]\tLoss: 0.213703\n",
      "Train Epoch: 13 [67000/68994 (97%)]\tLoss: 0.195899\n",
      "Train Epoch: 13 [68000/68994 (99%)]\tLoss: 0.212179\n",
      "====> Epoch: 13 Average train loss: 0.2098\n",
      "====> Epoch: 13 Average test loss: 0.2097 Average hit rate: 66.0000\n",
      "Train Epoch: 14 [0/68994 (0%)]\tLoss: 0.211069\n",
      "Train Epoch: 14 [1000/68994 (1%)]\tLoss: 0.220257\n",
      "Train Epoch: 14 [2000/68994 (3%)]\tLoss: 0.216183\n",
      "Train Epoch: 14 [3000/68994 (4%)]\tLoss: 0.229795\n",
      "Train Epoch: 14 [4000/68994 (6%)]\tLoss: 0.202381\n",
      "Train Epoch: 14 [5000/68994 (7%)]\tLoss: 0.221301\n",
      "Train Epoch: 14 [6000/68994 (9%)]\tLoss: 0.202871\n",
      "Train Epoch: 14 [7000/68994 (10%)]\tLoss: 0.204942\n",
      "Train Epoch: 14 [8000/68994 (12%)]\tLoss: 0.220477\n",
      "Train Epoch: 14 [9000/68994 (13%)]\tLoss: 0.224313\n",
      "Train Epoch: 14 [10000/68994 (14%)]\tLoss: 0.210983\n",
      "Train Epoch: 14 [11000/68994 (16%)]\tLoss: 0.208725\n",
      "Train Epoch: 14 [12000/68994 (17%)]\tLoss: 0.198891\n",
      "Train Epoch: 14 [13000/68994 (19%)]\tLoss: 0.217855\n",
      "Train Epoch: 14 [14000/68994 (20%)]\tLoss: 0.198930\n",
      "Train Epoch: 14 [15000/68994 (22%)]\tLoss: 0.226058\n",
      "Train Epoch: 14 [16000/68994 (23%)]\tLoss: 0.218723\n",
      "Train Epoch: 14 [17000/68994 (25%)]\tLoss: 0.230170\n",
      "Train Epoch: 14 [18000/68994 (26%)]\tLoss: 0.185989\n",
      "Train Epoch: 14 [19000/68994 (28%)]\tLoss: 0.217661\n",
      "Train Epoch: 14 [20000/68994 (29%)]\tLoss: 0.210112\n",
      "Train Epoch: 14 [21000/68994 (30%)]\tLoss: 0.189044\n",
      "Train Epoch: 14 [22000/68994 (32%)]\tLoss: 0.240655\n",
      "Train Epoch: 14 [23000/68994 (33%)]\tLoss: 0.228419\n",
      "Train Epoch: 14 [24000/68994 (35%)]\tLoss: 0.188694\n",
      "Train Epoch: 14 [25000/68994 (36%)]\tLoss: 0.201593\n",
      "Train Epoch: 14 [26000/68994 (38%)]\tLoss: 0.242761\n",
      "Train Epoch: 14 [27000/68994 (39%)]\tLoss: 0.200166\n",
      "Train Epoch: 14 [28000/68994 (41%)]\tLoss: 0.185884\n",
      "Train Epoch: 14 [29000/68994 (42%)]\tLoss: 0.203009\n",
      "Train Epoch: 14 [30000/68994 (43%)]\tLoss: 0.225948\n",
      "Train Epoch: 14 [31000/68994 (45%)]\tLoss: 0.226525\n",
      "Train Epoch: 14 [32000/68994 (46%)]\tLoss: 0.227277\n",
      "Train Epoch: 14 [33000/68994 (48%)]\tLoss: 0.204684\n",
      "Train Epoch: 14 [34000/68994 (49%)]\tLoss: 0.200689\n",
      "Train Epoch: 14 [35000/68994 (51%)]\tLoss: 0.242478\n",
      "Train Epoch: 14 [36000/68994 (52%)]\tLoss: 0.188188\n",
      "Train Epoch: 14 [37000/68994 (54%)]\tLoss: 0.179011\n",
      "Train Epoch: 14 [38000/68994 (55%)]\tLoss: 0.199054\n",
      "Train Epoch: 14 [39000/68994 (57%)]\tLoss: 0.187950\n",
      "Train Epoch: 14 [40000/68994 (58%)]\tLoss: 0.220048\n",
      "Train Epoch: 14 [41000/68994 (59%)]\tLoss: 0.217067\n",
      "Train Epoch: 14 [42000/68994 (61%)]\tLoss: 0.220901\n",
      "Train Epoch: 14 [43000/68994 (62%)]\tLoss: 0.209916\n",
      "Train Epoch: 14 [44000/68994 (64%)]\tLoss: 0.220544\n",
      "Train Epoch: 14 [45000/68994 (65%)]\tLoss: 0.221674\n",
      "Train Epoch: 14 [46000/68994 (67%)]\tLoss: 0.191127\n",
      "Train Epoch: 14 [47000/68994 (68%)]\tLoss: 0.208967\n",
      "Train Epoch: 14 [48000/68994 (70%)]\tLoss: 0.264961\n",
      "Train Epoch: 14 [49000/68994 (71%)]\tLoss: 0.235941\n",
      "Train Epoch: 14 [50000/68994 (72%)]\tLoss: 0.218199\n",
      "Train Epoch: 14 [51000/68994 (74%)]\tLoss: 0.209609\n",
      "Train Epoch: 14 [52000/68994 (75%)]\tLoss: 0.214393\n",
      "Train Epoch: 14 [53000/68994 (77%)]\tLoss: 0.171861\n",
      "Train Epoch: 14 [54000/68994 (78%)]\tLoss: 0.199560\n",
      "Train Epoch: 14 [55000/68994 (80%)]\tLoss: 0.237681\n",
      "Train Epoch: 14 [56000/68994 (81%)]\tLoss: 0.216230\n",
      "Train Epoch: 14 [57000/68994 (83%)]\tLoss: 0.219932\n",
      "Train Epoch: 14 [58000/68994 (84%)]\tLoss: 0.228692\n",
      "Train Epoch: 14 [59000/68994 (86%)]\tLoss: 0.232249\n",
      "Train Epoch: 14 [60000/68994 (87%)]\tLoss: 0.184954\n",
      "Train Epoch: 14 [61000/68994 (88%)]\tLoss: 0.223354\n",
      "Train Epoch: 14 [62000/68994 (90%)]\tLoss: 0.220191\n",
      "Train Epoch: 14 [63000/68994 (91%)]\tLoss: 0.198164\n",
      "Train Epoch: 14 [64000/68994 (93%)]\tLoss: 0.210692\n",
      "Train Epoch: 14 [65000/68994 (94%)]\tLoss: 0.251634\n",
      "Train Epoch: 14 [66000/68994 (96%)]\tLoss: 0.204661\n",
      "Train Epoch: 14 [67000/68994 (97%)]\tLoss: 0.241773\n",
      "Train Epoch: 14 [68000/68994 (99%)]\tLoss: 0.230175\n",
      "====> Epoch: 14 Average train loss: 0.2096\n",
      "====> Epoch: 14 Average test loss: 0.2095 Average hit rate: 66.0000\n",
      "Train Epoch: 15 [0/68994 (0%)]\tLoss: 0.214364\n",
      "Train Epoch: 15 [1000/68994 (1%)]\tLoss: 0.236580\n",
      "Train Epoch: 15 [2000/68994 (3%)]\tLoss: 0.180692\n",
      "Train Epoch: 15 [3000/68994 (4%)]\tLoss: 0.201969\n",
      "Train Epoch: 15 [4000/68994 (6%)]\tLoss: 0.211597\n",
      "Train Epoch: 15 [5000/68994 (7%)]\tLoss: 0.218099\n",
      "Train Epoch: 15 [6000/68994 (9%)]\tLoss: 0.225119\n",
      "Train Epoch: 15 [7000/68994 (10%)]\tLoss: 0.214410\n",
      "Train Epoch: 15 [8000/68994 (12%)]\tLoss: 0.223131\n",
      "Train Epoch: 15 [9000/68994 (13%)]\tLoss: 0.229684\n",
      "Train Epoch: 15 [10000/68994 (14%)]\tLoss: 0.237946\n",
      "Train Epoch: 15 [11000/68994 (16%)]\tLoss: 0.219298\n",
      "Train Epoch: 15 [12000/68994 (17%)]\tLoss: 0.189987\n",
      "Train Epoch: 15 [13000/68994 (19%)]\tLoss: 0.225033\n",
      "Train Epoch: 15 [14000/68994 (20%)]\tLoss: 0.218846\n",
      "Train Epoch: 15 [15000/68994 (22%)]\tLoss: 0.223896\n",
      "Train Epoch: 15 [16000/68994 (23%)]\tLoss: 0.206812\n",
      "Train Epoch: 15 [17000/68994 (25%)]\tLoss: 0.217919\n",
      "Train Epoch: 15 [18000/68994 (26%)]\tLoss: 0.196654\n",
      "Train Epoch: 15 [19000/68994 (28%)]\tLoss: 0.233845\n",
      "Train Epoch: 15 [20000/68994 (29%)]\tLoss: 0.207934\n",
      "Train Epoch: 15 [21000/68994 (30%)]\tLoss: 0.207121\n",
      "Train Epoch: 15 [22000/68994 (32%)]\tLoss: 0.209165\n",
      "Train Epoch: 15 [23000/68994 (33%)]\tLoss: 0.224994\n",
      "Train Epoch: 15 [24000/68994 (35%)]\tLoss: 0.221141\n",
      "Train Epoch: 15 [25000/68994 (36%)]\tLoss: 0.194428\n",
      "Train Epoch: 15 [26000/68994 (38%)]\tLoss: 0.209146\n",
      "Train Epoch: 15 [27000/68994 (39%)]\tLoss: 0.210606\n",
      "Train Epoch: 15 [28000/68994 (41%)]\tLoss: 0.231254\n",
      "Train Epoch: 15 [29000/68994 (42%)]\tLoss: 0.210020\n",
      "Train Epoch: 15 [30000/68994 (43%)]\tLoss: 0.198705\n",
      "Train Epoch: 15 [31000/68994 (45%)]\tLoss: 0.222142\n",
      "Train Epoch: 15 [32000/68994 (46%)]\tLoss: 0.187666\n",
      "Train Epoch: 15 [33000/68994 (48%)]\tLoss: 0.224612\n",
      "Train Epoch: 15 [34000/68994 (49%)]\tLoss: 0.208665\n",
      "Train Epoch: 15 [35000/68994 (51%)]\tLoss: 0.224849\n",
      "Train Epoch: 15 [36000/68994 (52%)]\tLoss: 0.200851\n",
      "Train Epoch: 15 [37000/68994 (54%)]\tLoss: 0.203141\n",
      "Train Epoch: 15 [38000/68994 (55%)]\tLoss: 0.209448\n",
      "Train Epoch: 15 [39000/68994 (57%)]\tLoss: 0.228615\n",
      "Train Epoch: 15 [40000/68994 (58%)]\tLoss: 0.219304\n",
      "Train Epoch: 15 [41000/68994 (59%)]\tLoss: 0.216528\n",
      "Train Epoch: 15 [42000/68994 (61%)]\tLoss: 0.218132\n",
      "Train Epoch: 15 [43000/68994 (62%)]\tLoss: 0.195901\n",
      "Train Epoch: 15 [44000/68994 (64%)]\tLoss: 0.198033\n",
      "Train Epoch: 15 [45000/68994 (65%)]\tLoss: 0.233352\n",
      "Train Epoch: 15 [46000/68994 (67%)]\tLoss: 0.228344\n",
      "Train Epoch: 15 [47000/68994 (68%)]\tLoss: 0.212182\n",
      "Train Epoch: 15 [48000/68994 (70%)]\tLoss: 0.197692\n",
      "Train Epoch: 15 [49000/68994 (71%)]\tLoss: 0.179085\n",
      "Train Epoch: 15 [50000/68994 (72%)]\tLoss: 0.167615\n",
      "Train Epoch: 15 [51000/68994 (74%)]\tLoss: 0.217002\n",
      "Train Epoch: 15 [52000/68994 (75%)]\tLoss: 0.219498\n",
      "Train Epoch: 15 [53000/68994 (77%)]\tLoss: 0.205371\n",
      "Train Epoch: 15 [54000/68994 (78%)]\tLoss: 0.202674\n",
      "Train Epoch: 15 [55000/68994 (80%)]\tLoss: 0.210646\n",
      "Train Epoch: 15 [56000/68994 (81%)]\tLoss: 0.210897\n",
      "Train Epoch: 15 [57000/68994 (83%)]\tLoss: 0.182864\n",
      "Train Epoch: 15 [58000/68994 (84%)]\tLoss: 0.215965\n",
      "Train Epoch: 15 [59000/68994 (86%)]\tLoss: 0.228527\n",
      "Train Epoch: 15 [60000/68994 (87%)]\tLoss: 0.248062\n",
      "Train Epoch: 15 [61000/68994 (88%)]\tLoss: 0.206455\n",
      "Train Epoch: 15 [62000/68994 (90%)]\tLoss: 0.205767\n",
      "Train Epoch: 15 [63000/68994 (91%)]\tLoss: 0.234335\n",
      "Train Epoch: 15 [64000/68994 (93%)]\tLoss: 0.223923\n",
      "Train Epoch: 15 [65000/68994 (94%)]\tLoss: 0.211326\n",
      "Train Epoch: 15 [66000/68994 (96%)]\tLoss: 0.196881\n",
      "Train Epoch: 15 [67000/68994 (97%)]\tLoss: 0.201289\n",
      "Train Epoch: 15 [68000/68994 (99%)]\tLoss: 0.191953\n",
      "====> Epoch: 15 Average train loss: 0.2094\n",
      "====> Epoch: 15 Average test loss: 0.2082 Average hit rate: 66.0000\n",
      "Train Epoch: 16 [0/68994 (0%)]\tLoss: 0.188950\n",
      "Train Epoch: 16 [1000/68994 (1%)]\tLoss: 0.206314\n",
      "Train Epoch: 16 [2000/68994 (3%)]\tLoss: 0.200566\n",
      "Train Epoch: 16 [3000/68994 (4%)]\tLoss: 0.237862\n",
      "Train Epoch: 16 [4000/68994 (6%)]\tLoss: 0.209674\n",
      "Train Epoch: 16 [5000/68994 (7%)]\tLoss: 0.198413\n",
      "Train Epoch: 16 [6000/68994 (9%)]\tLoss: 0.210455\n",
      "Train Epoch: 16 [7000/68994 (10%)]\tLoss: 0.178070\n",
      "Train Epoch: 16 [8000/68994 (12%)]\tLoss: 0.200341\n",
      "Train Epoch: 16 [9000/68994 (13%)]\tLoss: 0.182101\n",
      "Train Epoch: 16 [10000/68994 (14%)]\tLoss: 0.217278\n",
      "Train Epoch: 16 [11000/68994 (16%)]\tLoss: 0.229723\n",
      "Train Epoch: 16 [12000/68994 (17%)]\tLoss: 0.233650\n",
      "Train Epoch: 16 [13000/68994 (19%)]\tLoss: 0.202533\n",
      "Train Epoch: 16 [14000/68994 (20%)]\tLoss: 0.195160\n",
      "Train Epoch: 16 [15000/68994 (22%)]\tLoss: 0.218839\n",
      "Train Epoch: 16 [16000/68994 (23%)]\tLoss: 0.223738\n",
      "Train Epoch: 16 [17000/68994 (25%)]\tLoss: 0.214342\n",
      "Train Epoch: 16 [18000/68994 (26%)]\tLoss: 0.175178\n",
      "Train Epoch: 16 [19000/68994 (28%)]\tLoss: 0.209710\n",
      "Train Epoch: 16 [20000/68994 (29%)]\tLoss: 0.199498\n",
      "Train Epoch: 16 [21000/68994 (30%)]\tLoss: 0.195493\n",
      "Train Epoch: 16 [22000/68994 (32%)]\tLoss: 0.225444\n",
      "Train Epoch: 16 [23000/68994 (33%)]\tLoss: 0.211385\n",
      "Train Epoch: 16 [24000/68994 (35%)]\tLoss: 0.189446\n",
      "Train Epoch: 16 [25000/68994 (36%)]\tLoss: 0.231874\n",
      "Train Epoch: 16 [26000/68994 (38%)]\tLoss: 0.213448\n",
      "Train Epoch: 16 [27000/68994 (39%)]\tLoss: 0.209693\n",
      "Train Epoch: 16 [28000/68994 (41%)]\tLoss: 0.213511\n",
      "Train Epoch: 16 [29000/68994 (42%)]\tLoss: 0.219840\n",
      "Train Epoch: 16 [30000/68994 (43%)]\tLoss: 0.219670\n",
      "Train Epoch: 16 [31000/68994 (45%)]\tLoss: 0.192704\n",
      "Train Epoch: 16 [32000/68994 (46%)]\tLoss: 0.194341\n",
      "Train Epoch: 16 [33000/68994 (48%)]\tLoss: 0.203934\n",
      "Train Epoch: 16 [34000/68994 (49%)]\tLoss: 0.245631\n",
      "Train Epoch: 16 [35000/68994 (51%)]\tLoss: 0.195509\n",
      "Train Epoch: 16 [36000/68994 (52%)]\tLoss: 0.214138\n",
      "Train Epoch: 16 [37000/68994 (54%)]\tLoss: 0.223307\n",
      "Train Epoch: 16 [38000/68994 (55%)]\tLoss: 0.231498\n",
      "Train Epoch: 16 [39000/68994 (57%)]\tLoss: 0.167424\n",
      "Train Epoch: 16 [40000/68994 (58%)]\tLoss: 0.182910\n",
      "Train Epoch: 16 [41000/68994 (59%)]\tLoss: 0.224970\n",
      "Train Epoch: 16 [42000/68994 (61%)]\tLoss: 0.208762\n",
      "Train Epoch: 16 [43000/68994 (62%)]\tLoss: 0.229411\n",
      "Train Epoch: 16 [44000/68994 (64%)]\tLoss: 0.229514\n",
      "Train Epoch: 16 [45000/68994 (65%)]\tLoss: 0.186869\n",
      "Train Epoch: 16 [46000/68994 (67%)]\tLoss: 0.196975\n",
      "Train Epoch: 16 [47000/68994 (68%)]\tLoss: 0.209269\n",
      "Train Epoch: 16 [48000/68994 (70%)]\tLoss: 0.204845\n",
      "Train Epoch: 16 [49000/68994 (71%)]\tLoss: 0.207474\n",
      "Train Epoch: 16 [50000/68994 (72%)]\tLoss: 0.211893\n",
      "Train Epoch: 16 [51000/68994 (74%)]\tLoss: 0.198470\n",
      "Train Epoch: 16 [52000/68994 (75%)]\tLoss: 0.206818\n",
      "Train Epoch: 16 [53000/68994 (77%)]\tLoss: 0.210511\n",
      "Train Epoch: 16 [54000/68994 (78%)]\tLoss: 0.193498\n",
      "Train Epoch: 16 [55000/68994 (80%)]\tLoss: 0.229236\n",
      "Train Epoch: 16 [56000/68994 (81%)]\tLoss: 0.191942\n",
      "Train Epoch: 16 [57000/68994 (83%)]\tLoss: 0.212355\n",
      "Train Epoch: 16 [58000/68994 (84%)]\tLoss: 0.201543\n",
      "Train Epoch: 16 [59000/68994 (86%)]\tLoss: 0.195655\n",
      "Train Epoch: 16 [60000/68994 (87%)]\tLoss: 0.212658\n",
      "Train Epoch: 16 [61000/68994 (88%)]\tLoss: 0.238581\n",
      "Train Epoch: 16 [62000/68994 (90%)]\tLoss: 0.188546\n",
      "Train Epoch: 16 [63000/68994 (91%)]\tLoss: 0.232724\n",
      "Train Epoch: 16 [64000/68994 (93%)]\tLoss: 0.200829\n",
      "Train Epoch: 16 [65000/68994 (94%)]\tLoss: 0.200176\n",
      "Train Epoch: 16 [66000/68994 (96%)]\tLoss: 0.204198\n",
      "Train Epoch: 16 [67000/68994 (97%)]\tLoss: 0.202663\n",
      "Train Epoch: 16 [68000/68994 (99%)]\tLoss: 0.222172\n",
      "====> Epoch: 16 Average train loss: 0.2095\n",
      "====> Epoch: 16 Average test loss: 0.2080 Average hit rate: 66.0000\n",
      "Train Epoch: 17 [0/68994 (0%)]\tLoss: 0.186954\n",
      "Train Epoch: 17 [1000/68994 (1%)]\tLoss: 0.215840\n",
      "Train Epoch: 17 [2000/68994 (3%)]\tLoss: 0.204804\n",
      "Train Epoch: 17 [3000/68994 (4%)]\tLoss: 0.214635\n",
      "Train Epoch: 17 [4000/68994 (6%)]\tLoss: 0.172673\n",
      "Train Epoch: 17 [5000/68994 (7%)]\tLoss: 0.214528\n",
      "Train Epoch: 17 [6000/68994 (9%)]\tLoss: 0.233005\n",
      "Train Epoch: 17 [7000/68994 (10%)]\tLoss: 0.203357\n",
      "Train Epoch: 17 [8000/68994 (12%)]\tLoss: 0.218459\n",
      "Train Epoch: 17 [9000/68994 (13%)]\tLoss: 0.219381\n",
      "Train Epoch: 17 [10000/68994 (14%)]\tLoss: 0.196539\n",
      "Train Epoch: 17 [11000/68994 (16%)]\tLoss: 0.219262\n",
      "Train Epoch: 17 [12000/68994 (17%)]\tLoss: 0.184139\n",
      "Train Epoch: 17 [13000/68994 (19%)]\tLoss: 0.209602\n",
      "Train Epoch: 17 [14000/68994 (20%)]\tLoss: 0.200232\n",
      "Train Epoch: 17 [15000/68994 (22%)]\tLoss: 0.217167\n",
      "Train Epoch: 17 [16000/68994 (23%)]\tLoss: 0.211562\n",
      "Train Epoch: 17 [17000/68994 (25%)]\tLoss: 0.243533\n",
      "Train Epoch: 17 [18000/68994 (26%)]\tLoss: 0.253250\n",
      "Train Epoch: 17 [19000/68994 (28%)]\tLoss: 0.169537\n",
      "Train Epoch: 17 [20000/68994 (29%)]\tLoss: 0.214341\n",
      "Train Epoch: 17 [21000/68994 (30%)]\tLoss: 0.190518\n",
      "Train Epoch: 17 [22000/68994 (32%)]\tLoss: 0.206063\n",
      "Train Epoch: 17 [23000/68994 (33%)]\tLoss: 0.199392\n",
      "Train Epoch: 17 [24000/68994 (35%)]\tLoss: 0.181953\n",
      "Train Epoch: 17 [25000/68994 (36%)]\tLoss: 0.189734\n",
      "Train Epoch: 17 [26000/68994 (38%)]\tLoss: 0.209882\n",
      "Train Epoch: 17 [27000/68994 (39%)]\tLoss: 0.211386\n",
      "Train Epoch: 17 [28000/68994 (41%)]\tLoss: 0.213266\n",
      "Train Epoch: 17 [29000/68994 (42%)]\tLoss: 0.206886\n",
      "Train Epoch: 17 [30000/68994 (43%)]\tLoss: 0.211673\n",
      "Train Epoch: 17 [31000/68994 (45%)]\tLoss: 0.183991\n",
      "Train Epoch: 17 [32000/68994 (46%)]\tLoss: 0.219743\n",
      "Train Epoch: 17 [33000/68994 (48%)]\tLoss: 0.239082\n",
      "Train Epoch: 17 [34000/68994 (49%)]\tLoss: 0.200924\n",
      "Train Epoch: 17 [35000/68994 (51%)]\tLoss: 0.207411\n",
      "Train Epoch: 17 [36000/68994 (52%)]\tLoss: 0.207748\n",
      "Train Epoch: 17 [37000/68994 (54%)]\tLoss: 0.235909\n",
      "Train Epoch: 17 [38000/68994 (55%)]\tLoss: 0.226674\n",
      "Train Epoch: 17 [39000/68994 (57%)]\tLoss: 0.232032\n",
      "Train Epoch: 17 [40000/68994 (58%)]\tLoss: 0.210466\n",
      "Train Epoch: 17 [41000/68994 (59%)]\tLoss: 0.191382\n",
      "Train Epoch: 17 [42000/68994 (61%)]\tLoss: 0.200868\n",
      "Train Epoch: 17 [43000/68994 (62%)]\tLoss: 0.214502\n",
      "Train Epoch: 17 [44000/68994 (64%)]\tLoss: 0.226085\n",
      "Train Epoch: 17 [45000/68994 (65%)]\tLoss: 0.200341\n",
      "Train Epoch: 17 [46000/68994 (67%)]\tLoss: 0.186405\n",
      "Train Epoch: 17 [47000/68994 (68%)]\tLoss: 0.220079\n",
      "Train Epoch: 17 [48000/68994 (70%)]\tLoss: 0.209348\n",
      "Train Epoch: 17 [49000/68994 (71%)]\tLoss: 0.200321\n",
      "Train Epoch: 17 [50000/68994 (72%)]\tLoss: 0.180475\n",
      "Train Epoch: 17 [51000/68994 (74%)]\tLoss: 0.204341\n",
      "Train Epoch: 17 [52000/68994 (75%)]\tLoss: 0.206215\n",
      "Train Epoch: 17 [53000/68994 (77%)]\tLoss: 0.195813\n",
      "Train Epoch: 17 [54000/68994 (78%)]\tLoss: 0.191487\n",
      "Train Epoch: 17 [55000/68994 (80%)]\tLoss: 0.222341\n",
      "Train Epoch: 17 [56000/68994 (81%)]\tLoss: 0.232445\n",
      "Train Epoch: 17 [57000/68994 (83%)]\tLoss: 0.212257\n",
      "Train Epoch: 17 [58000/68994 (84%)]\tLoss: 0.185302\n",
      "Train Epoch: 17 [59000/68994 (86%)]\tLoss: 0.200125\n",
      "Train Epoch: 17 [60000/68994 (87%)]\tLoss: 0.231619\n",
      "Train Epoch: 17 [61000/68994 (88%)]\tLoss: 0.229187\n",
      "Train Epoch: 17 [62000/68994 (90%)]\tLoss: 0.220503\n",
      "Train Epoch: 17 [63000/68994 (91%)]\tLoss: 0.181996\n",
      "Train Epoch: 17 [64000/68994 (93%)]\tLoss: 0.271821\n",
      "Train Epoch: 17 [65000/68994 (94%)]\tLoss: 0.215609\n",
      "Train Epoch: 17 [66000/68994 (96%)]\tLoss: 0.189348\n",
      "Train Epoch: 17 [67000/68994 (97%)]\tLoss: 0.201802\n",
      "Train Epoch: 17 [68000/68994 (99%)]\tLoss: 0.209073\n",
      "====> Epoch: 17 Average train loss: 0.2093\n",
      "====> Epoch: 17 Average test loss: 0.2083 Average hit rate: 65.0000\n",
      "Train Epoch: 18 [0/68994 (0%)]\tLoss: 0.207195\n",
      "Train Epoch: 18 [1000/68994 (1%)]\tLoss: 0.216718\n",
      "Train Epoch: 18 [2000/68994 (3%)]\tLoss: 0.190573\n",
      "Train Epoch: 18 [3000/68994 (4%)]\tLoss: 0.219263\n",
      "Train Epoch: 18 [4000/68994 (6%)]\tLoss: 0.208255\n",
      "Train Epoch: 18 [5000/68994 (7%)]\tLoss: 0.167051\n",
      "Train Epoch: 18 [6000/68994 (9%)]\tLoss: 0.191581\n",
      "Train Epoch: 18 [7000/68994 (10%)]\tLoss: 0.213865\n",
      "Train Epoch: 18 [8000/68994 (12%)]\tLoss: 0.210947\n",
      "Train Epoch: 18 [9000/68994 (13%)]\tLoss: 0.236103\n",
      "Train Epoch: 18 [10000/68994 (14%)]\tLoss: 0.213356\n",
      "Train Epoch: 18 [11000/68994 (16%)]\tLoss: 0.226690\n",
      "Train Epoch: 18 [12000/68994 (17%)]\tLoss: 0.177937\n",
      "Train Epoch: 18 [13000/68994 (19%)]\tLoss: 0.176069\n",
      "Train Epoch: 18 [14000/68994 (20%)]\tLoss: 0.200086\n",
      "Train Epoch: 18 [15000/68994 (22%)]\tLoss: 0.216167\n",
      "Train Epoch: 18 [16000/68994 (23%)]\tLoss: 0.172409\n",
      "Train Epoch: 18 [17000/68994 (25%)]\tLoss: 0.218000\n",
      "Train Epoch: 18 [18000/68994 (26%)]\tLoss: 0.220037\n",
      "Train Epoch: 18 [19000/68994 (28%)]\tLoss: 0.225164\n",
      "Train Epoch: 18 [20000/68994 (29%)]\tLoss: 0.210909\n",
      "Train Epoch: 18 [21000/68994 (30%)]\tLoss: 0.216790\n",
      "Train Epoch: 18 [22000/68994 (32%)]\tLoss: 0.220606\n",
      "Train Epoch: 18 [23000/68994 (33%)]\tLoss: 0.244010\n",
      "Train Epoch: 18 [24000/68994 (35%)]\tLoss: 0.177978\n",
      "Train Epoch: 18 [25000/68994 (36%)]\tLoss: 0.181761\n",
      "Train Epoch: 18 [26000/68994 (38%)]\tLoss: 0.207254\n",
      "Train Epoch: 18 [27000/68994 (39%)]\tLoss: 0.204293\n",
      "Train Epoch: 18 [28000/68994 (41%)]\tLoss: 0.217684\n",
      "Train Epoch: 18 [29000/68994 (42%)]\tLoss: 0.182616\n",
      "Train Epoch: 18 [30000/68994 (43%)]\tLoss: 0.190627\n",
      "Train Epoch: 18 [31000/68994 (45%)]\tLoss: 0.182924\n",
      "Train Epoch: 18 [32000/68994 (46%)]\tLoss: 0.222018\n",
      "Train Epoch: 18 [33000/68994 (48%)]\tLoss: 0.213097\n",
      "Train Epoch: 18 [34000/68994 (49%)]\tLoss: 0.253447\n",
      "Train Epoch: 18 [35000/68994 (51%)]\tLoss: 0.194076\n",
      "Train Epoch: 18 [36000/68994 (52%)]\tLoss: 0.208908\n",
      "Train Epoch: 18 [37000/68994 (54%)]\tLoss: 0.191840\n",
      "Train Epoch: 18 [38000/68994 (55%)]\tLoss: 0.226882\n",
      "Train Epoch: 18 [39000/68994 (57%)]\tLoss: 0.198202\n",
      "Train Epoch: 18 [40000/68994 (58%)]\tLoss: 0.209141\n",
      "Train Epoch: 18 [41000/68994 (59%)]\tLoss: 0.223200\n",
      "Train Epoch: 18 [42000/68994 (61%)]\tLoss: 0.239247\n",
      "Train Epoch: 18 [43000/68994 (62%)]\tLoss: 0.202433\n",
      "Train Epoch: 18 [44000/68994 (64%)]\tLoss: 0.197069\n",
      "Train Epoch: 18 [45000/68994 (65%)]\tLoss: 0.215240\n",
      "Train Epoch: 18 [46000/68994 (67%)]\tLoss: 0.210251\n",
      "Train Epoch: 18 [47000/68994 (68%)]\tLoss: 0.184951\n",
      "Train Epoch: 18 [48000/68994 (70%)]\tLoss: 0.215160\n",
      "Train Epoch: 18 [49000/68994 (71%)]\tLoss: 0.213880\n",
      "Train Epoch: 18 [50000/68994 (72%)]\tLoss: 0.216603\n",
      "Train Epoch: 18 [51000/68994 (74%)]\tLoss: 0.212928\n",
      "Train Epoch: 18 [52000/68994 (75%)]\tLoss: 0.216823\n",
      "Train Epoch: 18 [53000/68994 (77%)]\tLoss: 0.205110\n",
      "Train Epoch: 18 [54000/68994 (78%)]\tLoss: 0.211627\n",
      "Train Epoch: 18 [55000/68994 (80%)]\tLoss: 0.237399\n",
      "Train Epoch: 18 [56000/68994 (81%)]\tLoss: 0.221100\n",
      "Train Epoch: 18 [57000/68994 (83%)]\tLoss: 0.217654\n",
      "Train Epoch: 18 [58000/68994 (84%)]\tLoss: 0.235046\n",
      "Train Epoch: 18 [59000/68994 (86%)]\tLoss: 0.195641\n",
      "Train Epoch: 18 [60000/68994 (87%)]\tLoss: 0.193137\n",
      "Train Epoch: 18 [61000/68994 (88%)]\tLoss: 0.216742\n",
      "Train Epoch: 18 [62000/68994 (90%)]\tLoss: 0.203049\n",
      "Train Epoch: 18 [63000/68994 (91%)]\tLoss: 0.222999\n",
      "Train Epoch: 18 [64000/68994 (93%)]\tLoss: 0.210025\n",
      "Train Epoch: 18 [65000/68994 (94%)]\tLoss: 0.219301\n",
      "Train Epoch: 18 [66000/68994 (96%)]\tLoss: 0.208866\n",
      "Train Epoch: 18 [67000/68994 (97%)]\tLoss: 0.234014\n",
      "Train Epoch: 18 [68000/68994 (99%)]\tLoss: 0.183679\n",
      "====> Epoch: 18 Average train loss: 0.2094\n",
      "====> Epoch: 18 Average test loss: 0.2097 Average hit rate: 66.0000\n",
      "Train Epoch: 19 [0/68994 (0%)]\tLoss: 0.203993\n",
      "Train Epoch: 19 [1000/68994 (1%)]\tLoss: 0.208249\n",
      "Train Epoch: 19 [2000/68994 (3%)]\tLoss: 0.225736\n",
      "Train Epoch: 19 [3000/68994 (4%)]\tLoss: 0.222880\n",
      "Train Epoch: 19 [4000/68994 (6%)]\tLoss: 0.207736\n",
      "Train Epoch: 19 [5000/68994 (7%)]\tLoss: 0.187209\n",
      "Train Epoch: 19 [6000/68994 (9%)]\tLoss: 0.211796\n",
      "Train Epoch: 19 [7000/68994 (10%)]\tLoss: 0.220554\n",
      "Train Epoch: 19 [8000/68994 (12%)]\tLoss: 0.219440\n",
      "Train Epoch: 19 [9000/68994 (13%)]\tLoss: 0.222097\n",
      "Train Epoch: 19 [10000/68994 (14%)]\tLoss: 0.216302\n",
      "Train Epoch: 19 [11000/68994 (16%)]\tLoss: 0.234636\n",
      "Train Epoch: 19 [12000/68994 (17%)]\tLoss: 0.227365\n",
      "Train Epoch: 19 [13000/68994 (19%)]\tLoss: 0.211464\n",
      "Train Epoch: 19 [14000/68994 (20%)]\tLoss: 0.248818\n",
      "Train Epoch: 19 [15000/68994 (22%)]\tLoss: 0.213800\n",
      "Train Epoch: 19 [16000/68994 (23%)]\tLoss: 0.189927\n",
      "Train Epoch: 19 [17000/68994 (25%)]\tLoss: 0.215342\n",
      "Train Epoch: 19 [18000/68994 (26%)]\tLoss: 0.201733\n",
      "Train Epoch: 19 [19000/68994 (28%)]\tLoss: 0.227935\n",
      "Train Epoch: 19 [20000/68994 (29%)]\tLoss: 0.212438\n",
      "Train Epoch: 19 [21000/68994 (30%)]\tLoss: 0.210842\n",
      "Train Epoch: 19 [22000/68994 (32%)]\tLoss: 0.219263\n",
      "Train Epoch: 19 [23000/68994 (33%)]\tLoss: 0.216185\n",
      "Train Epoch: 19 [24000/68994 (35%)]\tLoss: 0.219255\n",
      "Train Epoch: 19 [25000/68994 (36%)]\tLoss: 0.209444\n",
      "Train Epoch: 19 [26000/68994 (38%)]\tLoss: 0.202396\n",
      "Train Epoch: 19 [27000/68994 (39%)]\tLoss: 0.224554\n",
      "Train Epoch: 19 [28000/68994 (41%)]\tLoss: 0.229308\n",
      "Train Epoch: 19 [29000/68994 (42%)]\tLoss: 0.203933\n",
      "Train Epoch: 19 [30000/68994 (43%)]\tLoss: 0.185064\n",
      "Train Epoch: 19 [31000/68994 (45%)]\tLoss: 0.247253\n",
      "Train Epoch: 19 [32000/68994 (46%)]\tLoss: 0.236127\n",
      "Train Epoch: 19 [33000/68994 (48%)]\tLoss: 0.193489\n",
      "Train Epoch: 19 [34000/68994 (49%)]\tLoss: 0.221717\n",
      "Train Epoch: 19 [35000/68994 (51%)]\tLoss: 0.207649\n",
      "Train Epoch: 19 [36000/68994 (52%)]\tLoss: 0.245851\n",
      "Train Epoch: 19 [37000/68994 (54%)]\tLoss: 0.222717\n",
      "Train Epoch: 19 [38000/68994 (55%)]\tLoss: 0.195756\n",
      "Train Epoch: 19 [39000/68994 (57%)]\tLoss: 0.224016\n",
      "Train Epoch: 19 [40000/68994 (58%)]\tLoss: 0.210077\n",
      "Train Epoch: 19 [41000/68994 (59%)]\tLoss: 0.207134\n",
      "Train Epoch: 19 [42000/68994 (61%)]\tLoss: 0.225958\n",
      "Train Epoch: 19 [43000/68994 (62%)]\tLoss: 0.201871\n",
      "Train Epoch: 19 [44000/68994 (64%)]\tLoss: 0.225668\n",
      "Train Epoch: 19 [45000/68994 (65%)]\tLoss: 0.185092\n",
      "Train Epoch: 19 [46000/68994 (67%)]\tLoss: 0.229778\n",
      "Train Epoch: 19 [47000/68994 (68%)]\tLoss: 0.197829\n",
      "Train Epoch: 19 [48000/68994 (70%)]\tLoss: 0.221875\n",
      "Train Epoch: 19 [49000/68994 (71%)]\tLoss: 0.205790\n",
      "Train Epoch: 19 [50000/68994 (72%)]\tLoss: 0.207154\n",
      "Train Epoch: 19 [51000/68994 (74%)]\tLoss: 0.196260\n",
      "Train Epoch: 19 [52000/68994 (75%)]\tLoss: 0.212196\n",
      "Train Epoch: 19 [53000/68994 (77%)]\tLoss: 0.214247\n",
      "Train Epoch: 19 [54000/68994 (78%)]\tLoss: 0.196383\n",
      "Train Epoch: 19 [55000/68994 (80%)]\tLoss: 0.219837\n",
      "Train Epoch: 19 [56000/68994 (81%)]\tLoss: 0.196716\n",
      "Train Epoch: 19 [57000/68994 (83%)]\tLoss: 0.221125\n",
      "Train Epoch: 19 [58000/68994 (84%)]\tLoss: 0.173123\n",
      "Train Epoch: 19 [59000/68994 (86%)]\tLoss: 0.191151\n",
      "Train Epoch: 19 [60000/68994 (87%)]\tLoss: 0.201710\n",
      "Train Epoch: 19 [61000/68994 (88%)]\tLoss: 0.203985\n",
      "Train Epoch: 19 [62000/68994 (90%)]\tLoss: 0.170185\n",
      "Train Epoch: 19 [63000/68994 (91%)]\tLoss: 0.220273\n",
      "Train Epoch: 19 [64000/68994 (93%)]\tLoss: 0.217986\n",
      "Train Epoch: 19 [65000/68994 (94%)]\tLoss: 0.209684\n",
      "Train Epoch: 19 [66000/68994 (96%)]\tLoss: 0.241790\n",
      "Train Epoch: 19 [67000/68994 (97%)]\tLoss: 0.181154\n",
      "Train Epoch: 19 [68000/68994 (99%)]\tLoss: 0.206161\n",
      "====> Epoch: 19 Average train loss: 0.2098\n",
      "====> Epoch: 19 Average test loss: 0.2074 Average hit rate: 66.0000\n",
      "Train Epoch: 20 [0/68994 (0%)]\tLoss: 0.201983\n",
      "Train Epoch: 20 [1000/68994 (1%)]\tLoss: 0.210915\n",
      "Train Epoch: 20 [2000/68994 (3%)]\tLoss: 0.216999\n",
      "Train Epoch: 20 [3000/68994 (4%)]\tLoss: 0.185049\n",
      "Train Epoch: 20 [4000/68994 (6%)]\tLoss: 0.197428\n",
      "Train Epoch: 20 [5000/68994 (7%)]\tLoss: 0.213051\n",
      "Train Epoch: 20 [6000/68994 (9%)]\tLoss: 0.211505\n",
      "Train Epoch: 20 [7000/68994 (10%)]\tLoss: 0.213818\n",
      "Train Epoch: 20 [8000/68994 (12%)]\tLoss: 0.233293\n",
      "Train Epoch: 20 [9000/68994 (13%)]\tLoss: 0.212683\n",
      "Train Epoch: 20 [10000/68994 (14%)]\tLoss: 0.266771\n",
      "Train Epoch: 20 [11000/68994 (16%)]\tLoss: 0.205241\n",
      "Train Epoch: 20 [12000/68994 (17%)]\tLoss: 0.198741\n",
      "Train Epoch: 20 [13000/68994 (19%)]\tLoss: 0.223582\n",
      "Train Epoch: 20 [14000/68994 (20%)]\tLoss: 0.196502\n",
      "Train Epoch: 20 [15000/68994 (22%)]\tLoss: 0.204167\n",
      "Train Epoch: 20 [16000/68994 (23%)]\tLoss: 0.215213\n",
      "Train Epoch: 20 [17000/68994 (25%)]\tLoss: 0.226735\n",
      "Train Epoch: 20 [18000/68994 (26%)]\tLoss: 0.194178\n",
      "Train Epoch: 20 [19000/68994 (28%)]\tLoss: 0.224412\n",
      "Train Epoch: 20 [20000/68994 (29%)]\tLoss: 0.205380\n",
      "Train Epoch: 20 [21000/68994 (30%)]\tLoss: 0.210823\n",
      "Train Epoch: 20 [22000/68994 (32%)]\tLoss: 0.232721\n",
      "Train Epoch: 20 [23000/68994 (33%)]\tLoss: 0.216155\n",
      "Train Epoch: 20 [24000/68994 (35%)]\tLoss: 0.203661\n",
      "Train Epoch: 20 [25000/68994 (36%)]\tLoss: 0.201203\n",
      "Train Epoch: 20 [26000/68994 (38%)]\tLoss: 0.209592\n",
      "Train Epoch: 20 [27000/68994 (39%)]\tLoss: 0.216482\n",
      "Train Epoch: 20 [28000/68994 (41%)]\tLoss: 0.210184\n",
      "Train Epoch: 20 [29000/68994 (42%)]\tLoss: 0.194409\n",
      "Train Epoch: 20 [30000/68994 (43%)]\tLoss: 0.219394\n",
      "Train Epoch: 20 [31000/68994 (45%)]\tLoss: 0.214252\n",
      "Train Epoch: 20 [32000/68994 (46%)]\tLoss: 0.209813\n",
      "Train Epoch: 20 [33000/68994 (48%)]\tLoss: 0.219234\n",
      "Train Epoch: 20 [34000/68994 (49%)]\tLoss: 0.217580\n",
      "Train Epoch: 20 [35000/68994 (51%)]\tLoss: 0.213090\n",
      "Train Epoch: 20 [36000/68994 (52%)]\tLoss: 0.203309\n",
      "Train Epoch: 20 [37000/68994 (54%)]\tLoss: 0.207103\n",
      "Train Epoch: 20 [38000/68994 (55%)]\tLoss: 0.200887\n",
      "Train Epoch: 20 [39000/68994 (57%)]\tLoss: 0.223071\n",
      "Train Epoch: 20 [40000/68994 (58%)]\tLoss: 0.224534\n",
      "Train Epoch: 20 [41000/68994 (59%)]\tLoss: 0.187460\n",
      "Train Epoch: 20 [42000/68994 (61%)]\tLoss: 0.188134\n",
      "Train Epoch: 20 [43000/68994 (62%)]\tLoss: 0.215154\n",
      "Train Epoch: 20 [44000/68994 (64%)]\tLoss: 0.233647\n",
      "Train Epoch: 20 [45000/68994 (65%)]\tLoss: 0.184301\n",
      "Train Epoch: 20 [46000/68994 (67%)]\tLoss: 0.219931\n",
      "Train Epoch: 20 [47000/68994 (68%)]\tLoss: 0.209036\n",
      "Train Epoch: 20 [48000/68994 (70%)]\tLoss: 0.222691\n",
      "Train Epoch: 20 [49000/68994 (71%)]\tLoss: 0.204383\n",
      "Train Epoch: 20 [50000/68994 (72%)]\tLoss: 0.181809\n",
      "Train Epoch: 20 [51000/68994 (74%)]\tLoss: 0.230557\n",
      "Train Epoch: 20 [52000/68994 (75%)]\tLoss: 0.218542\n",
      "Train Epoch: 20 [53000/68994 (77%)]\tLoss: 0.221860\n",
      "Train Epoch: 20 [54000/68994 (78%)]\tLoss: 0.234844\n",
      "Train Epoch: 20 [55000/68994 (80%)]\tLoss: 0.222776\n",
      "Train Epoch: 20 [56000/68994 (81%)]\tLoss: 0.214811\n",
      "Train Epoch: 20 [57000/68994 (83%)]\tLoss: 0.167983\n",
      "Train Epoch: 20 [58000/68994 (84%)]\tLoss: 0.196437\n",
      "Train Epoch: 20 [59000/68994 (86%)]\tLoss: 0.194375\n",
      "Train Epoch: 20 [60000/68994 (87%)]\tLoss: 0.228005\n",
      "Train Epoch: 20 [61000/68994 (88%)]\tLoss: 0.192670\n",
      "Train Epoch: 20 [62000/68994 (90%)]\tLoss: 0.209614\n",
      "Train Epoch: 20 [63000/68994 (91%)]\tLoss: 0.205426\n",
      "Train Epoch: 20 [64000/68994 (93%)]\tLoss: 0.172070\n",
      "Train Epoch: 20 [65000/68994 (94%)]\tLoss: 0.220342\n",
      "Train Epoch: 20 [66000/68994 (96%)]\tLoss: 0.234068\n",
      "Train Epoch: 20 [67000/68994 (97%)]\tLoss: 0.213231\n",
      "Train Epoch: 20 [68000/68994 (99%)]\tLoss: 0.228647\n",
      "====> Epoch: 20 Average train loss: 0.2094\n",
      "====> Epoch: 20 Average test loss: 0.2075 Average hit rate: 66.0000\n",
      "Train Epoch: 21 [0/68994 (0%)]\tLoss: 0.242275\n",
      "Train Epoch: 21 [1000/68994 (1%)]\tLoss: 0.227931\n",
      "Train Epoch: 21 [2000/68994 (3%)]\tLoss: 0.219070\n",
      "Train Epoch: 21 [3000/68994 (4%)]\tLoss: 0.199393\n",
      "Train Epoch: 21 [4000/68994 (6%)]\tLoss: 0.229865\n",
      "Train Epoch: 21 [5000/68994 (7%)]\tLoss: 0.214106\n",
      "Train Epoch: 21 [6000/68994 (9%)]\tLoss: 0.224956\n",
      "Train Epoch: 21 [7000/68994 (10%)]\tLoss: 0.219681\n",
      "Train Epoch: 21 [8000/68994 (12%)]\tLoss: 0.193763\n",
      "Train Epoch: 21 [9000/68994 (13%)]\tLoss: 0.223186\n",
      "Train Epoch: 21 [10000/68994 (14%)]\tLoss: 0.204842\n",
      "Train Epoch: 21 [11000/68994 (16%)]\tLoss: 0.221607\n",
      "Train Epoch: 21 [12000/68994 (17%)]\tLoss: 0.184268\n",
      "Train Epoch: 21 [13000/68994 (19%)]\tLoss: 0.203908\n",
      "Train Epoch: 21 [14000/68994 (20%)]\tLoss: 0.204583\n",
      "Train Epoch: 21 [15000/68994 (22%)]\tLoss: 0.191512\n",
      "Train Epoch: 21 [16000/68994 (23%)]\tLoss: 0.184105\n",
      "Train Epoch: 21 [17000/68994 (25%)]\tLoss: 0.197844\n",
      "Train Epoch: 21 [18000/68994 (26%)]\tLoss: 0.204398\n",
      "Train Epoch: 21 [19000/68994 (28%)]\tLoss: 0.224446\n",
      "Train Epoch: 21 [20000/68994 (29%)]\tLoss: 0.212895\n"
     ]
    }
   ],
   "source": [
    "run_mlp(X_train.values, Y_train.astype(int).values, X_test.values, Y_test.astype(int).values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 线性方程中有a但没有b？？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## deal with data balance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## distribution, feature selection and modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "MLA = {\n",
    "    # Ensemble Methods \n",
    "    ensemble.AdaBoostClassifier(),\n",
    "    ensemble.BaggingClassifier(),\n",
    "    ensemble.ExtraTreesClassifier(),\n",
    "    ensemble.GradientBoostingClassifier(),\n",
    "    ensemble.RandomForestClassifier(),\n",
    "    # Gaussian Processes\n",
    "    #gaussian_process.GaussianProcessClassifier(),\n",
    "    # GLM\n",
    "    linear_model.LogisticRegressionCV(),\n",
    "    linear_model.PassiveAggressiveClassifier(),\n",
    "    #linear_model.RidgeClassifierCV(),\n",
    "    linear_model.SGDClassifier(),\n",
    "    linear_model.Perceptron(),\n",
    "    # Navies Bayes\n",
    "    naive_bayes.BernoulliNB(),\n",
    "    naive_bayes.GaussianNB(),\n",
    "    # Nearest Neighbor\n",
    "    neighbors.KNeighborsClassifier(),\n",
    "    # SVM\n",
    "    #svm.SVC(probability = True),\n",
    "    #svm.NuSVC(probability = True),\n",
    "    svm.LinearSVC(),\n",
    "    # Trees\n",
    "    tree.DecisionTreeClassifier(),\n",
    "    tree.ExtraTreeClassifier(),\n",
    "    # Descriminate Analysis\n",
    "    #discriminant_analysis.LinearDiscriminantAnalysis(),\n",
    "    discriminant_analysis.QuadraticDiscriminantAnalysis(),\n",
    "    # xgboot\n",
    "    XGBClassifier()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# cross validation\n",
    "# run model with 10x with 60/30 split intentionally leaving out 10%\n",
    "cv_split = model_selection.ShuffleSplit(n_splits = 10, test_size = .3, train_size = .6, random_state = 0)\n",
    "# create table to compare MLA metrics\n",
    "MLA_columns = ['MLA Name', 'MLA Parameters', 'MLA Train Accuracy Mean',\n",
    "               'MLA Test Accuracy Mean', 'MLA Test Accuracy 3*STD', 'MLA Time']\n",
    "MLA_compare = pd.DataFrame(columns = MLA_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########################################################################################\n",
      "SGDClassifier\n",
      "########################################################################################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ihuangyiran/anaconda2/envs/data_mining/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:144: FutureWarning: max_iter and tol parameters have been added in SGDClassifier in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  FutureWarning)\n",
      "/Users/ihuangyiran/anaconda2/envs/data_mining/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:144: FutureWarning: max_iter and tol parameters have been added in SGDClassifier in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  FutureWarning)\n",
      "/Users/ihuangyiran/anaconda2/envs/data_mining/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:144: FutureWarning: max_iter and tol parameters have been added in SGDClassifier in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  FutureWarning)\n",
      "/Users/ihuangyiran/anaconda2/envs/data_mining/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:144: FutureWarning: max_iter and tol parameters have been added in SGDClassifier in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  FutureWarning)\n",
      "/Users/ihuangyiran/anaconda2/envs/data_mining/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:144: FutureWarning: max_iter and tol parameters have been added in SGDClassifier in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  FutureWarning)\n",
      "/Users/ihuangyiran/anaconda2/envs/data_mining/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:144: FutureWarning: max_iter and tol parameters have been added in SGDClassifier in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  FutureWarning)\n",
      "/Users/ihuangyiran/anaconda2/envs/data_mining/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:144: FutureWarning: max_iter and tol parameters have been added in SGDClassifier in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  FutureWarning)\n",
      "/Users/ihuangyiran/anaconda2/envs/data_mining/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:144: FutureWarning: max_iter and tol parameters have been added in SGDClassifier in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  FutureWarning)\n",
      "/Users/ihuangyiran/anaconda2/envs/data_mining/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:144: FutureWarning: max_iter and tol parameters have been added in SGDClassifier in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  FutureWarning)\n",
      "/Users/ihuangyiran/anaconda2/envs/data_mining/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:144: FutureWarning: max_iter and tol parameters have been added in SGDClassifier in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  FutureWarning)\n",
      "/Users/ihuangyiran/anaconda2/envs/data_mining/lib/python3.6/site-packages/sklearn/utils/deprecation.py:125: FutureWarning: You are accessing a training score ('train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "/Users/ihuangyiran/anaconda2/envs/data_mining/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:144: FutureWarning: max_iter and tol parameters have been added in SGDClassifier in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########################################################################################\n",
      "AdaBoostClassifier\n",
      "########################################################################################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ihuangyiran/anaconda2/envs/data_mining/lib/python3.6/site-packages/sklearn/utils/deprecation.py:125: FutureWarning: You are accessing a training score ('train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########################################################################################\n",
      "BernoulliNB\n",
      "########################################################################################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ihuangyiran/anaconda2/envs/data_mining/lib/python3.6/site-packages/sklearn/naive_bayes.py:948: RuntimeWarning: invalid value encountered in log\n",
      "  neg_prob = np.log(1 - np.exp(self.feature_log_prob_))\n",
      "/Users/ihuangyiran/anaconda2/envs/data_mining/lib/python3.6/site-packages/sklearn/naive_bayes.py:948: RuntimeWarning: invalid value encountered in log\n",
      "  neg_prob = np.log(1 - np.exp(self.feature_log_prob_))\n",
      "/Users/ihuangyiran/anaconda2/envs/data_mining/lib/python3.6/site-packages/sklearn/naive_bayes.py:948: RuntimeWarning: invalid value encountered in log\n",
      "  neg_prob = np.log(1 - np.exp(self.feature_log_prob_))\n",
      "/Users/ihuangyiran/anaconda2/envs/data_mining/lib/python3.6/site-packages/sklearn/naive_bayes.py:948: RuntimeWarning: invalid value encountered in log\n",
      "  neg_prob = np.log(1 - np.exp(self.feature_log_prob_))\n",
      "/Users/ihuangyiran/anaconda2/envs/data_mining/lib/python3.6/site-packages/sklearn/naive_bayes.py:948: RuntimeWarning: invalid value encountered in log\n",
      "  neg_prob = np.log(1 - np.exp(self.feature_log_prob_))\n",
      "/Users/ihuangyiran/anaconda2/envs/data_mining/lib/python3.6/site-packages/sklearn/naive_bayes.py:948: RuntimeWarning: invalid value encountered in log\n",
      "  neg_prob = np.log(1 - np.exp(self.feature_log_prob_))\n",
      "/Users/ihuangyiran/anaconda2/envs/data_mining/lib/python3.6/site-packages/sklearn/naive_bayes.py:948: RuntimeWarning: invalid value encountered in log\n",
      "  neg_prob = np.log(1 - np.exp(self.feature_log_prob_))\n",
      "/Users/ihuangyiran/anaconda2/envs/data_mining/lib/python3.6/site-packages/sklearn/naive_bayes.py:948: RuntimeWarning: invalid value encountered in log\n",
      "  neg_prob = np.log(1 - np.exp(self.feature_log_prob_))\n",
      "/Users/ihuangyiran/anaconda2/envs/data_mining/lib/python3.6/site-packages/sklearn/naive_bayes.py:948: RuntimeWarning: invalid value encountered in log\n",
      "  neg_prob = np.log(1 - np.exp(self.feature_log_prob_))\n",
      "/Users/ihuangyiran/anaconda2/envs/data_mining/lib/python3.6/site-packages/sklearn/naive_bayes.py:948: RuntimeWarning: invalid value encountered in log\n",
      "  neg_prob = np.log(1 - np.exp(self.feature_log_prob_))\n",
      "/Users/ihuangyiran/anaconda2/envs/data_mining/lib/python3.6/site-packages/sklearn/naive_bayes.py:948: RuntimeWarning: invalid value encountered in log\n",
      "  neg_prob = np.log(1 - np.exp(self.feature_log_prob_))\n",
      "/Users/ihuangyiran/anaconda2/envs/data_mining/lib/python3.6/site-packages/sklearn/naive_bayes.py:948: RuntimeWarning: invalid value encountered in log\n",
      "  neg_prob = np.log(1 - np.exp(self.feature_log_prob_))\n",
      "/Users/ihuangyiran/anaconda2/envs/data_mining/lib/python3.6/site-packages/sklearn/naive_bayes.py:948: RuntimeWarning: invalid value encountered in log\n",
      "  neg_prob = np.log(1 - np.exp(self.feature_log_prob_))\n",
      "/Users/ihuangyiran/anaconda2/envs/data_mining/lib/python3.6/site-packages/sklearn/naive_bayes.py:948: RuntimeWarning: invalid value encountered in log\n",
      "  neg_prob = np.log(1 - np.exp(self.feature_log_prob_))\n",
      "/Users/ihuangyiran/anaconda2/envs/data_mining/lib/python3.6/site-packages/sklearn/naive_bayes.py:948: RuntimeWarning: invalid value encountered in log\n",
      "  neg_prob = np.log(1 - np.exp(self.feature_log_prob_))\n",
      "/Users/ihuangyiran/anaconda2/envs/data_mining/lib/python3.6/site-packages/sklearn/naive_bayes.py:948: RuntimeWarning: invalid value encountered in log\n",
      "  neg_prob = np.log(1 - np.exp(self.feature_log_prob_))\n",
      "/Users/ihuangyiran/anaconda2/envs/data_mining/lib/python3.6/site-packages/sklearn/naive_bayes.py:948: RuntimeWarning: invalid value encountered in log\n",
      "  neg_prob = np.log(1 - np.exp(self.feature_log_prob_))\n",
      "/Users/ihuangyiran/anaconda2/envs/data_mining/lib/python3.6/site-packages/sklearn/naive_bayes.py:948: RuntimeWarning: invalid value encountered in log\n",
      "  neg_prob = np.log(1 - np.exp(self.feature_log_prob_))\n",
      "/Users/ihuangyiran/anaconda2/envs/data_mining/lib/python3.6/site-packages/sklearn/naive_bayes.py:948: RuntimeWarning: invalid value encountered in log\n",
      "  neg_prob = np.log(1 - np.exp(self.feature_log_prob_))\n",
      "/Users/ihuangyiran/anaconda2/envs/data_mining/lib/python3.6/site-packages/sklearn/naive_bayes.py:948: RuntimeWarning: invalid value encountered in log\n",
      "  neg_prob = np.log(1 - np.exp(self.feature_log_prob_))\n",
      "/Users/ihuangyiran/anaconda2/envs/data_mining/lib/python3.6/site-packages/sklearn/utils/deprecation.py:125: FutureWarning: You are accessing a training score ('train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "/Users/ihuangyiran/anaconda2/envs/data_mining/lib/python3.6/site-packages/sklearn/naive_bayes.py:948: RuntimeWarning: invalid value encountered in log\n",
      "  neg_prob = np.log(1 - np.exp(self.feature_log_prob_))\n",
      "/Users/ihuangyiran/anaconda2/envs/data_mining/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:144: FutureWarning: max_iter and tol parameters have been added in Perceptron in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  FutureWarning)\n",
      "/Users/ihuangyiran/anaconda2/envs/data_mining/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:144: FutureWarning: max_iter and tol parameters have been added in Perceptron in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  FutureWarning)\n",
      "/Users/ihuangyiran/anaconda2/envs/data_mining/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:144: FutureWarning: max_iter and tol parameters have been added in Perceptron in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########################################################################################\n",
      "Perceptron\n",
      "########################################################################################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ihuangyiran/anaconda2/envs/data_mining/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:144: FutureWarning: max_iter and tol parameters have been added in Perceptron in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  FutureWarning)\n",
      "/Users/ihuangyiran/anaconda2/envs/data_mining/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:144: FutureWarning: max_iter and tol parameters have been added in Perceptron in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  FutureWarning)\n",
      "/Users/ihuangyiran/anaconda2/envs/data_mining/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:144: FutureWarning: max_iter and tol parameters have been added in Perceptron in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  FutureWarning)\n",
      "/Users/ihuangyiran/anaconda2/envs/data_mining/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:144: FutureWarning: max_iter and tol parameters have been added in Perceptron in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  FutureWarning)\n",
      "/Users/ihuangyiran/anaconda2/envs/data_mining/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:144: FutureWarning: max_iter and tol parameters have been added in Perceptron in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  FutureWarning)\n",
      "/Users/ihuangyiran/anaconda2/envs/data_mining/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:144: FutureWarning: max_iter and tol parameters have been added in Perceptron in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  FutureWarning)\n",
      "/Users/ihuangyiran/anaconda2/envs/data_mining/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:144: FutureWarning: max_iter and tol parameters have been added in Perceptron in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  FutureWarning)\n",
      "/Users/ihuangyiran/anaconda2/envs/data_mining/lib/python3.6/site-packages/sklearn/utils/deprecation.py:125: FutureWarning: You are accessing a training score ('train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "/Users/ihuangyiran/anaconda2/envs/data_mining/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:144: FutureWarning: max_iter and tol parameters have been added in Perceptron in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########################################################################################\n",
      "RandomForestClassifier\n",
      "########################################################################################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ihuangyiran/anaconda2/envs/data_mining/lib/python3.6/site-packages/sklearn/ensemble/forest.py:248: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "/Users/ihuangyiran/anaconda2/envs/data_mining/lib/python3.6/site-packages/sklearn/ensemble/forest.py:248: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "/Users/ihuangyiran/anaconda2/envs/data_mining/lib/python3.6/site-packages/sklearn/ensemble/forest.py:248: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "/Users/ihuangyiran/anaconda2/envs/data_mining/lib/python3.6/site-packages/sklearn/ensemble/forest.py:248: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "/Users/ihuangyiran/anaconda2/envs/data_mining/lib/python3.6/site-packages/sklearn/ensemble/forest.py:248: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "/Users/ihuangyiran/anaconda2/envs/data_mining/lib/python3.6/site-packages/sklearn/ensemble/forest.py:248: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "/Users/ihuangyiran/anaconda2/envs/data_mining/lib/python3.6/site-packages/sklearn/ensemble/forest.py:248: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "/Users/ihuangyiran/anaconda2/envs/data_mining/lib/python3.6/site-packages/sklearn/ensemble/forest.py:248: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "/Users/ihuangyiran/anaconda2/envs/data_mining/lib/python3.6/site-packages/sklearn/ensemble/forest.py:248: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "/Users/ihuangyiran/anaconda2/envs/data_mining/lib/python3.6/site-packages/sklearn/ensemble/forest.py:248: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "/Users/ihuangyiran/anaconda2/envs/data_mining/lib/python3.6/site-packages/sklearn/utils/deprecation.py:125: FutureWarning: You are accessing a training score ('train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "/Users/ihuangyiran/anaconda2/envs/data_mining/lib/python3.6/site-packages/sklearn/ensemble/forest.py:248: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########################################################################################\n",
      "KNeighborsClassifier\n",
      "########################################################################################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ihuangyiran/anaconda2/envs/data_mining/lib/python3.6/site-packages/sklearn/utils/deprecation.py:125: FutureWarning: You are accessing a training score ('train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########################################################################################\n",
      "ExtraTreesClassifier\n",
      "########################################################################################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ihuangyiran/anaconda2/envs/data_mining/lib/python3.6/site-packages/sklearn/ensemble/forest.py:248: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "/Users/ihuangyiran/anaconda2/envs/data_mining/lib/python3.6/site-packages/sklearn/ensemble/forest.py:248: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "/Users/ihuangyiran/anaconda2/envs/data_mining/lib/python3.6/site-packages/sklearn/ensemble/forest.py:248: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "/Users/ihuangyiran/anaconda2/envs/data_mining/lib/python3.6/site-packages/sklearn/ensemble/forest.py:248: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "/Users/ihuangyiran/anaconda2/envs/data_mining/lib/python3.6/site-packages/sklearn/ensemble/forest.py:248: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "/Users/ihuangyiran/anaconda2/envs/data_mining/lib/python3.6/site-packages/sklearn/ensemble/forest.py:248: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "/Users/ihuangyiran/anaconda2/envs/data_mining/lib/python3.6/site-packages/sklearn/ensemble/forest.py:248: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "/Users/ihuangyiran/anaconda2/envs/data_mining/lib/python3.6/site-packages/sklearn/ensemble/forest.py:248: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "/Users/ihuangyiran/anaconda2/envs/data_mining/lib/python3.6/site-packages/sklearn/ensemble/forest.py:248: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "/Users/ihuangyiran/anaconda2/envs/data_mining/lib/python3.6/site-packages/sklearn/ensemble/forest.py:248: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "/Users/ihuangyiran/anaconda2/envs/data_mining/lib/python3.6/site-packages/sklearn/utils/deprecation.py:125: FutureWarning: You are accessing a training score ('train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "/Users/ihuangyiran/anaconda2/envs/data_mining/lib/python3.6/site-packages/sklearn/ensemble/forest.py:248: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########################################################################################\n",
      "ExtraTreeClassifier\n",
      "########################################################################################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ihuangyiran/anaconda2/envs/data_mining/lib/python3.6/site-packages/sklearn/utils/deprecation.py:125: FutureWarning: You are accessing a training score ('train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########################################################################################\n",
      "XGBClassifier\n",
      "########################################################################################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ihuangyiran/anaconda2/envs/data_mining/lib/python3.6/site-packages/sklearn/utils/deprecation.py:125: FutureWarning: You are accessing a training score ('train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########################################################################################\n",
      "PassiveAggressiveClassifier\n",
      "########################################################################################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ihuangyiran/anaconda2/envs/data_mining/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:144: FutureWarning: max_iter and tol parameters have been added in PassiveAggressiveClassifier in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  FutureWarning)\n",
      "/Users/ihuangyiran/anaconda2/envs/data_mining/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:144: FutureWarning: max_iter and tol parameters have been added in PassiveAggressiveClassifier in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  FutureWarning)\n",
      "/Users/ihuangyiran/anaconda2/envs/data_mining/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:144: FutureWarning: max_iter and tol parameters have been added in PassiveAggressiveClassifier in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  FutureWarning)\n",
      "/Users/ihuangyiran/anaconda2/envs/data_mining/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:144: FutureWarning: max_iter and tol parameters have been added in PassiveAggressiveClassifier in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  FutureWarning)\n",
      "/Users/ihuangyiran/anaconda2/envs/data_mining/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:144: FutureWarning: max_iter and tol parameters have been added in PassiveAggressiveClassifier in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  FutureWarning)\n",
      "/Users/ihuangyiran/anaconda2/envs/data_mining/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:144: FutureWarning: max_iter and tol parameters have been added in PassiveAggressiveClassifier in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  FutureWarning)\n",
      "/Users/ihuangyiran/anaconda2/envs/data_mining/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:144: FutureWarning: max_iter and tol parameters have been added in PassiveAggressiveClassifier in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  FutureWarning)\n",
      "/Users/ihuangyiran/anaconda2/envs/data_mining/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:144: FutureWarning: max_iter and tol parameters have been added in PassiveAggressiveClassifier in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  FutureWarning)\n",
      "/Users/ihuangyiran/anaconda2/envs/data_mining/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:144: FutureWarning: max_iter and tol parameters have been added in PassiveAggressiveClassifier in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  FutureWarning)\n",
      "/Users/ihuangyiran/anaconda2/envs/data_mining/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:144: FutureWarning: max_iter and tol parameters have been added in PassiveAggressiveClassifier in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  FutureWarning)\n",
      "/Users/ihuangyiran/anaconda2/envs/data_mining/lib/python3.6/site-packages/sklearn/utils/deprecation.py:125: FutureWarning: You are accessing a training score ('train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "/Users/ihuangyiran/anaconda2/envs/data_mining/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:144: FutureWarning: max_iter and tol parameters have been added in PassiveAggressiveClassifier in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########################################################################################\n",
      "LogisticRegressionCV\n",
      "########################################################################################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ihuangyiran/anaconda2/envs/data_mining/lib/python3.6/site-packages/sklearn/model_selection/_split.py:1943: FutureWarning: You should specify a value for 'cv' instead of relying on the default value. The default value will change from 3 to 5 in version 0.22.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n",
      "/Users/ihuangyiran/anaconda2/envs/data_mining/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:757: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "/Users/ihuangyiran/anaconda2/envs/data_mining/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:757: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "/Users/ihuangyiran/anaconda2/envs/data_mining/lib/python3.6/site-packages/sklearn/model_selection/_split.py:1943: FutureWarning: You should specify a value for 'cv' instead of relying on the default value. The default value will change from 3 to 5 in version 0.22.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n",
      "/Users/ihuangyiran/anaconda2/envs/data_mining/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:757: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "/Users/ihuangyiran/anaconda2/envs/data_mining/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:757: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "/Users/ihuangyiran/anaconda2/envs/data_mining/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:757: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "/Users/ihuangyiran/anaconda2/envs/data_mining/lib/python3.6/site-packages/sklearn/model_selection/_split.py:1943: FutureWarning: You should specify a value for 'cv' instead of relying on the default value. The default value will change from 3 to 5 in version 0.22.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n",
      "/Users/ihuangyiran/anaconda2/envs/data_mining/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:757: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "/Users/ihuangyiran/anaconda2/envs/data_mining/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:757: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "/Users/ihuangyiran/anaconda2/envs/data_mining/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:757: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "/Users/ihuangyiran/anaconda2/envs/data_mining/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:757: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "/Users/ihuangyiran/anaconda2/envs/data_mining/lib/python3.6/site-packages/sklearn/model_selection/_split.py:1943: FutureWarning: You should specify a value for 'cv' instead of relying on the default value. The default value will change from 3 to 5 in version 0.22.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n",
      "/Users/ihuangyiran/anaconda2/envs/data_mining/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:757: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "/Users/ihuangyiran/anaconda2/envs/data_mining/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:757: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "/Users/ihuangyiran/anaconda2/envs/data_mining/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:757: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "/Users/ihuangyiran/anaconda2/envs/data_mining/lib/python3.6/site-packages/sklearn/model_selection/_split.py:1943: FutureWarning: You should specify a value for 'cv' instead of relying on the default value. The default value will change from 3 to 5 in version 0.22.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n",
      "/Users/ihuangyiran/anaconda2/envs/data_mining/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:757: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "/Users/ihuangyiran/anaconda2/envs/data_mining/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:757: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "/Users/ihuangyiran/anaconda2/envs/data_mining/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:757: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "/Users/ihuangyiran/anaconda2/envs/data_mining/lib/python3.6/site-packages/sklearn/model_selection/_split.py:1943: FutureWarning: You should specify a value for 'cv' instead of relying on the default value. The default value will change from 3 to 5 in version 0.22.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n",
      "/Users/ihuangyiran/anaconda2/envs/data_mining/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:757: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "/Users/ihuangyiran/anaconda2/envs/data_mining/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:757: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "/Users/ihuangyiran/anaconda2/envs/data_mining/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:757: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "/Users/ihuangyiran/anaconda2/envs/data_mining/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:757: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "/Users/ihuangyiran/anaconda2/envs/data_mining/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:757: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "/Users/ihuangyiran/anaconda2/envs/data_mining/lib/python3.6/site-packages/sklearn/model_selection/_split.py:1943: FutureWarning: You should specify a value for 'cv' instead of relying on the default value. The default value will change from 3 to 5 in version 0.22.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n",
      "/Users/ihuangyiran/anaconda2/envs/data_mining/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:757: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "/Users/ihuangyiran/anaconda2/envs/data_mining/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:757: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "/Users/ihuangyiran/anaconda2/envs/data_mining/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:757: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "/Users/ihuangyiran/anaconda2/envs/data_mining/lib/python3.6/site-packages/sklearn/model_selection/_split.py:1943: FutureWarning: You should specify a value for 'cv' instead of relying on the default value. The default value will change from 3 to 5 in version 0.22.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n",
      "/Users/ihuangyiran/anaconda2/envs/data_mining/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:757: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "/Users/ihuangyiran/anaconda2/envs/data_mining/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:757: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "/Users/ihuangyiran/anaconda2/envs/data_mining/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:757: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "/Users/ihuangyiran/anaconda2/envs/data_mining/lib/python3.6/site-packages/sklearn/model_selection/_split.py:1943: FutureWarning: You should specify a value for 'cv' instead of relying on the default value. The default value will change from 3 to 5 in version 0.22.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n",
      "/Users/ihuangyiran/anaconda2/envs/data_mining/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:757: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "/Users/ihuangyiran/anaconda2/envs/data_mining/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:757: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "/Users/ihuangyiran/anaconda2/envs/data_mining/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:757: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "/Users/ihuangyiran/anaconda2/envs/data_mining/lib/python3.6/site-packages/sklearn/model_selection/_split.py:1943: FutureWarning: You should specify a value for 'cv' instead of relying on the default value. The default value will change from 3 to 5 in version 0.22.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n",
      "/Users/ihuangyiran/anaconda2/envs/data_mining/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:757: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "/Users/ihuangyiran/anaconda2/envs/data_mining/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:757: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "/Users/ihuangyiran/anaconda2/envs/data_mining/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:757: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "/Users/ihuangyiran/anaconda2/envs/data_mining/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:757: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "/Users/ihuangyiran/anaconda2/envs/data_mining/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:757: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "/Users/ihuangyiran/anaconda2/envs/data_mining/lib/python3.6/site-packages/sklearn/utils/deprecation.py:125: FutureWarning: You are accessing a training score ('train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "/Users/ihuangyiran/anaconda2/envs/data_mining/lib/python3.6/site-packages/sklearn/model_selection/_split.py:1943: FutureWarning: You should specify a value for 'cv' instead of relying on the default value. The default value will change from 3 to 5 in version 0.22.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n",
      "/Users/ihuangyiran/anaconda2/envs/data_mining/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:757: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "/Users/ihuangyiran/anaconda2/envs/data_mining/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:757: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "/Users/ihuangyiran/anaconda2/envs/data_mining/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:757: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "/Users/ihuangyiran/anaconda2/envs/data_mining/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:757: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "/Users/ihuangyiran/anaconda2/envs/data_mining/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:757: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "/Users/ihuangyiran/anaconda2/envs/data_mining/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:757: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########################################################################################\n",
      "GaussianNB\n",
      "########################################################################################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ihuangyiran/anaconda2/envs/data_mining/lib/python3.6/site-packages/sklearn/utils/deprecation.py:125: FutureWarning: You are accessing a training score ('train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########################################################################################\n",
      "BaggingClassifier\n",
      "########################################################################################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ihuangyiran/anaconda2/envs/data_mining/lib/python3.6/site-packages/sklearn/utils/deprecation.py:125: FutureWarning: You are accessing a training score ('train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########################################################################################\n",
      "DecisionTreeClassifier\n",
      "########################################################################################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ihuangyiran/anaconda2/envs/data_mining/lib/python3.6/site-packages/sklearn/utils/deprecation.py:125: FutureWarning: You are accessing a training score ('train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########################################################################################\n",
      "LinearSVC\n",
      "########################################################################################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ihuangyiran/anaconda2/envs/data_mining/lib/python3.6/site-packages/sklearn/svm/base.py:922: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/ihuangyiran/anaconda2/envs/data_mining/lib/python3.6/site-packages/sklearn/svm/base.py:922: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/ihuangyiran/anaconda2/envs/data_mining/lib/python3.6/site-packages/sklearn/svm/base.py:922: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/ihuangyiran/anaconda2/envs/data_mining/lib/python3.6/site-packages/sklearn/svm/base.py:922: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/ihuangyiran/anaconda2/envs/data_mining/lib/python3.6/site-packages/sklearn/svm/base.py:922: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/ihuangyiran/anaconda2/envs/data_mining/lib/python3.6/site-packages/sklearn/svm/base.py:922: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/ihuangyiran/anaconda2/envs/data_mining/lib/python3.6/site-packages/sklearn/svm/base.py:922: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/ihuangyiran/anaconda2/envs/data_mining/lib/python3.6/site-packages/sklearn/svm/base.py:922: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/ihuangyiran/anaconda2/envs/data_mining/lib/python3.6/site-packages/sklearn/svm/base.py:922: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/ihuangyiran/anaconda2/envs/data_mining/lib/python3.6/site-packages/sklearn/svm/base.py:922: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/ihuangyiran/anaconda2/envs/data_mining/lib/python3.6/site-packages/sklearn/utils/deprecation.py:125: FutureWarning: You are accessing a training score ('train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "/Users/ihuangyiran/anaconda2/envs/data_mining/lib/python3.6/site-packages/sklearn/svm/base.py:922: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########################################################################################\n",
      "QuadraticDiscriminantAnalysis\n",
      "########################################################################################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ihuangyiran/anaconda2/envs/data_mining/lib/python3.6/site-packages/sklearn/utils/deprecation.py:125: FutureWarning: You are accessing a training score ('train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########################################################################################\n",
      "GradientBoostingClassifier\n",
      "########################################################################################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ihuangyiran/anaconda2/envs/data_mining/lib/python3.6/site-packages/sklearn/utils/deprecation.py:125: FutureWarning: You are accessing a training score ('train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n"
     ]
    }
   ],
   "source": [
    "# index through MLA and save performance to table\n",
    "MLA_predict = {}\n",
    "row_index = 0\n",
    "for alg in MLA:\n",
    "    # set name and parameters\n",
    "    MLA_name = alg.__class__.__name__\n",
    "    print(\"#\"*88)\n",
    "    print(MLA_name)\n",
    "    print(\"#\"*88)\n",
    "    MLA_compare.loc[row_index, 'MLA Name'] = MLA_name\n",
    "    MLA_compare.loc[row_index, 'MLA Parameters'] = str(alg.get_params())\n",
    "    # score model with cross validation\n",
    "    cv_results = model_selection.cross_validate(alg, X_train, Y_train, cv = cv_split)\n",
    "    MLA_compare.loc[row_index, 'MLA Time'] = cv_results['fit_time'].mean()\n",
    "    MLA_compare.loc[row_index, 'MLA Train Accuracy Mean'] = cv_results['train_score'].mean()\n",
    "    MLA_compare.loc[row_index, 'MLA Test Accuracy Mean'] = cv_results['test_score'].mean()\n",
    "    MLA_compare.loc[row_index, 'MLA Test Accuracy 3*STD'] = cv_results['test_score'].std()*3\n",
    "    # save MLA predictions\n",
    "    alg.fit(X_train, Y_train)\n",
    "    MLA_predict[MLA_name] = alg.predict(X_train)\n",
    "    row_index += 1\n",
    "\n",
    "    # sort and print\n",
    "    MLA_compare.sort_values(by = ['MLA Test Accuracy Mean'], ascending = False, inplace = True)\n",
    "    MLA_compare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MLA Name</th>\n",
       "      <th>MLA Parameters</th>\n",
       "      <th>MLA Train Accuracy Mean</th>\n",
       "      <th>MLA Test Accuracy Mean</th>\n",
       "      <th>MLA Test Accuracy 3*STD</th>\n",
       "      <th>MLA Time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>BaggingClassifier</td>\n",
       "      <td>{'base_estimator': None, 'bootstrap': True, 'b...</td>\n",
       "      <td>0.985803</td>\n",
       "      <td>0.726996</td>\n",
       "      <td>0.00517335</td>\n",
       "      <td>1.99144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>RandomForestClassifier</td>\n",
       "      <td>{'bootstrap': True, 'class_weight': None, 'cri...</td>\n",
       "      <td>0.986934</td>\n",
       "      <td>0.720808</td>\n",
       "      <td>0.00568949</td>\n",
       "      <td>0.575658</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>GradientBoostingClassifier</td>\n",
       "      <td>{'criterion': 'friedman_mse', 'init': None, 'l...</td>\n",
       "      <td>0.718405</td>\n",
       "      <td>0.713044</td>\n",
       "      <td>0.00746942</td>\n",
       "      <td>4.06738</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>{'base_score': 0.5, 'booster': 'gbtree', 'cols...</td>\n",
       "      <td>0.717246</td>\n",
       "      <td>0.711754</td>\n",
       "      <td>0.00675221</td>\n",
       "      <td>1.88304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>ExtraTreesClassifier</td>\n",
       "      <td>{'bootstrap': False, 'class_weight': None, 'cr...</td>\n",
       "      <td>0.999094</td>\n",
       "      <td>0.699386</td>\n",
       "      <td>0.00794162</td>\n",
       "      <td>0.563557</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>{'algorithm': 'SAMME.R', 'base_estimator': Non...</td>\n",
       "      <td>0.696367</td>\n",
       "      <td>0.693473</td>\n",
       "      <td>0.00456799</td>\n",
       "      <td>1.38899</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>DecisionTreeClassifier</td>\n",
       "      <td>{'class_weight': None, 'criterion': 'gini', 'm...</td>\n",
       "      <td>0.999094</td>\n",
       "      <td>0.685038</td>\n",
       "      <td>0.00671227</td>\n",
       "      <td>0.354512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>ExtraTreeClassifier</td>\n",
       "      <td>{'class_weight': None, 'criterion': 'gini', 'm...</td>\n",
       "      <td>0.999094</td>\n",
       "      <td>0.639881</td>\n",
       "      <td>0.0178932</td>\n",
       "      <td>0.0970144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>LogisticRegressionCV</td>\n",
       "      <td>{'Cs': 10, 'class_weight': None, 'cv': 'warn',...</td>\n",
       "      <td>0.632421</td>\n",
       "      <td>0.63277</td>\n",
       "      <td>0.00682149</td>\n",
       "      <td>1.10849</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>QuadraticDiscriminantAnalysis</td>\n",
       "      <td>{'priors': None, 'reg_param': 0.0, 'store_cova...</td>\n",
       "      <td>0.626899</td>\n",
       "      <td>0.624721</td>\n",
       "      <td>0.0121201</td>\n",
       "      <td>0.0843212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>KNeighborsClassifier</td>\n",
       "      <td>{'algorithm': 'auto', 'leaf_size': 30, 'metric...</td>\n",
       "      <td>0.75474</td>\n",
       "      <td>0.62345</td>\n",
       "      <td>0.00967003</td>\n",
       "      <td>0.120837</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>{'priors': None, 'var_smoothing': 1e-09}</td>\n",
       "      <td>0.614443</td>\n",
       "      <td>0.612846</td>\n",
       "      <td>0.00823888</td>\n",
       "      <td>0.0715528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>BernoulliNB</td>\n",
       "      <td>{'alpha': 1.0, 'binarize': 0.0, 'class_prior':...</td>\n",
       "      <td>0.533539</td>\n",
       "      <td>0.530084</td>\n",
       "      <td>0.012945</td>\n",
       "      <td>0.0627701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>LinearSVC</td>\n",
       "      <td>{'C': 1.0, 'class_weight': None, 'dual': True,...</td>\n",
       "      <td>0.529602</td>\n",
       "      <td>0.528407</td>\n",
       "      <td>0.125547</td>\n",
       "      <td>7.16617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Perceptron</td>\n",
       "      <td>{'alpha': 0.0001, 'class_weight': None, 'early...</td>\n",
       "      <td>0.523971</td>\n",
       "      <td>0.527562</td>\n",
       "      <td>0.0929827</td>\n",
       "      <td>0.132129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>SGDClassifier</td>\n",
       "      <td>{'alpha': 0.0001, 'average': False, 'class_wei...</td>\n",
       "      <td>0.518437</td>\n",
       "      <td>0.518465</td>\n",
       "      <td>0.0994891</td>\n",
       "      <td>0.0861691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>PassiveAggressiveClassifier</td>\n",
       "      <td>{'C': 1.0, 'average': False, 'class_weight': N...</td>\n",
       "      <td>0.516705</td>\n",
       "      <td>0.51545</td>\n",
       "      <td>0.110644</td>\n",
       "      <td>0.10439</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         MLA Name  \\\n",
       "3               BaggingClassifier   \n",
       "10         RandomForestClassifier   \n",
       "9      GradientBoostingClassifier   \n",
       "2                   XGBClassifier   \n",
       "6            ExtraTreesClassifier   \n",
       "1              AdaBoostClassifier   \n",
       "12         DecisionTreeClassifier   \n",
       "13            ExtraTreeClassifier   \n",
       "14           LogisticRegressionCV   \n",
       "15  QuadraticDiscriminantAnalysis   \n",
       "8            KNeighborsClassifier   \n",
       "5                      GaussianNB   \n",
       "7                     BernoulliNB   \n",
       "11                      LinearSVC   \n",
       "0                      Perceptron   \n",
       "4                   SGDClassifier   \n",
       "16    PassiveAggressiveClassifier   \n",
       "\n",
       "                                       MLA Parameters MLA Train Accuracy Mean  \\\n",
       "3   {'base_estimator': None, 'bootstrap': True, 'b...                0.985803   \n",
       "10  {'bootstrap': True, 'class_weight': None, 'cri...                0.986934   \n",
       "9   {'criterion': 'friedman_mse', 'init': None, 'l...                0.718405   \n",
       "2   {'base_score': 0.5, 'booster': 'gbtree', 'cols...                0.717246   \n",
       "6   {'bootstrap': False, 'class_weight': None, 'cr...                0.999094   \n",
       "1   {'algorithm': 'SAMME.R', 'base_estimator': Non...                0.696367   \n",
       "12  {'class_weight': None, 'criterion': 'gini', 'm...                0.999094   \n",
       "13  {'class_weight': None, 'criterion': 'gini', 'm...                0.999094   \n",
       "14  {'Cs': 10, 'class_weight': None, 'cv': 'warn',...                0.632421   \n",
       "15  {'priors': None, 'reg_param': 0.0, 'store_cova...                0.626899   \n",
       "8   {'algorithm': 'auto', 'leaf_size': 30, 'metric...                 0.75474   \n",
       "5            {'priors': None, 'var_smoothing': 1e-09}                0.614443   \n",
       "7   {'alpha': 1.0, 'binarize': 0.0, 'class_prior':...                0.533539   \n",
       "11  {'C': 1.0, 'class_weight': None, 'dual': True,...                0.529602   \n",
       "0   {'alpha': 0.0001, 'class_weight': None, 'early...                0.523971   \n",
       "4   {'alpha': 0.0001, 'average': False, 'class_wei...                0.518437   \n",
       "16  {'C': 1.0, 'average': False, 'class_weight': N...                0.516705   \n",
       "\n",
       "   MLA Test Accuracy Mean MLA Test Accuracy 3*STD   MLA Time  \n",
       "3                0.726996              0.00517335    1.99144  \n",
       "10               0.720808              0.00568949   0.575658  \n",
       "9                0.713044              0.00746942    4.06738  \n",
       "2                0.711754              0.00675221    1.88304  \n",
       "6                0.699386              0.00794162   0.563557  \n",
       "1                0.693473              0.00456799    1.38899  \n",
       "12               0.685038              0.00671227   0.354512  \n",
       "13               0.639881               0.0178932  0.0970144  \n",
       "14                0.63277              0.00682149    1.10849  \n",
       "15               0.624721               0.0121201  0.0843212  \n",
       "8                 0.62345              0.00967003   0.120837  \n",
       "5                0.612846              0.00823888  0.0715528  \n",
       "7                0.530084                0.012945  0.0627701  \n",
       "11               0.528407                0.125547    7.16617  \n",
       "0                0.527562               0.0929827   0.132129  \n",
       "4                0.518465               0.0994891  0.0861691  \n",
       "16                0.51545                0.110644    0.10439  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MLA_compare.sort_values(by = ['MLA Test Accuracy Mean'], ascending = False, inplace = True)\n",
    "MLA_compare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x1a3397bda0>"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1296x1296 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "MLA_predict_pd = pd.DataFrame.from_dict(MLA_predict)\n",
    "f, ax = plt.subplots(figsize = (18,18))\n",
    "sns.heatmap(MLA_predict_pd.astype(float).corr(), \n",
    "            linewidths = 0.1, vmax = 1.0, ax = ax,\n",
    "            square = True, linecolor = 'white', \n",
    "            annot = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#  除此之外，还可以弄个rnn模型，从而，不单单考虑当前的transaction，而是考虑所有的，来得到最终结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vote_est = [\n",
    "    ('ada', ensemble.AdaBoostClassifier()),\n",
    "    ('bc', ensemble.BaggingClassifier()),\n",
    "    ('etc', ensemble.ExtraTreesClassifier()),\n",
    "    ('gnb', naive_bayes.GaussianNB()),\n",
    "    ('knn', neighbors.KNeighborsClassifier()),\n",
    "    ('rf', ensemble.RandomForestClassifier()),\n",
    "    ('bnb', naive_bayes.BernoulliNB()),\n",
    "    ('sgd', linear_model.SGDClassifier()),\n",
    "    ('pec', linear_model.Perceptron()),\n",
    "    ('pac', linear_model.PassiveAggressiveClassifier()),\n",
    "    ('lsvc', svm.LinearSVC())\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ihuangyiran/anaconda2/envs/data_mining/lib/python3.6/site-packages/sklearn/ensemble/forest.py:248: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "/Users/ihuangyiran/anaconda2/envs/data_mining/lib/python3.6/site-packages/sklearn/ensemble/forest.py:248: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "/Users/ihuangyiran/anaconda2/envs/data_mining/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:144: FutureWarning: max_iter and tol parameters have been added in SGDClassifier in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  FutureWarning)\n",
      "/Users/ihuangyiran/anaconda2/envs/data_mining/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:144: FutureWarning: max_iter and tol parameters have been added in Perceptron in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  FutureWarning)\n",
      "/Users/ihuangyiran/anaconda2/envs/data_mining/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:144: FutureWarning: max_iter and tol parameters have been added in PassiveAggressiveClassifier in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  FutureWarning)\n",
      "/Users/ihuangyiran/anaconda2/envs/data_mining/lib/python3.6/site-packages/sklearn/svm/base.py:922: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "probability estimates are not available for loss='hinge'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-200-90b886b772f2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mvote_soft\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mensemble\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mVotingClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimators\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvote_est\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvoting\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'soft'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mvote_soft_cv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_selection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcross_validate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvote_soft\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0mcv_split\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mvote_soft\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Soft voting Training w/bin score mean: {:.2f}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvote_soft_cv\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'train_score'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Soft voting Test w/bin score mean: {:.2f}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvote_soft_cv\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'test_score'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/data_mining/lib/python3.6/site-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36mcross_validate\u001b[0;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, return_train_score, return_estimator, error_score)\u001b[0m\n\u001b[1;32m    238\u001b[0m             \u001b[0mreturn_times\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_estimator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_estimator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    239\u001b[0m             error_score=error_score)\n\u001b[0;32m--> 240\u001b[0;31m         for train, test in cv.split(X, y, groups))\n\u001b[0m\u001b[1;32m    241\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    242\u001b[0m     \u001b[0mzipped_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/data_mining/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m    981\u001b[0m             \u001b[0;31m# remaining jobs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    982\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 983\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    984\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_iterator\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    985\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/data_mining/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    823\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    824\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 825\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    826\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/data_mining/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    780\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    781\u001b[0m             \u001b[0mjob_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 782\u001b[0;31m             \u001b[0mjob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    783\u001b[0m             \u001b[0;31m# A job can complete so quickly than its callback is\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    784\u001b[0m             \u001b[0;31m# called before we get here, causing self._jobs to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/data_mining/lib/python3.6/site-packages/sklearn/externals/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[0;34m(self, func, callback)\u001b[0m\n\u001b[1;32m    180\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m         \u001b[0;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 182\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    183\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/data_mining/lib/python3.6/site-packages/sklearn/externals/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    543\u001b[0m         \u001b[0;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    544\u001b[0m         \u001b[0;31m# arguments in memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 545\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    546\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    547\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/data_mining/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    259\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    260\u001b[0m             return [func(*args, **kwargs)\n\u001b[0;32m--> 261\u001b[0;31m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[1;32m    262\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    263\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/data_mining/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    259\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    260\u001b[0m             return [func(*args, **kwargs)\n\u001b[0;32m--> 261\u001b[0;31m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[1;32m    262\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    263\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/data_mining/lib/python3.6/site-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36m_fit_and_score\u001b[0;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, error_score)\u001b[0m\n\u001b[1;32m    566\u001b[0m         \u001b[0mfit_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    567\u001b[0m         \u001b[0;31m# _score will return dict if is_multimetric is True\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 568\u001b[0;31m         \u001b[0mtest_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscorer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_multimetric\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    569\u001b[0m         \u001b[0mscore_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart_time\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mfit_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    570\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mreturn_train_score\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/data_mining/lib/python3.6/site-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36m_score\u001b[0;34m(estimator, X_test, y_test, scorer, is_multimetric)\u001b[0m\n\u001b[1;32m    603\u001b[0m     \"\"\"\n\u001b[1;32m    604\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mis_multimetric\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 605\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_multimetric_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscorer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    606\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    607\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0my_test\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/data_mining/lib/python3.6/site-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36m_multimetric_score\u001b[0;34m(estimator, X_test, y_test, scorers)\u001b[0m\n\u001b[1;32m    633\u001b[0m             \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscorer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    634\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 635\u001b[0;31m             \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscorer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    636\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    637\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'item'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/data_mining/lib/python3.6/site-packages/sklearn/metrics/scorer.py\u001b[0m in \u001b[0;36m_passthrough_scorer\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m    226\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_passthrough_scorer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    227\u001b[0m     \u001b[0;34m\"\"\"Function that wraps estimator.score\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 228\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    229\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    230\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/data_mining/lib/python3.6/site-packages/sklearn/base.py\u001b[0m in \u001b[0;36mscore\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    286\u001b[0m         \"\"\"\n\u001b[1;32m    287\u001b[0m         \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 288\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    289\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    290\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/data_mining/lib/python3.6/site-packages/sklearn/ensemble/voting_classifier.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    231\u001b[0m         \u001b[0mcheck_is_fitted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'estimators_'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvoting\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'soft'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 233\u001b[0;31m             \u001b[0mmaj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    234\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    235\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# 'hard' voting\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/data_mining/lib/python3.6/site-packages/sklearn/ensemble/voting_classifier.py\u001b[0m in \u001b[0;36m_predict_proba\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    254\u001b[0m                                  \" voting=%r\" % self.voting)\n\u001b[1;32m    255\u001b[0m         \u001b[0mcheck_is_fitted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'estimators_'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 256\u001b[0;31m         avg = np.average(self._collect_probas(X), axis=0,\n\u001b[0m\u001b[1;32m    257\u001b[0m                          weights=self._weights_not_none)\n\u001b[1;32m    258\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mavg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/data_mining/lib/python3.6/site-packages/sklearn/ensemble/voting_classifier.py\u001b[0m in \u001b[0;36m_collect_probas\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    246\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_collect_probas\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m         \u001b[0;34m\"\"\"Collect results from clf.predict calls. \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 248\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mclf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mestimators_\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    249\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_predict_proba\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/data_mining/lib/python3.6/site-packages/sklearn/ensemble/voting_classifier.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    246\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_collect_probas\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m         \u001b[0;34m\"\"\"Collect results from clf.predict calls. \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 248\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mclf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mestimators_\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    249\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_predict_proba\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/data_mining/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py\u001b[0m in \u001b[0;36mpredict_proba\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1001\u001b[0m         \u001b[0mhttp\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m//\u001b[0m\u001b[0mjmlr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcsail\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0medu\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mpapers\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mvolume2\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mzhang02c\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mzhang02c\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpdf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1002\u001b[0m         \"\"\"\n\u001b[0;32m-> 1003\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_proba\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1004\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_predict_proba\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1005\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/data_mining/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py\u001b[0m in \u001b[0;36m_check_proba\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    963\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"log\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"modified_huber\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    964\u001b[0m             raise AttributeError(\"probability estimates are not available for\"\n\u001b[0;32m--> 965\u001b[0;31m                                  \" loss=%r\" % self.loss)\n\u001b[0m\u001b[1;32m    966\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    967\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: probability estimates are not available for loss='hinge'"
     ]
    }
   ],
   "source": [
    "vote_soft = ensemble.VotingClassifier(estimators = vote_est, voting = 'soft')\n",
    "vote_soft_cv = model_selection.cross_validate(vote_soft, X_train, Y_train, cv =cv_split)\n",
    "vote_soft.fit(X_train, Y_train)\n",
    "print(\"Soft voting Training w/bin score mean: {:.2f}\".format(vote_soft_cv['train_score'].mean()*100))\n",
    "print(\"Soft voting Test w/bin score mean: {:.2f}\".format(vote_soft_cv['test_score'].mean()*100))\n",
    "print(\"Soft voting Test w/bin score 3*std: ±{:.2f}\".format(vote_soft_cv['test_score'].std()*100*3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ihuangyiran/anaconda2/envs/data_mining/lib/python3.6/site-packages/sklearn/ensemble/forest.py:248: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "/Users/ihuangyiran/anaconda2/envs/data_mining/lib/python3.6/site-packages/sklearn/ensemble/forest.py:248: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "/Users/ihuangyiran/anaconda2/envs/data_mining/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:144: FutureWarning: max_iter and tol parameters have been added in SGDClassifier in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  FutureWarning)\n",
      "/Users/ihuangyiran/anaconda2/envs/data_mining/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:144: FutureWarning: max_iter and tol parameters have been added in Perceptron in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  FutureWarning)\n",
      "/Users/ihuangyiran/anaconda2/envs/data_mining/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:144: FutureWarning: max_iter and tol parameters have been added in PassiveAggressiveClassifier in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  FutureWarning)\n",
      "/Users/ihuangyiran/anaconda2/envs/data_mining/lib/python3.6/site-packages/sklearn/svm/base.py:922: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/ihuangyiran/anaconda2/envs/data_mining/lib/python3.6/site-packages/sklearn/ensemble/forest.py:248: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "/Users/ihuangyiran/anaconda2/envs/data_mining/lib/python3.6/site-packages/sklearn/ensemble/forest.py:248: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "/Users/ihuangyiran/anaconda2/envs/data_mining/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:144: FutureWarning: max_iter and tol parameters have been added in SGDClassifier in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  FutureWarning)\n",
      "/Users/ihuangyiran/anaconda2/envs/data_mining/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:144: FutureWarning: max_iter and tol parameters have been added in Perceptron in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  FutureWarning)\n",
      "/Users/ihuangyiran/anaconda2/envs/data_mining/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:144: FutureWarning: max_iter and tol parameters have been added in PassiveAggressiveClassifier in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  FutureWarning)\n",
      "/Users/ihuangyiran/anaconda2/envs/data_mining/lib/python3.6/site-packages/sklearn/svm/base.py:922: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/ihuangyiran/anaconda2/envs/data_mining/lib/python3.6/site-packages/sklearn/ensemble/forest.py:248: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "/Users/ihuangyiran/anaconda2/envs/data_mining/lib/python3.6/site-packages/sklearn/ensemble/forest.py:248: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "/Users/ihuangyiran/anaconda2/envs/data_mining/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:144: FutureWarning: max_iter and tol parameters have been added in SGDClassifier in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  FutureWarning)\n",
      "/Users/ihuangyiran/anaconda2/envs/data_mining/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:144: FutureWarning: max_iter and tol parameters have been added in Perceptron in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  FutureWarning)\n",
      "/Users/ihuangyiran/anaconda2/envs/data_mining/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:144: FutureWarning: max_iter and tol parameters have been added in PassiveAggressiveClassifier in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  FutureWarning)\n",
      "/Users/ihuangyiran/anaconda2/envs/data_mining/lib/python3.6/site-packages/sklearn/svm/base.py:922: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/ihuangyiran/anaconda2/envs/data_mining/lib/python3.6/site-packages/sklearn/ensemble/forest.py:248: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "/Users/ihuangyiran/anaconda2/envs/data_mining/lib/python3.6/site-packages/sklearn/ensemble/forest.py:248: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "/Users/ihuangyiran/anaconda2/envs/data_mining/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:144: FutureWarning: max_iter and tol parameters have been added in SGDClassifier in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  FutureWarning)\n",
      "/Users/ihuangyiran/anaconda2/envs/data_mining/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:144: FutureWarning: max_iter and tol parameters have been added in Perceptron in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  FutureWarning)\n",
      "/Users/ihuangyiran/anaconda2/envs/data_mining/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:144: FutureWarning: max_iter and tol parameters have been added in PassiveAggressiveClassifier in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  FutureWarning)\n",
      "/Users/ihuangyiran/anaconda2/envs/data_mining/lib/python3.6/site-packages/sklearn/svm/base.py:922: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/ihuangyiran/anaconda2/envs/data_mining/lib/python3.6/site-packages/sklearn/ensemble/forest.py:248: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "/Users/ihuangyiran/anaconda2/envs/data_mining/lib/python3.6/site-packages/sklearn/ensemble/forest.py:248: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "/Users/ihuangyiran/anaconda2/envs/data_mining/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:144: FutureWarning: max_iter and tol parameters have been added in SGDClassifier in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  FutureWarning)\n",
      "/Users/ihuangyiran/anaconda2/envs/data_mining/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:144: FutureWarning: max_iter and tol parameters have been added in Perceptron in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  FutureWarning)\n",
      "/Users/ihuangyiran/anaconda2/envs/data_mining/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:144: FutureWarning: max_iter and tol parameters have been added in PassiveAggressiveClassifier in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  FutureWarning)\n",
      "/Users/ihuangyiran/anaconda2/envs/data_mining/lib/python3.6/site-packages/sklearn/svm/base.py:922: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/ihuangyiran/anaconda2/envs/data_mining/lib/python3.6/site-packages/sklearn/ensemble/forest.py:248: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "/Users/ihuangyiran/anaconda2/envs/data_mining/lib/python3.6/site-packages/sklearn/ensemble/forest.py:248: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "/Users/ihuangyiran/anaconda2/envs/data_mining/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:144: FutureWarning: max_iter and tol parameters have been added in SGDClassifier in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  FutureWarning)\n",
      "/Users/ihuangyiran/anaconda2/envs/data_mining/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:144: FutureWarning: max_iter and tol parameters have been added in Perceptron in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  FutureWarning)\n",
      "/Users/ihuangyiran/anaconda2/envs/data_mining/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:144: FutureWarning: max_iter and tol parameters have been added in PassiveAggressiveClassifier in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  FutureWarning)\n",
      "/Users/ihuangyiran/anaconda2/envs/data_mining/lib/python3.6/site-packages/sklearn/svm/base.py:922: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/ihuangyiran/anaconda2/envs/data_mining/lib/python3.6/site-packages/sklearn/ensemble/forest.py:248: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "/Users/ihuangyiran/anaconda2/envs/data_mining/lib/python3.6/site-packages/sklearn/ensemble/forest.py:248: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "/Users/ihuangyiran/anaconda2/envs/data_mining/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:144: FutureWarning: max_iter and tol parameters have been added in SGDClassifier in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  FutureWarning)\n",
      "/Users/ihuangyiran/anaconda2/envs/data_mining/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:144: FutureWarning: max_iter and tol parameters have been added in Perceptron in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  FutureWarning)\n",
      "/Users/ihuangyiran/anaconda2/envs/data_mining/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:144: FutureWarning: max_iter and tol parameters have been added in PassiveAggressiveClassifier in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  FutureWarning)\n",
      "/Users/ihuangyiran/anaconda2/envs/data_mining/lib/python3.6/site-packages/sklearn/svm/base.py:922: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/ihuangyiran/anaconda2/envs/data_mining/lib/python3.6/site-packages/sklearn/ensemble/forest.py:248: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "/Users/ihuangyiran/anaconda2/envs/data_mining/lib/python3.6/site-packages/sklearn/ensemble/forest.py:248: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "/Users/ihuangyiran/anaconda2/envs/data_mining/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:144: FutureWarning: max_iter and tol parameters have been added in SGDClassifier in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  FutureWarning)\n",
      "/Users/ihuangyiran/anaconda2/envs/data_mining/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:144: FutureWarning: max_iter and tol parameters have been added in Perceptron in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  FutureWarning)\n",
      "/Users/ihuangyiran/anaconda2/envs/data_mining/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:144: FutureWarning: max_iter and tol parameters have been added in PassiveAggressiveClassifier in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  FutureWarning)\n",
      "/Users/ihuangyiran/anaconda2/envs/data_mining/lib/python3.6/site-packages/sklearn/svm/base.py:922: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/ihuangyiran/anaconda2/envs/data_mining/lib/python3.6/site-packages/sklearn/ensemble/forest.py:248: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "/Users/ihuangyiran/anaconda2/envs/data_mining/lib/python3.6/site-packages/sklearn/ensemble/forest.py:248: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "/Users/ihuangyiran/anaconda2/envs/data_mining/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:144: FutureWarning: max_iter and tol parameters have been added in SGDClassifier in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  FutureWarning)\n",
      "/Users/ihuangyiran/anaconda2/envs/data_mining/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:144: FutureWarning: max_iter and tol parameters have been added in Perceptron in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  FutureWarning)\n",
      "/Users/ihuangyiran/anaconda2/envs/data_mining/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:144: FutureWarning: max_iter and tol parameters have been added in PassiveAggressiveClassifier in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  FutureWarning)\n",
      "/Users/ihuangyiran/anaconda2/envs/data_mining/lib/python3.6/site-packages/sklearn/svm/base.py:922: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/ihuangyiran/anaconda2/envs/data_mining/lib/python3.6/site-packages/sklearn/ensemble/forest.py:248: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "/Users/ihuangyiran/anaconda2/envs/data_mining/lib/python3.6/site-packages/sklearn/ensemble/forest.py:248: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "/Users/ihuangyiran/anaconda2/envs/data_mining/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:144: FutureWarning: max_iter and tol parameters have been added in SGDClassifier in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  FutureWarning)\n",
      "/Users/ihuangyiran/anaconda2/envs/data_mining/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:144: FutureWarning: max_iter and tol parameters have been added in Perceptron in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  FutureWarning)\n",
      "/Users/ihuangyiran/anaconda2/envs/data_mining/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:144: FutureWarning: max_iter and tol parameters have been added in PassiveAggressiveClassifier in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  FutureWarning)\n",
      "/Users/ihuangyiran/anaconda2/envs/data_mining/lib/python3.6/site-packages/sklearn/svm/base.py:922: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/ihuangyiran/anaconda2/envs/data_mining/lib/python3.6/site-packages/sklearn/ensemble/forest.py:248: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "/Users/ihuangyiran/anaconda2/envs/data_mining/lib/python3.6/site-packages/sklearn/ensemble/forest.py:248: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "/Users/ihuangyiran/anaconda2/envs/data_mining/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:144: FutureWarning: max_iter and tol parameters have been added in SGDClassifier in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  FutureWarning)\n",
      "/Users/ihuangyiran/anaconda2/envs/data_mining/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:144: FutureWarning: max_iter and tol parameters have been added in Perceptron in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  FutureWarning)\n",
      "/Users/ihuangyiran/anaconda2/envs/data_mining/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:144: FutureWarning: max_iter and tol parameters have been added in PassiveAggressiveClassifier in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hard voting Training w/bin score mean: 88.26\n",
      "Hard voting Test w/bin score mean: 65.32\n",
      "Hard voting Test w/bin score 3*std: ±5.48\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ihuangyiran/anaconda2/envs/data_mining/lib/python3.6/site-packages/sklearn/svm/base.py:922: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/ihuangyiran/anaconda2/envs/data_mining/lib/python3.6/site-packages/sklearn/utils/deprecation.py:125: FutureWarning: You are accessing a training score ('train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n"
     ]
    }
   ],
   "source": [
    "vote_hard = ensemble.VotingClassifier(estimators = vote_est, voting = 'hard')\n",
    "vote_hard_cv = model_selection.cross_validate(vote_hard, X_train, Y_train, cv = cv_split)\n",
    "vote_hard.fit(X_train, Y_train)\n",
    "print(\"Hard voting Training w/bin score mean: {:.2f}\".format(vote_hard_cv['train_score'].mean()*100))\n",
    "print(\"Hard voting Test w/bin score mean: {:.2f}\".format(vote_hard_cv['test_score'].mean()*100))\n",
    "print(\"Hard voting Test w/bin score 3*std: ±{:.2f}\".format(vote_hard_cv['test_score'].std()*100*3))\n",
    "print('-'*10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.637375\n"
     ]
    }
   ],
   "source": [
    "test_model(ensemble.BaggingClassifier())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.63425\n"
     ]
    }
   ],
   "source": [
    "finetune_model_ada = finetune_gridsearch('xgbc')\n",
    "test_model(ensemble.BaggingClassifier())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 68994 entries, 0 to 68993\n",
      "Data columns (total 12 columns):\n",
      "frequent_in_last_3_month                             68994 non-null float64\n",
      "frequent_in_last_4_month                             68994 non-null float64\n",
      "frequent_in_last_5_month                             68994 non-null float64\n",
      "mean_km_distance_in_group                            68994 non-null float64\n",
      "trend_of_frequent_in_the_past                        68994 non-null float64\n",
      "km_distance_from_last_ordinal_encode_after_mdlp      68994 non-null float64\n",
      "time_distance_from_last_ordinal_encode_after_mdlp    68994 non-null float64\n",
      "num_act_ordinal_encode_after_mdlp                    68994 non-null float64\n",
      "total AW_ordinal_encode_after_mdlp                   68994 non-null float64\n",
      "KM-Stand_ordinal_encode_after_mdlp                   68994 non-null float64\n",
      "Leistung (KW)_ordinal_encode_after_mdlp              68994 non-null float64\n",
      "num_teile_ordinal_encode_after_mdlp                  68994 non-null float64\n",
      "dtypes: float64(12)\n",
      "memory usage: 6.3 MB\n"
     ]
    }
   ],
   "source": [
    "X_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# test the result with test set\n",
    "def test_model(model):\n",
    "    #model = ensemble.BaggingClassifier()\n",
    "    model.fit(X_train, Y_train)\n",
    "    Y_predict = model.predict(X_test)\n",
    "    result = pd.concat([Y_test.reset_index().iloc[:,1:], pd.Series(Y_predict, name = 'predict')], axis = 1)\n",
    "    print(len(result[result['weather_come_in_n_month'] == result['predict']])/len(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CODE "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# MLP\n",
    "# to use the classification model, you should change the codes in following position:\n",
    "#   1. model = TS_mlp(12)\n",
    "#   2. loss = torch.nn.CrossEntropyLoss()\n",
    "#   3. train = CData(train_x, train_y)\n",
    "#   4. test = CData(train_x, train_y)\n",
    "#   5. test_loss, test_rate = test_model(dl_test, model, loss)\n",
    "def run_mlp(train_x, train_y, test_x, test_y, b_s = 100, epochs = 100, verbose = True):\n",
    "    \"\"\"\n",
    "    input:\n",
    "      train_x, type of np array\n",
    "        features of the training set\n",
    "      train_y, type of np array\n",
    "        target of the training set\n",
    "      test_x, type of np array\n",
    "        features of the testing set\n",
    "      test_y, type of np array\n",
    "        target of the testing set\n",
    "    \"\"\"\n",
    "    # set models and loss\n",
    "    model = TS_mlp2(12)\n",
    "    #loss = torch.nn.CrossEntropyLoss()\n",
    "    loss = torch.nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr = 1e-2)\n",
    "    # set the scheduler\n",
    "    lamb1 = lambda x: .1**(x//25)\n",
    "    scheduler = lr_scheduler.LambdaLR(optimizer, lr_lambda = lamb1)\n",
    "    # loda data\n",
    "    train = RData(train_x, train_y)\n",
    "    test = RData(test_x, test_y)\n",
    "    dl_train = DataLoader(train, batch_size = b_s, shuffle = True)\n",
    "    dl_test = DataLoader(test, batch_size = b_s, shuffle = True)\n",
    "    # train the model\n",
    "    num_epochs = epochs\n",
    "    for epoch in range(num_epochs):\n",
    "        scheduler.step()\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        counter = 0\n",
    "        for batch_idx, dat in enumerate(dl_train):\n",
    "            counter += 1\n",
    "            # train the model\n",
    "            optimizer.zero_grad()\n",
    "            inp, target = dat\n",
    "            out = model(inp)\n",
    "            lo = loss(out, target)\n",
    "            lo.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += lo.data\n",
    "            if verbose:\n",
    "                if batch_idx % 10 == 0:\n",
    "                    print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                        epoch,\n",
    "                        batch_idx * b_s,\n",
    "                        len(train),\n",
    "                        100.*batch_idx*b_s/len(train),\n",
    "                        lo.data\n",
    "                        ))\n",
    "        test_lo, test_rate = test_model2(dl_test, model, loss)\n",
    "        if verbose:\n",
    "            # train loss\n",
    "            print('====> Epoch: {} Average train loss: {:.4f}'.format(\n",
    "                epoch,\n",
    "                train_loss/counter\n",
    "                ))\n",
    "            # test loss\n",
    "            print('====> Epoch: {} Average test loss: {:.4f} Average hit rate: {:.4f}'.format(\n",
    "                epoch,\n",
    "                test_lo,\n",
    "                test_rate\n",
    "                ))\n",
    "\n",
    "#####################\n",
    "# assist function\n",
    "####################\n",
    "class CData:\n",
    "    def __init__(self, x, y):\n",
    "        \"\"\"\n",
    "        input:\n",
    "            x: np array\n",
    "              training features\n",
    "            y: np array\n",
    "              training target\n",
    "        \"\"\"\n",
    "        self.data = {}\n",
    "        self.data['train_x'] = torch.from_numpy(x).type(torch.FloatTensor)\n",
    "        self.data['train_y'] = torch.from_numpy(y).type(torch.LongTensor)\n",
    "        assert(len(self.data['train_x']) == len(self.data['train_y']))\n",
    "        self.len = len(self.data['train_x'])\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return (self.data['train_x'][index],\n",
    "                self.data['train_y'][index])\n",
    "    \n",
    "class RData:\n",
    "    def __init__(self, x, y):\n",
    "        \"\"\"\n",
    "        input:\n",
    "            x: np array\n",
    "              training features\n",
    "            y: np array\n",
    "              training target\n",
    "        \"\"\"\n",
    "        self.data = {}\n",
    "        self.data['train_x'] = torch.from_numpy(x).type(torch.FloatTensor)\n",
    "        self.data['train_y'] = torch.from_numpy(y).type(torch.FloatTensor)\n",
    "        assert(len(self.data['train_x']) == len(self.data['train_y']))\n",
    "        self.len = len(self.data['train_x'])\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return (self.data['train_x'][index],\n",
    "                self.data['train_y'][index])\n",
    "\n",
    "def test_model(dl_test, model, loss):\n",
    "    \"\"\"\n",
    "    for classificator\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    test_rate = 0\n",
    "    counter = 0\n",
    "    for batch_idx, dat in enumerate(dl_test):\n",
    "        counter += 1\n",
    "        #TODO codes to be changed\n",
    "        inp, target = dat\n",
    "        out = model(inp)\n",
    "        lo = loss(out, target)\n",
    "        rate = (out.max(dim = 1)[1] == target).sum()\n",
    "        test_loss += lo.data\n",
    "        test_rate += rate\n",
    "    test_rate = 1.0*test_rate/len(dl_test)\n",
    "    return test_loss/counter, test_rate\n",
    "\n",
    "def test_model2(dl_test, model, loss):\n",
    "    \"\"\"\n",
    "    for regression\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    test_rate = 0\n",
    "    counter = 0\n",
    "    for batch_idx, dat in enumerate(dl_test):\n",
    "        counter += 1\n",
    "        #TODO codes to be changed\n",
    "        inp, target = dat\n",
    "        out = model(inp)\n",
    "        lo = loss(out, target)\n",
    "        rate = (out.round() == target).sum()\n",
    "        test_loss += lo.data\n",
    "        test_rate += rate\n",
    "    test_rate = 1.0*test_rate/len(dl_test)\n",
    "    return test_loss/counter, test_rate\n",
    "########################\n",
    "## model\n",
    "########################\n",
    "class TS_mlp(torch.nn.Module):\n",
    "    def __init__(self, dim_input = 7):\n",
    "        super(TS_mlp, self).__init__()\n",
    "        num_hidden = 128\n",
    "        self.mlp = torch.nn.Sequential(\n",
    "                torch.nn.Linear(dim_input, num_hidden),\n",
    "                #torch.nn.Dropout(),\n",
    "                torch.nn.BatchNorm1d(128),\n",
    "                torch.nn.ReLU(),\n",
    "                torch.nn.Linear(num_hidden, 2),\n",
    "                )\n",
    "    \n",
    "    def forward(self, inp):\n",
    "        out = self.mlp(inp)\n",
    "        return out.squeeze()\n",
    "class TS_mlp2(torch.nn.Module):\n",
    "    def __init__(self, dim_input = 7):\n",
    "        super(TS_mlp2, self).__init__()\n",
    "        num_hidden = 128\n",
    "        self.mlp = torch.nn.Sequential(\n",
    "                torch.nn.Linear(dim_input, num_hidden),\n",
    "                #torch.nn.Dropout(),\n",
    "                torch.nn.BatchNorm1d(128),\n",
    "                torch.nn.ReLU(),\n",
    "                torch.nn.Linear(num_hidden, 1),\n",
    "                )\n",
    "    \n",
    "    def forward(self, inp):\n",
    "        out = self.mlp(inp)\n",
    "        return out.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# finetune\n",
    "def _get_param_grid_and_model_for_gridsearch(model = 'ada'):\n",
    "    \"\"\"\n",
    "    input:\n",
    "        model: type of model\n",
    "    output:\n",
    "        param_grid, type of dict\n",
    "        mod: MLA model \n",
    "    \"\"\"\n",
    "    grid_n_estimator = [2, 4, 8, 16, 32, 64, 128, 256, 512, 1024]\n",
    "    grid_ratio = [.1, .25, .5, .75, 1.0]\n",
    "    grid_learn = [.01, .03, .05, .1, .25]\n",
    "    grid_max_depth = [2, 4, 6, 8, 10, None]\n",
    "    grid_min_samples = [5, 10, .03, .05, .01]\n",
    "    grid_criterion = ['gini', 'entropy']\n",
    "    grid_bool = [True, False]\n",
    "    grid_seed = [0]\n",
    "    if model == 'adaBoost':\n",
    "        grid_param = {\n",
    "#                'base_estimator__criterion': ['gini', 'entropy'],\n",
    "#                'base_estimator__splitter': ['best', 'random'],\n",
    "                'n_estimators': grid_n_estimator, # default = 50\n",
    "                'learning_rate': grid_learn, # default = 1\n",
    "                'algorithm': ['SAMME', 'SAMME.R'], # default = 'SAMME.R'\n",
    "                'random_state': grid_seed\n",
    "                }\n",
    "        mod = ensemble.AdaBoostClassifier()\n",
    "    elif model == 'bagging':\n",
    "        grid_param = {\n",
    "                'n_estimators': grid_n_estimator, # default = 10\n",
    "                'max_samples': grid_ratio, # default = 1.0\n",
    "                'random_state': grid_seed\n",
    "                }\n",
    "        mod = ensemble.BaggingClassifier()\n",
    "    elif model == 'extraTrees':\n",
    "        grid_param = {\n",
    "                'n_estimators': grid_n_estimator, # default  = 10\n",
    "                'criterion': grid_criterion, # default = 'gini'\n",
    "                'max_depth': grid_max_depth, # default = None\n",
    "                'max_features': [1, 3, 7],\n",
    "                'min_samples_split': [2, 3, 7],\n",
    "                'min_samples_leaf': [1, 3, 7],\n",
    "                'bootstrap': [False],\n",
    "                'random_state': grid_seed\n",
    "                }\n",
    "        mod = ensemble.ExtraTreesClassifier()\n",
    "    elif model == 'gradientBoosting':\n",
    "        grid_param = {\n",
    "                'loss': ['deviance', 'exponential'], # default = 'deviance'\n",
    "                'learning_rate': [.1, .05, .01], # default = 0.1\n",
    "                'n_estimators': [300], # default = 100\n",
    "                'criterion': ['friedman_mse', 'mse', 'mae'], # default = 'friedman_mse'\n",
    "                'max_depth': grid_max_depth, # default = 3\n",
    "                'min_samples_leaf': [100,150],\n",
    "                'max_features': [.3, .1],\n",
    "                'random_state': grid_seed\n",
    "                }\n",
    "        mod = ensemble.GradientBoostingClassifier()\n",
    "    elif model == 'randomForest':\n",
    "        grid_param = {\n",
    "                'n_estimators': grid_n_estimator, # default = 0\n",
    "                'criterion': grid_criterion, # default = 'gini'\n",
    "                'max_depth': grid_max_depth, # default = None\n",
    "                'oob_score': [True], # default = False\n",
    "                'random_state': grid_seed,\n",
    "                'min_samples_split': [2, 3, 7],\n",
    "                'min_samples_leaf': [1, 3, 7],\n",
    "                }\n",
    "        mod = ensemble.RandomForestClassifier()\n",
    "    elif model == 'gaussianProcess':\n",
    "        grid_param = {\n",
    "                'max_iter_predict': grid_n_estimator, # default = 100\n",
    "                'random_state': grid_seed\n",
    "                }\n",
    "        mod = gaussian_process.GaussianProcessClassifier()\n",
    "    elif model == 'decisionTree':\n",
    "        param_grid = {\n",
    "                'criterion': ['gini', 'entropy'], # default gini\n",
    "                'splitter': ['best', 'random'], # default best\n",
    "                'max_depth': [2, 4, 6, 8, 10, None], # default None\n",
    "                'min_samples_split': [2, 5, 10, .03, .05], # minimum subset size before new split, default 2\n",
    "                'min_samples_lear': [1, 5, 10, .03, .05], # minimum subset size after new split, default 1\n",
    "                'max_features': [None, 'auto'], # max features to consider when performing split; default noen or all\n",
    "                'random_state': [0] # seed or control random number generator\n",
    "                }\n",
    "        mod = tree.DecisionTreeClassifier()\n",
    "    elif model == 'logisticRegression':\n",
    "        grid_param = {\n",
    "                'fit_intercept': grid_bool, # default = True\n",
    "                #'penalty': ['11', '12'],\n",
    "                'solver': ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga'], # default: lbfgs\n",
    "                'random_state': grid_seed\n",
    "                }\n",
    "        mod = linear_model.LogisticRegressionCV() # ??\n",
    "    elif model == 'bernoulliNB':\n",
    "        grid_param = {\n",
    "                'alpha': grid_ratio, # default1.0\n",
    "                }\n",
    "        mod = naive_bayes.BernoulliNB()\n",
    "    elif model == 'kNeighbors':\n",
    "        grid_param = {\n",
    "                'n_neighbors': [1, 2, 3, 4, 5, 6, 7], # default = 5\n",
    "                'weights': ['uniform', 'distance'], #default = 'uniform'\n",
    "                'algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute']\n",
    "                }\n",
    "        mod = neighbors.KNeighborsClassifier()\n",
    "    elif model == 'svc':\n",
    "        grid_param = {\n",
    "                'kernel': ['linear', 'poly', 'rbf', 'sigmoid'],\n",
    "                'C': [1, 5, 10, 50, 100, 200, 300, 1000], # default = 1.0\n",
    "                'gamma': grid_ratio, # default = auto\n",
    "                'decision_function_shape': ['ovo', 'ovr'], # default = ovr\n",
    "                'probability': [True],\n",
    "                'random_state': grid_seed\n",
    "                }\n",
    "        mod = svm.SVC(probability = True)\n",
    "    elif model == 'xgbc':\n",
    "        grid_param = {\n",
    "                'learning_rate': grid_learn, # default = .3\n",
    "                'max_depth': [1, 2, 4, 6, 8, 10], # default = 2\n",
    "                'n_estimators': grid_n_estimator,\n",
    "                'seed': grid_seed\n",
    "                }\n",
    "        mod = XGBClassifier()\n",
    "    else:\n",
    "        print('unrecognized model: '+ model)\n",
    "    return grid_param, mod\n",
    "def finetune_gridsearch(model = 'adaBoost'):\n",
    "    \"\"\"\n",
    "    ada: AdaBoostClassifier\n",
    "    \"\"\"\n",
    "    param_grid, mod = _get_param_grid_and_model_for_gridsearch(model)\n",
    "    cv_split = model_selection.ShuffleSplit(n_splits = 10, test_size = .3, train_size = .6, random_state = 0)\n",
    "    tune_model = model_selection.GridSearchCV(mod, param_grid = param_grid, scoring = 'roc_auc', cv = cv_split)\n",
    "    return tune_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def check_to_1(df, atta, attb):\n",
    "    \"\"\"\n",
    "    input:\n",
    "        df, Dataframe:\n",
    "            the data frame\n",
    "        atta, string:\n",
    "            first attribute\n",
    "        attb, string:\n",
    "            second attribute\n",
    "    output:\n",
    "        out boolean:\n",
    "            if for an a has more than one b, return True, otherwise return False\n",
    "    \"\"\"\n",
    "    df = df[[atta, attb]]\n",
    "    df.to_csv('/tmp/tmp.csv', sep = ';', index = False)\n",
    "    abdict = ABDict('/tmp/tmp.csv', header = True, sep = ';')\n",
    "    dic = abdict.dic\n",
    "    flag = 0\n",
    "    for i,j in dic.items():\n",
    "        if len(j) > 1:\n",
    "            print(str(i) + '\\t' + str(j))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 转化为auftrag table， 但是这次合并的是Teile-Nr项\n",
    "# 给的数据的每一行都是一个维修项，初衷是，把属于同一个auftrag的维修项合并到一起，看一下，在同一个Auftrag中，经常一起修的是那些内容\n",
    "\n",
    "def toAuftragTable(df, att, auftn, clean = True):\n",
    "    \"\"\"\n",
    "    input:\n",
    "        df, DataFrame:\n",
    "            the dataframe\n",
    "        att, string:\n",
    "            the column name of the target attribute\n",
    "        auftn, string:\n",
    "            the column name of the aftragsnummer attribute\n",
    "        clean:\n",
    "            when true, drop the null item in auftn attribute.\n",
    "    output:\n",
    "        df_g, DataFrame:\n",
    "            dataframe contrains two columns auftn and att\n",
    "            type of item in att is string, separate with ';'\n",
    "    \"\"\"\n",
    "    # assert: make sure the type of the attributes inputted\n",
    "    \n",
    "    # extract the att and date columns\n",
    "    df = df[[att, auftn]]\n",
    "    # set type to object\n",
    "    #df[att] = df[att].astype('object')\n",
    "    #df[auftn] = df[auftn].astype('object')\n",
    "    # if clean is True, drop the fake data, like the null data\n",
    "    if clean:\n",
    "        print(\"Falls Null date exist, drop these dates directly\")\n",
    "        #df = df.drop(df[df[att].isnull()].index)\n",
    "        df = df.drop(df[df[auftn].isnull()].index)\n",
    "    # group and sum \n",
    "    df_g = df.groupby([auftn], as_index = False).apply(agg)\n",
    "    return df_g\n",
    "\n",
    "# apply 只能对单行进行处理，而不是对整个分组进行处理，所以估计应该把axis换成1，比较好\n",
    "def agg(x):\n",
    "    # 是否用‘ ’分隔会比较好，这样就不用对初始的属性，\n",
    "    # x 在这里是dataframe？？？\n",
    "    #x = [str(i) for i in x]\n",
    "    x = x.apply(lambda x: ';'.join(set([str(i) for i in x])), axis = 0)\n",
    "    #x = x.apply(lambda x: ' '.join(set(x)), axis = 0)\n",
    "    #print(x.columns.values)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ABDict:\n",
    "    \"\"\"\n",
    "    make sure that only two columns in the df, otherwise only the first two columns will be used\n",
    "    用于两列数据互补空值\n",
    "    \"\"\"\n",
    "    def __init__(self, path, header = True, sep = ';'):\n",
    "        self.dic = self._load_dict(path, header, sep) \n",
    "        self.dic_conv = self._load_dict_conv(path, header, sep)\n",
    "        print(len(self.dic))\n",
    "        print(len(self.dic_conv))\n",
    "        counter = 0\n",
    "        ########\n",
    "        # test #\n",
    "        ########\n",
    "        #for i in self.dic.keys():\n",
    "        #    if counter > 1:\n",
    "        #        break\n",
    "        #    print(i, self.dic[i])\n",
    "        #    counter += 1\n",
    "        #print('schraube' in self.dic_conv.keys())\n",
    "    \n",
    "    def AToB(self, a):\n",
    "        if a in self.dic.keys():\n",
    "            out = self.dic[a][0]\n",
    "            if type(out) == list:\n",
    "                print(out)\n",
    "            return out\n",
    "        else:\n",
    "            #print(a)\n",
    "            return float('nan')\n",
    "    \n",
    "    def BToA(self, b):\n",
    "        if b in self.dic_conv.keys():\n",
    "            out = self.dic_conv[b][0]\n",
    "            #print(out)\n",
    "            return out\n",
    "        else:\n",
    "            #print(b)\n",
    "            return float('nan')\n",
    "\n",
    "    def _load_dict(self, path, header, sep):\n",
    "        dic = {} \n",
    "        with open(path) as fi:\n",
    "            counter = 0\n",
    "            for li in fi:\n",
    "                if header and counter ==0:\n",
    "                    # drop first line if header is true\n",
    "                    counter += 1\n",
    "                    continue\n",
    "                li = li.strip()   # 忘记去换行符了，导致调了一晚上的错\n",
    "                items = li.split(sep)\n",
    "                if items[0] not in dic.keys():\n",
    "                    # if not exit, add new item\n",
    "                    if items[1] != 'nan':\n",
    "                        dic[items[0]] = [items[1]]\n",
    "                elif items[1] not in dic[items[0]]:\n",
    "                    # if item exit but value not eixt, add the value to the list\n",
    "                    dic[items[0]].append(items[1])\n",
    "                counter += 1\n",
    "        return dic\n",
    "    \n",
    "    def _load_dict_conv(self, path, header, sep):\n",
    "        dic = {}\n",
    "        with open(path) as fi:\n",
    "            counter = 0\n",
    "            for li in fi:\n",
    "                if header and counter == 0:\n",
    "                    # drop first line if header is true\n",
    "                    counter += 1\n",
    "                    continue\n",
    "                li = li.strip()\n",
    "                items = li.split(sep)\n",
    "                if items[1] not in dic.keys():\n",
    "                    if items[0] != 'nan':\n",
    "                        #print(items[0])\n",
    "                        dic[items[1]] = [items[0]]\n",
    "                elif items[0] not in dic[items[1]]:\n",
    "                    dic[items[1]].append(items[0])\n",
    "                counter += 1\n",
    "        return dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from math import log\n",
    "import random\n",
    "\n",
    "\n",
    "def entropy(data_classes, base=2):\n",
    "    '''\n",
    "    Computes the entropy of a set of labels (class instantiations)\n",
    "    :param base: logarithm base for computation\n",
    "    :param data_classes: Series with labels of examples in a dataset\n",
    "    :return: value of entropy\n",
    "    '''\n",
    "    if not isinstance(data_classes, pd.core.series.Series):\n",
    "        raise AttributeError('input array should be a pandas series')\n",
    "    classes = data_classes.unique()\n",
    "    N = len(data_classes)\n",
    "    ent = 0  # initialize entropy\n",
    "\n",
    "    # iterate over classes\n",
    "    for c in classes:\n",
    "        partition = data_classes[data_classes == c]  # data with class = c\n",
    "        proportion = len(partition) / N\n",
    "        #update entropy\n",
    "        ent -= proportion * log(proportion, base)\n",
    "\n",
    "    return ent\n",
    "\n",
    "def cut_point_information_gain(dataset, cut_point, feature_label, class_label):\n",
    "    '''\n",
    "    Return de information gain obtained by splitting a numeric attribute in two according to cut_point\n",
    "    :param dataset: pandas dataframe with a column for attribute values and a column for class\n",
    "    :param cut_point: threshold at which to partition the numeric attribute\n",
    "    :param feature_label: column label of the numeric attribute values in data\n",
    "    :param class_label: column label of the array of instance classes\n",
    "    :return: information gain of partition obtained by threshold cut_point\n",
    "    '''\n",
    "    if not isinstance(dataset, pd.core.frame.DataFrame):\n",
    "        raise AttributeError('input dataset should be a pandas data frame')\n",
    "\n",
    "    entropy_full = entropy(dataset[class_label])  # compute entropy of full dataset (w/o split)\n",
    "\n",
    "    #split data at cut_point\n",
    "    data_left = dataset[dataset[feature_label] <= cut_point]\n",
    "    data_right = dataset[dataset[feature_label] > cut_point]\n",
    "    (N, N_left, N_right) = (len(dataset), len(data_left), len(data_right))\n",
    "\n",
    "    gain = entropy_full - (N_left / N) * entropy(data_left[class_label]) - \\\n",
    "        (N_right / N) * entropy(data_right[class_label])\n",
    "\n",
    "    return gain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "__author__ = 'Victor Ruiz, vmr11@pitt.edu'\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from math import log\n",
    "import sys\n",
    "import getopt\n",
    "import re\n",
    "\n",
    "class MDLP_Discretizer(object):\n",
    "    def __init__(self, dataset, class_label, out_path_data=None, out_path_bins=None, features=None):\n",
    "        '''\n",
    "        initializes discretizer object:\n",
    "            saves raw copy of data and creates self._data with only features to discretize and class\n",
    "            computes initial entropy (before any splitting)\n",
    "            self._features = features to be discretized\n",
    "            self._classes = unique classes in raw_data\n",
    "            self._class_name = label of class in pandas dataframe\n",
    "            self._data = partition of data with only features of interest and class\n",
    "            self._cuts = dictionary with cut points for each feature\n",
    "        :param dataset: pandas dataframe with data to discretize\n",
    "        :param class_label: name of the column containing class in input dataframe\n",
    "        :param features: if !None, features that the user wants to discretize specifically\n",
    "        :return:\n",
    "        '''\n",
    "\n",
    "        if not isinstance(dataset, pd.core.frame.DataFrame):  # class needs a pandas dataframe\n",
    "            raise AttributeError('input dataset should be a pandas data frame')\n",
    "\n",
    "        self._data_raw = dataset #copy or original input data\n",
    "\n",
    "        self._class_name = class_label\n",
    "\n",
    "        self._classes = self._data_raw[self._class_name] #.unique()\n",
    "        self._classes.drop_duplicates()\n",
    "\n",
    "\n",
    "        #if user specifies which attributes to discretize\n",
    "        if features:\n",
    "            self._features = [f for f in features if f in self._data_raw.columns]  # check if features in dataframe\n",
    "            missing = set(features) - set(self._features)  # specified columns not in dataframe\n",
    "            if missing:\n",
    "                print('WARNING: user-specified features %s not in input dataframe' % str(missing))\n",
    "        else:  # then we need to recognize which features are numeric\n",
    "            numeric_cols = self._data_raw._data.get_numeric_data().items\n",
    "            self._features = [f for f in numeric_cols if f != class_label]\n",
    "        #other features that won't be discretized\n",
    "        self._ignored_features = set(self._data_raw.columns) - set(self._features)\n",
    "\n",
    "        #create copy of data only including features to discretize and class\n",
    "        self._data = self._data_raw.loc[:, self._features + [class_label]]\n",
    "        self._data = self._data.convert_objects(convert_numeric=True)\n",
    "        #pre-compute all boundary points in dataset\n",
    "        self._boundaries = self.compute_boundary_points_all_features()\n",
    "        #initialize feature bins with empty arrays\n",
    "        self._cuts = {f: [] for f in self._features}\n",
    "        #get cuts for all features\n",
    "        self.all_features_accepted_cutpoints()\n",
    "        #discretize self._data\n",
    "        #self.apply_cutpoints(out_data_path=out_path_data, out_bins_path=out_path_bins)\n",
    "\n",
    "    def MDLPC_criterion(self, data, feature, cut_point):\n",
    "        '''\n",
    "        Determines whether a partition is accepted according to the MDLPC criterion\n",
    "        :param feature: feature of interest\n",
    "        :param cut_point: proposed cut_point\n",
    "        :param partition_index: index of the sample (dataframe partition) in the interval of interest\n",
    "        :return: True/False, whether to accept the partition\n",
    "        '''\n",
    "        #get dataframe only with desired attribute and class columns, and split by cut_point\n",
    "        data_partition = data.copy(deep=True)\n",
    "        data_left = data_partition[data_partition[feature] <= cut_point]\n",
    "        data_right = data_partition[data_partition[feature] > cut_point]\n",
    "\n",
    "        #compute information gain obtained when splitting data at cut_point\n",
    "        cut_point_gain = cut_point_information_gain(dataset=data_partition, cut_point=cut_point,\n",
    "                                                    feature_label=feature, class_label=self._class_name)\n",
    "        #compute delta term in MDLPC criterion\n",
    "        N = len(data_partition) # number of examples in current partition\n",
    "        partition_entropy = entropy(data_partition[self._class_name])\n",
    "        k = len(data_partition[self._class_name].unique())\n",
    "        k_left = len(data_left[self._class_name].unique())\n",
    "        k_right = len(data_right[self._class_name].unique())\n",
    "        entropy_left = entropy(data_left[self._class_name])  # entropy of partition\n",
    "        entropy_right = entropy(data_right[self._class_name])\n",
    "        delta = log(3 ** k, 2) - (k * partition_entropy) + (k_left * entropy_left) + (k_right * entropy_right)\n",
    "\n",
    "        #to split or not to split\n",
    "        gain_threshold = (log(N - 1, 2) + delta) / N\n",
    "\n",
    "        if cut_point_gain > gain_threshold:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "    def feature_boundary_points(self, data, feature):\n",
    "        '''\n",
    "        Given an attribute, find all potential cut_points (boundary points)\n",
    "        :param feature: feature of interest\n",
    "        :param partition_index: indices of rows for which feature value falls whithin interval of interest\n",
    "        :return: array with potential cut_points\n",
    "        '''\n",
    "        #get dataframe with only rows of interest, and feature and class columns\n",
    "        data_partition = data.copy(deep=True)\n",
    "        data_partition.sort_values(feature, ascending=True, inplace=True)\n",
    "\n",
    "        boundary_points = []\n",
    "\n",
    "        #add temporary columns\n",
    "        data_partition['class_offset'] = data_partition[self._class_name].shift(1)  # column where first value is now second, and so forth\n",
    "        data_partition['feature_offset'] = data_partition[feature].shift(1)  # column where first value is now second, and so forth\n",
    "        data_partition['feature_change'] = (data_partition[feature] != data_partition['feature_offset'])\n",
    "        data_partition['mid_points'] = data_partition.loc[:, [feature, 'feature_offset']].mean(axis=1)\n",
    "\n",
    "        potential_cuts = data_partition[data_partition['feature_change'] == True].index[1:]\n",
    "        sorted_index = data_partition.index.tolist()\n",
    "\n",
    "        for row in potential_cuts:\n",
    "            old_value = data_partition.loc[sorted_index[sorted_index.index(row) - 1]][feature]\n",
    "            new_value = data_partition.loc[row][feature]\n",
    "            old_classes = data_partition[data_partition[feature] == old_value][self._class_name].unique()\n",
    "            new_classes = data_partition[data_partition[feature] == new_value][self._class_name].unique()\n",
    "            if len(set.union(set(old_classes), set(new_classes))) > 1:\n",
    "                boundary_points += [data_partition.loc[row]['mid_points']]\n",
    "\n",
    "        return set(boundary_points)\n",
    "\n",
    "    def compute_boundary_points_all_features(self):\n",
    "        '''\n",
    "        Computes all possible boundary points for each attribute in self._features (features to discretize)\n",
    "        :return:\n",
    "        '''\n",
    "        boundaries = {}\n",
    "        for attr in self._features:\n",
    "            data_partition = self._data.loc[:, [attr, self._class_name]]\n",
    "            boundaries[attr] = self.feature_boundary_points(data=data_partition, feature=attr)\n",
    "        return boundaries\n",
    "\n",
    "    def boundaries_in_partition(self, data, feature):\n",
    "        '''\n",
    "        From the collection of all cut points for all features, find cut points that fall within a feature-partition's\n",
    "        attribute-values' range\n",
    "        :param data: data partition (pandas dataframe)\n",
    "        :param feature: attribute of interest\n",
    "        :return: points within feature's range\n",
    "        '''\n",
    "        range_min, range_max = (data[feature].min(), data[feature].max())\n",
    "        return set([x for x in self._boundaries[feature] if (x > range_min) and (x < range_max)])\n",
    "\n",
    "    def best_cut_point(self, data, feature):\n",
    "        '''\n",
    "        Selects the best cut point for a feature in a data partition based on information gain\n",
    "        :param data: data partition (pandas dataframe)\n",
    "        :param feature: target attribute\n",
    "        :return: value of cut point with highest information gain (if many, picks first). None if no candidates\n",
    "        '''\n",
    "        candidates = self.boundaries_in_partition(data=data, feature=feature)\n",
    "        # candidates = self.feature_boundary_points(data=data, feature=feature)\n",
    "        if not candidates:\n",
    "            return None\n",
    "        gains = [(cut, cut_point_information_gain(dataset=data, cut_point=cut, feature_label=feature,\n",
    "                                                  class_label=self._class_name)) for cut in candidates]\n",
    "        gains = sorted(gains, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "        return gains[0][0] #return cut point\n",
    "\n",
    "    def single_feature_accepted_cutpoints(self, feature, partition_index=pd.DataFrame().index):\n",
    "        '''\n",
    "        Computes the cuts for binning a feature according to the MDLP criterion\n",
    "        :param feature: attribute of interest\n",
    "        :param partition_index: index of examples in data partition for which cuts are required\n",
    "        :return: list of cuts for binning feature in partition covered by partition_index\n",
    "        '''\n",
    "        if partition_index.size == 0:\n",
    "            partition_index = self._data.index  # if not specified, full sample to be considered for partition\n",
    "\n",
    "        data_partition = self._data.loc[partition_index, [feature, self._class_name]]\n",
    "\n",
    "        #exclude missing data:\n",
    "        if data_partition[feature].isnull().values.any:\n",
    "            data_partition = data_partition[~data_partition[feature].isnull()]\n",
    "\n",
    "        #stop if constant or null feature values\n",
    "        if len(data_partition[feature].unique()) < 2:\n",
    "            return\n",
    "        #determine whether to cut and where\n",
    "        cut_candidate = self.best_cut_point(data=data_partition, feature=feature)\n",
    "        if cut_candidate == None:\n",
    "            return\n",
    "        decision = self.MDLPC_criterion(data=data_partition, feature=feature, cut_point=cut_candidate)\n",
    "\n",
    "        #apply decision\n",
    "        if not decision:\n",
    "            return  # if partition wasn't accepted, there's nothing else to do\n",
    "        if decision:\n",
    "            # try:\n",
    "            #now we have two new partitions that need to be examined\n",
    "            left_partition = data_partition[data_partition[feature] <= cut_candidate]\n",
    "            right_partition = data_partition[data_partition[feature] > cut_candidate]\n",
    "            if left_partition.empty or right_partition.empty:\n",
    "                return #extreme point selected, don't partition\n",
    "            self._cuts[feature] += [cut_candidate]  # accept partition\n",
    "            self.single_feature_accepted_cutpoints(feature=feature, partition_index=left_partition.index)\n",
    "            self.single_feature_accepted_cutpoints(feature=feature, partition_index=right_partition.index)\n",
    "            #order cutpoints in ascending order\n",
    "            self._cuts[feature] = sorted(self._cuts[feature])\n",
    "            return\n",
    "\n",
    "    def all_features_accepted_cutpoints(self):\n",
    "        '''\n",
    "        Computes cut points for all numeric features (the ones in self._features)\n",
    "        :return:\n",
    "        '''\n",
    "        for attr in self._features:\n",
    "            self.single_feature_accepted_cutpoints(feature=attr)\n",
    "        return\n",
    "\n",
    "    def apply_cutpoints(self, out_data_path=None, out_bins_path=None):\n",
    "        '''\n",
    "        Discretizes data by applying bins according to self._cuts. Saves a new, discretized file, and a description of\n",
    "        the bins\n",
    "        :param out_data_path: path to save discretized data\n",
    "        :param out_bins_path: path to save bins description\n",
    "        :return:\n",
    "        '''\n",
    "        bin_label_collection = {}\n",
    "        for attr in self._features:\n",
    "            if len(self._cuts[attr]) == 0:\n",
    "                self._data[attr] = 'All'\n",
    "                bin_label_collection[attr] = ['All']\n",
    "            else:\n",
    "                cuts = [-np.inf] + self._cuts[attr] + [np.inf]\n",
    "                start_bin_indices = range(0, len(cuts) - 1)\n",
    "                bin_labels = ['%s_to_%s' % (str(cuts[i]), str(cuts[i+1])) for i in start_bin_indices]\n",
    "                bin_label_collection[attr] = bin_labels\n",
    "                self._data[attr] = pd.cut(x=self._data[attr].values, bins=cuts, right=False, labels=bin_labels,\n",
    "                                          precision=6, include_lowest=True)\n",
    "\n",
    "        #reconstitute full data, now discretized\n",
    "        if self._ignored_features:\n",
    "            to_return = pd.concat([self._data, self._data_raw[list(self._ignored_features)]], axis=1)\n",
    "            to_return = to_return[self._data_raw.columns] #sort columns so they have the original order\n",
    "        else:\n",
    "            to_return = self._data\n",
    "        \n",
    "        return to_return\n",
    "    \n",
    "        #save data as csv\n",
    "        if out_data_path:\n",
    "            to_return.to_csv(out_data_path)\n",
    "        #save bins description\n",
    "        if out_bins_path:\n",
    "            with open(out_bins_path, 'w') as bins_file:\n",
    "                print>>bins_file, 'Description of bins in file: %s' % out_data_path\n",
    "                for attr in self._features:\n",
    "                    print>>bins_file, 'attr: %s\\n\\t%s' % (attr, ', '.join([bin_label for bin_label in bin_label_collection[attr]]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data_mining",
   "language": "python",
   "name": "data_mining"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
