{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ihuangyiran/anaconda2/envs/data_mining/lib/python3.6/site-packages/sklearn/utils/fixes.py:313: FutureWarning: numpy not_equal will not check object identity in the future. The comparison did not return the same result as suggested by the identity (`is`)) and will change.\n",
      "  _nan_object_mask = _nan_object_array != _nan_object_array\n"
     ]
    }
   ],
   "source": [
    "# load package\n",
    "# string \n",
    "import re\n",
    "\n",
    "# math\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import random\n",
    "\n",
    "# sys\n",
    "import sys\n",
    "import os\n",
    "import time\n",
    "import warnings\n",
    "\n",
    "# date\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# math\n",
    "import math\n",
    "\n",
    "# preprocessing\n",
    "from sklearn.preprocessing import KBinsDiscretizer, StandardScaler, MinMaxScaler, QuantileTransformer, PowerTransformer\n",
    "\n",
    "# machine learning\n",
    "from sklearn import svm, tree, linear_model, neighbors, naive_bayes, ensemble, gaussian_process, discriminant_analysis\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# model utils\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder, OrdinalEncoder\n",
    "from sklearn import feature_selection \n",
    "from sklearn import model_selection\n",
    "from sklearn import metrics\n",
    "\n",
    "# apriori\n",
    "from efficient_apriori import apriori\n",
    "\n",
    "# plot\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.pylab as pylab\n",
    "import seaborn as sns\n",
    "from pandas.tools.plotting import scatter_matrix #??\n",
    "# = show plots in Jupyter Notebook browser\n",
    "%matplotlib inline \n",
    "mpl.style.use('ggplot') #??\n",
    "sns.set_style('white')\n",
    "pylab.rcParams['figure.figsize'] = 12,8 #??\n",
    "\n",
    "# show all columns\n",
    "from IPython.display import display\n",
    "pd.options.display.max_columns = None\n",
    "\n",
    "# memory manage\n",
    "import gc\n",
    "\n",
    "# logging\n",
    "import logging \n",
    "\n",
    "# other\n",
    "import tqdm as tqdm\n",
    "\n",
    "# self define\n",
    "sys.path.append('../../utils/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# for network design\n",
    "import torch\n",
    "import argparse\n",
    "import os\n",
    "\n",
    "from scipy.stats import stats\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import lr_scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-11-29 12:04:00,191 - __main__ - INFO - This is a log info\n"
     ]
    }
   ],
   "source": [
    "logging.basicConfig(level = logging.INFO, format = '%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "handler = logging.FileHandler('../log/extract_features.log')\n",
    "handler.setLevel(logging.INFO)\n",
    "formater = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "handler.setFormatter(formater)\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.addHandler(handler)\n",
    "logger.info('This is a log info')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ihuangyiran/anaconda2/envs/data_mining/lib/python3.6/site-packages/IPython/core/interactiveshell.py:2785: DtypeWarning: Columns (15) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n",
      "/Users/ihuangyiran/anaconda2/envs/data_mining/lib/python3.6/site-packages/IPython/core/interactiveshell.py:2785: DtypeWarning: Columns (1,15) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "# read files\n",
    "lb_wsp_2014 = pd.read_csv('../data/Autohaus_weeber/leonberg_werkstattposten_2014.csv', sep = ';')\n",
    "lb_wsp_2015 = pd.read_csv('../data/Autohaus_weeber/leonberg_werkstattposten_2015.csv', sep = ';')\n",
    "lb_wsp_2016 = pd.read_csv('../data/Autohaus_weeber/leonberg_werkstattposten_2016.csv', sep = ';')\n",
    "lb_wsp_2017 = pd.read_csv('../data/Autohaus_weeber/leonberg_werkstattposten_2017.csv', sep = ';')\n",
    "lb_wsp_2018 = pd.read_csv('../data/Autohaus_weeber/leonberg_werkstattposten_2018.csv', sep = ';')\n",
    "std_wsp_2014 = pd.read_csv('../data/Autohaus_weeber/weil_der_stadt_werkstattposten_2014.csv', sep = ';')\n",
    "std_wsp_2015 = pd.read_csv('../data/Autohaus_weeber/weil_der_stadt_werkstattposten_2015.csv', sep = ';')\n",
    "std_wsp_2016 = pd.read_csv('../data/Autohaus_weeber/weil_der_stadt_werkstattposten_2016.csv', sep = ';')\n",
    "std_wsp_2017 = pd.read_csv('../data/Autohaus_weeber/weil_der_stadt_werkstattposten_2017.csv', sep = ';')\n",
    "std_wsp_2018 = pd.read_csv('../data/Autohaus_weeber/weil_der_stadt_werkstattposten_2018.csv', sep = ';')\n",
    "# cat\n",
    "d1 = lb_wsp_2014.copy()\n",
    "d2 = lb_wsp_2015.copy()\n",
    "d3 = lb_wsp_2016.copy()\n",
    "d4 = lb_wsp_2017.copy()\n",
    "d5 = lb_wsp_2018.copy()\n",
    "\n",
    "d6 = std_wsp_2014.copy()\n",
    "d7 = std_wsp_2015.copy()\n",
    "d8 = std_wsp_2016.copy()\n",
    "d9 = std_wsp_2017.copy()\n",
    "d10 = std_wsp_2018.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# for training we use data from 14 - 17 in lb_wsp\n",
    "train1 = pd.concat([d1,d2,d3,d4,d5], 0)\n",
    "train2 = pd.concat([d6,d7,d8,d9,d10], 0)\n",
    "train1['Autohaus'] = 'leonberg'\n",
    "train2['Autohaus'] = 'weil'\n",
    "#train = pd.concat([train1, train2], 0)\n",
    "#train = train1.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# because there exists reused Auftragsnummer in different Autohaus station. So we add some sign to the Auftragsnummer\n",
    "# in each Autohaus station\n",
    "train1['Auftragsnummer'] = 'A' + train1['Auftragsnummer']\n",
    "train2['Auftragsnummer'] = 'B' + train2['Auftragsnummer']\n",
    "train = pd.concat([train1, train2], 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# remove the outlier in the Teile-Nr\n",
    "# remove the na wert\n",
    "df = train[train['Teile-Nr'].isna().map(lambda x: not x)]\n",
    "# remove the value short value\n",
    "df = df[df['Teile-Nr'].map(lambda x: False if len(x) < 4 else True)]\n",
    "# remove the value that doesn't contain number: 30593 of 593527 in train1(5%)\n",
    "df = df[df['Teile-Nr'].map(lambda x: True if re.search('\\d', x) else False)]\n",
    "# remove the value that doesn't contrain adjoining number(min 2 number): 594 of 562934 in train1(0.1%)\n",
    "df = df[df['Teile-Nr'].map(lambda x: True if re.search('\\d\\d', x) else False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# use findall instead of search, because, the df here should contain adjoining number, otherwise it's wrong\n",
    "df['Gruppe-Nr'] = df['Teile-Nr'].map(lambda x: re.findall('\\d\\d', x)[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "91"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# clean the memory\n",
    "del train1, train2, lb_wsp_2014, lb_wsp_2015, lb_wsp_2016, lb_wsp_2017, lb_wsp_2018\n",
    "del std_wsp_2014, std_wsp_2015, std_wsp_2016, std_wsp_2017, std_wsp_2018\n",
    "del d1, d2, d3, d4, d5, d6, d7, d8, d9, d10\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "53"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cf = df.copy()\n",
    "# drop duplicate\n",
    "cf = cf.drop_duplicates()\n",
    "# find out the confused data. For the same Auftragsnummer, exists more than one value in the other attribute.\n",
    "# here is AWSAU310019, BWSAU386471, BWSAU435051\n",
    "cf = cf.drop(cf[cf['Auftragsnummer'] == 'AWSAU310019'].index, axis= 0) # 8 items\n",
    "cf = cf.drop(cf[cf['Auftragsnummer'] == 'BWSAU386471'].index, axis= 0) # 3 items\n",
    "cf = cf.drop(cf[cf['Auftragsnummer'] == 'BWSAU435051'].index, axis= 0) # 2 items\n",
    "cf = cf.drop(cf[cf['Auftragsnummer'] == 'BWSAU271939'].index, axis= 0) # ? items\n",
    "# remove wierd auftragsnummer\n",
    "cf = cf.drop(cf[cf['Auftragsnummer'] == '103K'].index) #\n",
    "cf = cf.drop(cf[cf['Auftragsnummer'] == '77KW'].index) #\n",
    "# remove na value in Fahrgestellnummer\n",
    "cf = cf.drop(cf[cf['Fahrgestellnummer'].isna()].index, axis = 0) #\n",
    "# remove the Fahrgestellnumer that only contain number, not tested!!!!\n",
    "cf = cf.drop(cf[cf['Fahrgestellnummer'].map(lambda x: False if re.search('[a-zA-Z]', x) else True)].index, axis = 0)\n",
    "# remove the items, which day of the Auftragsdatum lareger than 31, not tested!!!!\n",
    "cf = cf.drop(cf[cf['Auftragsdatum'].map(lambda x: int(x[0:2])) > 31].index, axis = 0)\n",
    "# remove the items, which length of the Auftragsdatum shorter than 10, not tested!!!!\n",
    "cf = cf.drop(cf[cf['Auftragsdatum'].map(lambda x:True if len(x) < 10 else False)].index, axis = 0)\n",
    "# remove the items, which year of the Auftragsdatum smaller than 2013, not tested!!!!\n",
    "cf = cf.drop(cf[cf['Auftragsdatum'].map(lambda x: int(x[6:]) < 2013)].index, axis = 0)\n",
    "df = cf\n",
    "del cf\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gn = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Falls Null date exist, drop these dates directly\n"
     ]
    }
   ],
   "source": [
    "gn = toAuftragTable(gn, 'Gruppe-Nr', 'Auftragsnummer') # number: 245018"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "af = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "af = af[['Auftragsnummer', 'Fahrgestellnummer', 'Auftragsdatum']]\n",
    "af = af.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "agn = pd.merge(gn, af, how = 'left', on = 'Auftragsnummer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "agn['Auftragsdatum'] = pd.to_datetime(agn['Auftragsdatum'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ihuangyiran/anaconda2/envs/data_mining/lib/python3.6/site-packages/ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "# extract feature: count\n",
    "fc = agn[['Fahrgestellnummer']]\n",
    "fc['count'] = 1\n",
    "fc = fc.groupby('Fahrgestellnummer', as_index = False).sum()\n",
    "agn = pd.merge(agn, fc, how = 'left', on = 'Fahrgestellnummer')\n",
    "del fc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# extract feature: time distance\n",
    "ft = agn[['Fahrgestellnummer', 'Auftragsdatum']]\n",
    "ft_max = ft.groupby('Fahrgestellnummer', as_index = False).max()\n",
    "ft_min = ft.groupby('Fahrgestellnummer', as_index = False).min()\n",
    "ft_com = pd.merge(ft_max, ft_min, how = 'inner', on = 'Fahrgestellnummer')\n",
    "ft_com['period'] = ft_com['Auftragsdatum_x'] - ft_com['Auftragsdatum_y']\n",
    "#ft = pd.merge(ft, ft_com[['Fahrgestellnummer', 'period']], on = 'Fahrgestellnummer', how = 'inner')\n",
    "agn = pd.merge(agn, ft_com[['Fahrgestellnummer', 'period']], on = 'Fahrgestellnummer', how = 'inner')\n",
    "agn['period'] = agn['period'].map(lambda x: x.days/120) # unit in 4 months\n",
    "del ft, ft_max, ft_min, ft_com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# extract feature: frequent\n",
    "def get_fre(x):\n",
    "    #if x['period'] == 0 and x['period'] != 1:\n",
    "    #    return 1\n",
    "    if x['period'] == 0:\n",
    "        return 0\n",
    "    return x['count']*1.0/x['period']\n",
    "agn['frequent'] = agn.apply(get_fre, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function gc.collect>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# combine Gruppe-Nr with the same Fahrgestellnummer and save the result as type of string\n",
    "out = []\n",
    "for name, group in agn[['Gruppe-Nr', 'Fahrgestellnummer']].groupby('Fahrgestellnummer', as_index = False):\n",
    "    out.append(pd.DataFrame({'Fahrgestellnummer': [group.iloc[0, group.columns.get_loc('Fahrgestellnummer')]],\n",
    "                            'Gruppe-Nr_concat': [';'.join(group['Gruppe-Nr'].tolist())]}))\n",
    "out = pd.concat(out, axis = 0)\n",
    "out = out.reset_index().iloc[:,1:]\n",
    "agn = out\n",
    "del out\n",
    "gc.collect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for nr in range(10):\n",
    "    agn['count_group_'+str(nr)] = agn['Gruppe-Nr_concat'].map(lambda x: x.count(str(nr)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# count the number of columns with value 0\n",
    "def get_coverage(x):\n",
    "    counter = 0\n",
    "    for i in range(10):\n",
    "        if x['count_group_'+str(i)] == 0:\n",
    "            counter += 1\n",
    "    return 10 - counter\n",
    "agn['coverage'] = agn.apply(get_coverage, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# get deep\n",
    "def get_deep(x):\n",
    "    deepest = 0\n",
    "    for i in range(10):\n",
    "        if x['count_group_'+str(i)] > deepest:\n",
    "            deepest = x['count_group_'+str(i)]\n",
    "    return deepest\n",
    "agn['deep'] = agn.apply(get_deep, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get total count of teile\n",
    "agn['count'] = agn['Gruppe-Nr_concat'].map(lambda x: len(x.split(';')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# get proportion of group\n",
    "for i in range(10):\n",
    "    agn['proportion_group_'+str(i)] = agn['count_group_'+str(i)]/agn['count']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 70658/70658 [05:40<00:00, 207.69it/s]\n"
     ]
    }
   ],
   "source": [
    "# 赋值为x则奇慢无比，赋值给df_proportions就很快，为什么？？？\n",
    "def get_rank(x):\n",
    "    df_proportions = x[['proportion_group_0', 'proportion_group_1', 'proportion_group_2', 'proportion_group_3', \n",
    "                     'proportion_group_4','proportion_group_5', 'proportion_group_6', 'proportion_group_7', \n",
    "                     'proportion_group_8', 'proportion_group_9']].copy()\n",
    "    sorted_df = df_proportions.sort_values(by = df_proportions.index[0], axis = 1)\n",
    "    counter = 0\n",
    "    value = 0\n",
    "    clock = 0\n",
    "    for index, i in enumerate(sorted_df.columns.tolist()):\n",
    "        if sorted_df.iloc[0, index] > value:\n",
    "            value = sorted_df.iloc[0, index]\n",
    "            counter = clock\n",
    "        df_proportions['rank_group_'+i[-1]] = counter\n",
    "        clock += 1\n",
    "    df_proportions['Fahrgestellnummer'] = x['Fahrgestellnummer']\n",
    "    return df_proportions\n",
    "out = []\n",
    "for name, group in tqdm.tqdm(agn.groupby('Fahrgestellnummer', as_index = False)):\n",
    "    out.append(get_rank(group))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "out = pd.concat(out, axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "out = out[['Fahrgestellnummer', 'rank_group_0', 'rank_group_1', 'rank_group_2', 'rank_group_3', 'rank_group_4', \n",
    "          'rank_group_5', 'rank_group_6', 'rank_group_7', 'rank_group_8', 'rank_group_9']]\n",
    "agn = pd.merge(agn, out, how = 'left', on = 'Fahrgestellnummer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 70658 entries, 0 to 70657\n",
      "Data columns (total 35 columns):\n",
      "Fahrgestellnummer     70658 non-null object\n",
      "Gruppe-Nr_concat      70658 non-null object\n",
      "count_group_0         70658 non-null int64\n",
      "count_group_1         70658 non-null int64\n",
      "count_group_2         70658 non-null int64\n",
      "count_group_3         70658 non-null int64\n",
      "count_group_4         70658 non-null int64\n",
      "count_group_5         70658 non-null int64\n",
      "count_group_6         70658 non-null int64\n",
      "count_group_7         70658 non-null int64\n",
      "count_group_8         70658 non-null int64\n",
      "count_group_9         70658 non-null int64\n",
      "coverage              70658 non-null int64\n",
      "deep                  70658 non-null int64\n",
      "count                 70658 non-null int64\n",
      "proportion_group_0    70658 non-null float64\n",
      "proportion_group_1    70658 non-null float64\n",
      "proportion_group_2    70658 non-null float64\n",
      "proportion_group_3    70658 non-null float64\n",
      "proportion_group_4    70658 non-null float64\n",
      "proportion_group_5    70658 non-null float64\n",
      "proportion_group_6    70658 non-null float64\n",
      "proportion_group_7    70658 non-null float64\n",
      "proportion_group_8    70658 non-null float64\n",
      "proportion_group_9    70658 non-null float64\n",
      "rank_group_0          70658 non-null int64\n",
      "rank_group_1          70658 non-null int64\n",
      "rank_group_2          70658 non-null int64\n",
      "rank_group_3          70658 non-null int64\n",
      "rank_group_4          70658 non-null int64\n",
      "rank_group_5          70658 non-null int64\n",
      "rank_group_6          70658 non-null int64\n",
      "rank_group_7          70658 non-null int64\n",
      "rank_group_8          70658 non-null int64\n",
      "rank_group_9          70658 non-null int64\n",
      "dtypes: float64(10), int64(23), object(2)\n",
      "memory usage: 19.4+ MB\n"
     ]
    }
   ],
   "source": [
    "agn.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "agn.to_pickle('./fahrverhalten_features.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 转化为auftrag table， 但是这次合并的是Teile-Nr项\n",
    "# 给的数据的每一行都是一个维修项，初衷是，把属于同一个auftrag的维修项合并到一起，看一下，在同一个Auftrag中，经常一起修的是那些内容\n",
    "\n",
    "def toAuftragTable(df, att, auftn, clean = True):\n",
    "    \"\"\"\n",
    "    input:\n",
    "        df, DataFrame:\n",
    "            the dataframe\n",
    "        att, string:\n",
    "            the column name of the target attribute\n",
    "        auftn, string:\n",
    "            the column name of the aftragsnummer attribute\n",
    "        clean:\n",
    "            when true, drop the null item in auftn attribute.\n",
    "    output:\n",
    "        df_g, DataFrame:\n",
    "            dataframe contrains two columns auftn and att\n",
    "            type of item in att is string, separate with ';'\n",
    "    \"\"\"\n",
    "    # assert: make sure the type of the attributes inputted\n",
    "    \n",
    "    # extract the att and date columns\n",
    "    df = df[[att, auftn]]\n",
    "    # set type to object\n",
    "    #df[att] = df[att].astype('object')\n",
    "    #df[auftn] = df[auftn].astype('object')\n",
    "    # if clean is True, drop the fake data, like the null data\n",
    "    if clean:\n",
    "        print(\"Falls Null date exist, drop these dates directly\")\n",
    "        #df = df.drop(df[df[att].isnull()].index)\n",
    "        df = df.drop(df[df[auftn].isnull()].index)\n",
    "    # group and sum \n",
    "    df_g = df.groupby([auftn], as_index = False).apply(agg)\n",
    "    return df_g\n",
    "\n",
    "# apply 只能对单行进行处理，而不是对整个分组进行处理，所以估计应该把axis换成1，比较好\n",
    "def agg(x):\n",
    "    # 是否用‘ ’分隔会比较好，这样就不用对初始的属性，\n",
    "    # x 在这里是dataframe？？？\n",
    "    #x = [str(i) for i in x]\n",
    "    x = x.apply(lambda x: ';'.join(set([str(i) for i in x])), axis = 0)\n",
    "    #x = x.apply(lambda x: ' '.join(set(x)), axis = 0)\n",
    "    #print(x.columns.values)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data_mining",
   "language": "python",
   "name": "data_mining"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
